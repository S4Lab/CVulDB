linux-scraping__linux-grsecurity
commit f3d2603b69911638238c6c6197f5ff285b9855a6
Author:     Brad Spengler <spender@grsecurity.net>
AuthorDate: Tue Jan 27 23:09:48 2015 -0500
Commit:     Mickaël Salaün <mic@digikod.net>
CommitDate: Tue Jan 27 23:09:48 2015 -0500

    grsec: Apply grsecurity-3.0-3.2.66-201501272306.patch
    
    commit 46d784468156a0712d4973b03d7cd6448834851e
    Author: Andy Lutomirski <luto@amacapital.net>
    Date:   Thu Jan 22 11:27:59 2015 -0800
    
        x86, tls: Interpret an all-zero struct user_desc as "no segment"
    
        The Witcher 2 did something like this to allocate a TLS segment index:
    
                struct user_desc u_info;
                bzero(&u_info, sizeof(u_info));
                u_info.entry_number = (uint32_t)-1;
    
                syscall(SYS_set_thread_area, &u_info);
    
        Strictly speaking, this code was never correct.  It should have set
        read_exec_only and seg_not_present to 1 to indicate that it wanted
        to find a free slot without putting anything there, or it should
        have put something sensible in the TLS slot if it wanted to allocate
        a TLS entry for real.  The actual effect of this code was to
        allocate a bogus segment that could be used to exploit espfix.
    
        The set_thread_area hardening patches changed the behavior, causing
        set_thread_area to return -EINVAL and crashing the game.
    
        This changes set_thread_area to interpret this as a request to find
        a free slot and to leave it empty, which isn't *quite* what the game
        expects but should be close enough to keep it working.  In
        particular, using the code above to allocate two segments will
        allocate the same segment both times.
    
        According to FrostbittenKing on Github, this fixes The Witcher 2.
    
        If this somehow still causes problems, we could instead allocate
        a limit==0 32-bit data segment, but that seems rather ugly to me.
    
        Fixes: 41bdc78544b8 x86/tls: Validate TLS entries to protect espfix
        Signed-off-by: Andy Lutomirski <luto@amacapital.net>
        Cc: stable@vger.kernel.org
        Cc: torvalds@linux-foundation.org
        Link: http://lkml.kernel.org/r/0cb251abe1ff0958b8e468a9a9a905b80ae3a746.1421954363.git.luto@amacapital.net
        Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    
     arch/x86/include/asm/desc.h |   13 +++++++++++++
     arch/x86/kernel/tls.c       |   25 +++++++++++++++++++++++--
     2 files changed, 36 insertions(+), 2 deletions(-)
    
    commit fbfda9617d4e349cb55343f5b265db7ada6b28f9
    Author: Andy Lutomirski <luto@amacapital.net>
    Date:   Thu Jan 22 11:27:58 2015 -0800
    
        x86, tls, ldt: Stop checking lm in LDT_empty
    
        32-bit programs don't have an lm bit in their ABI, so they can't
        reliably cause LDT_empty to return true without resorting to memset.
        They shouldn't need to do this.
    
        This should fix a longstanding, if minor, issue in all 64-bit kernels
        as well as a potential regression in the TLS hardening code.
    
        Fixes: 41bdc78544b8 x86/tls: Validate TLS entries to protect espfix
        Cc: stable@vger.kernel.org
        Signed-off-by: Andy Lutomirski <luto@amacapital.net>
        Cc: torvalds@linux-foundation.org
        Link: http://lkml.kernel.org/r/72a059de55e86ad5e2935c80aa91880ddf19d07c.1421954363.git.luto@amacapital.net
        Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    
     arch/x86/include/asm/desc.h |    9 ++-------
     1 files changed, 2 insertions(+), 7 deletions(-)
    
    commit 512a4bad2a5fff934aa1855ba0fa64cd5357733b
    Author: Nadav Amit <namit@cs.technion.ac.il>
    Date:   Thu Jan 1 23:11:11 2015 +0200
    
        KVM: x86: SYSENTER emulation is broken
    
        SYSENTER emulation is broken in several ways:
        1. It misses the case of 16-bit code segments completely (CVE-2015-0239).
        2. MSR_IA32_SYSENTER_CS is checked in 64-bit mode incorrectly (bits 0 and 1 can
           still be set without causing #GP).
        3. MSR_IA32_SYSENTER_EIP and MSR_IA32_SYSENTER_ESP are not masked in
           legacy-mode.
        4. There is some unneeded code.
    
        Fix it.
    
        Cc: stable@vger.linux.org
        Signed-off-by: Nadav Amit <namit@cs.technion.ac.il>
        Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    
        Conflicts:
    
            arch/x86/kvm/emulate.c
    
        Conflicts:
    
            arch/x86/kvm/emulate.c
    
     arch/x86/kvm/emulate.c |   23 +++++++----------------
     1 files changed, 7 insertions(+), 16 deletions(-)
    
    commit 7a964f0b7ee8c4641cc39ea736e551e24f3fddba
    Author: Daniel Borkmann <dborkman@redhat.com>
    Date:   Thu Jan 22 18:26:54 2015 +0100
    
        net: sctp: fix slab corruption from use after free on INIT collisions
    
        When hitting an INIT collision case during the 4WHS with AUTH enabled, as
        already described in detail in commit 1be9a950c646 ("net: sctp: inherit
        auth_capable on INIT collisions"), it can happen that we occasionally
        still remotely trigger the following panic on server side which seems to
        have been uncovered after the fix from commit 1be9a950c646 ...
    
        [  533.876389] BUG: unable to handle kernel paging request at 00000000ffffffff
        [  533.913657] IP: [<ffffffff811ac385>] __kmalloc+0x95/0x230
        [  533.940559] PGD 5030f2067 PUD 0
        [  533.957104] Oops: 0000 [#1] SMP
        [  533.974283] Modules linked in: sctp mlx4_en [...]
        [  534.939704] Call Trace:
        [  534.951833]  [<ffffffff81294e30>] ? crypto_init_shash_ops+0x60/0xf0
        [  534.984213]  [<ffffffff81294e30>] crypto_init_shash_ops+0x60/0xf0
        [  535.015025]  [<ffffffff8128c8ed>] __crypto_alloc_tfm+0x6d/0x170
        [  535.045661]  [<ffffffff8128d12c>] crypto_alloc_base+0x4c/0xb0
        [  535.074593]  [<ffffffff8160bd42>] ? _raw_spin_lock_bh+0x12/0x50
        [  535.105239]  [<ffffffffa0418c11>] sctp_inet_listen+0x161/0x1e0 [sctp]
        [  535.138606]  [<ffffffff814e43bd>] SyS_listen+0x9d/0xb0
        [  535.166848]  [<ffffffff816149a9>] system_call_fastpath+0x16/0x1b
    
        ... or depending on the the application, for example this one:
    
        [ 1370.026490] BUG: unable to handle kernel paging request at 00000000ffffffff
        [ 1370.026506] IP: [<ffffffff811ab455>] kmem_cache_alloc+0x75/0x1d0
        [ 1370.054568] PGD 633c94067 PUD 0
        [ 1370.070446] Oops: 0000 [#1] SMP
        [ 1370.085010] Modules linked in: sctp kvm_amd kvm [...]
        [ 1370.963431] Call Trace:
        [ 1370.974632]  [<ffffffff8120f7cf>] ? SyS_epoll_ctl+0x53f/0x960
        [ 1371.000863]  [<ffffffff8120f7cf>] SyS_epoll_ctl+0x53f/0x960
        [ 1371.027154]  [<ffffffff812100d3>] ? anon_inode_getfile+0xd3/0x170
        [ 1371.054679]  [<ffffffff811e3d67>] ? __alloc_fd+0xa7/0x130
        [ 1371.080183]  [<ffffffff816149a9>] system_call_fastpath+0x16/0x1b
    
        With slab debugging enabled, we can see that the poison has been overwritten:
    
        [  669.826368] BUG kmalloc-128 (Tainted: G        W     ): Poison overwritten
        [  669.826385] INFO: 0xffff880228b32e50-0xffff880228b32e50. First byte 0x6a instead of 0x6b
        [  669.826414] INFO: Allocated in sctp_auth_create_key+0x23/0x50 [sctp] age=3 cpu=0 pid=18494
        [  669.826424]  __slab_alloc+0x4bf/0x566
        [  669.826433]  __kmalloc+0x280/0x310
        [  669.826453]  sctp_auth_create_key+0x23/0x50 [sctp]
        [  669.826471]  sctp_auth_asoc_create_secret+0xcb/0x1e0 [sctp]
        [  669.826488]  sctp_auth_asoc_init_active_key+0x68/0xa0 [sctp]
        [  669.826505]  sctp_do_sm+0x29d/0x17c0 [sctp] [...]
        [  669.826629] INFO: Freed in kzfree+0x31/0x40 age=1 cpu=0 pid=18494
        [  669.826635]  __slab_free+0x39/0x2a8
        [  669.826643]  kfree+0x1d6/0x230
        [  669.826650]  kzfree+0x31/0x40
        [  669.826666]  sctp_auth_key_put+0x19/0x20 [sctp]
        [  669.826681]  sctp_assoc_update+0x1ee/0x2d0 [sctp]
        [  669.826695]  sctp_do_sm+0x674/0x17c0 [sctp]
    
        Since this only triggers in some collision-cases with AUTH, the problem at
        heart is that sctp_auth_key_put() on asoc->asoc_shared_key is called twice
        when having refcnt 1, once directly in sctp_assoc_update() and yet again
        from within sctp_auth_asoc_init_active_key() via sctp_assoc_update() on
        the already kzfree'd memory, which is also consistent with the observation
        of the poison decrease from 0x6b to 0x6a (note: the overwrite is detected
        at a later point in time when poison is checked on new allocation).
    
        Reference counting of auth keys revisited:
    
        Shared keys for AUTH chunks are being stored in endpoints and associations
        in endpoint_shared_keys list. On endpoint creation, a null key is being
        added; on association creation, all endpoint shared keys are being cached
        and thus cloned over to the association. struct sctp_shared_key only holds
        a pointer to the actual key bytes, that is, struct sctp_auth_bytes which
        keeps track of users internally through refcounting. Naturally, on assoc
        or enpoint destruction, sctp_shared_key are being destroyed directly and
        the reference on sctp_auth_bytes dropped.
    
        User space can add keys to either list via setsockopt(2) through struct
        sctp_authkey and by passing that to sctp_auth_set_key() which replaces or
        adds a new auth key. There, sctp_auth_create_key() creates a new sctp_auth_bytes
        with refcount 1 and in case of replacement drops the reference on the old
        sctp_auth_bytes. A key can be set active from user space through setsockopt()
        on the id via sctp_auth_set_active_key(), which iterates through either
        endpoint_shared_keys and in case of an assoc, invokes (one of various places)
        sctp_auth_asoc_init_active_key().
    
        sctp_auth_asoc_init_active_key() computes the actual secret from local's
        and peer's random, hmac and shared key parameters and returns a new key
        directly as sctp_auth_bytes, that is asoc->asoc_shared_key, plus drops
        the reference if there was a previous one. The secret, which where we
        eventually double drop the ref comes from sctp_auth_asoc_set_secret() with
        intitial refcount of 1, which also stays unchanged eventually in
        sctp_assoc_update(). This key is later being used for crypto layer to
        set the key for the hash in crypto_hash_setkey() from sctp_auth_calculate_hmac().
    
        To close the loop: asoc->asoc_shared_key is freshly allocated secret
        material and independant of the sctp_shared_key management keeping track
        of only shared keys in endpoints and assocs. Hence, also commit 4184b2a79a76
        ("net: sctp: fix memory leak in auth key management") is independant of
        this bug here since it concerns a different layer (though same structures
        being used eventually). asoc->asoc_shared_key is reference dropped correctly
        on assoc destruction in sctp_association_free() and when active keys are
        being replaced in sctp_auth_asoc_init_active_key(), it always has a refcount
        of 1. Hence, it's freed prematurely in sctp_assoc_update(). Simple fix is
        to remove that sctp_auth_key_put() from there which fixes these panics.
    
        Fixes: 730fc3d05cd4 ("[SCTP]: Implete SCTP-AUTH parameter processing")
        Signed-off-by: Daniel Borkmann <dborkman@redhat.com>
        Acked-by: Vlad Yasevich <vyasevich@gmail.com>
        Acked-by: Neil Horman <nhorman@tuxdriver.com>
        Signed-off-by: David S. Miller <davem@davemloft.net>
    
     net/sctp/associola.c |    1 -
     1 files changed, 0 insertions(+), 1 deletions(-)
    
    commit 1301d30706a83116f0c74659878d291fc6608a0e
    Author: subashab@codeaurora.org <subashab@codeaurora.org>
    Date:   Fri Jan 23 22:26:02 2015 +0000
    
        ping: Fix race in free in receive path
    
        An exception is seen in ICMP ping receive path where the skb
        destructor sock_rfree() tries to access a freed socket. This happens
        because ping_rcv() releases socket reference with sock_put() and this
        internally frees up the socket. Later icmp_rcv() will try to free the
        skb and as part of this, skb destructor is called and which leads
        to a kernel panic as the socket is freed already in ping_rcv().
    
        -->|exception
        -007|sk_mem_uncharge
        -007|sock_rfree
        -008|skb_release_head_state
        -009|skb_release_all
        -009|__kfree_skb
        -010|kfree_skb
        -011|icmp_rcv
        -012|ip_local_deliver_finish
    
        Fix this incorrect free by cloning this skb and processing this cloned
        skb instead.
    
        This patch was suggested by Eric Dumazet
    
        Signed-off-by: Subash Abhinov Kasiviswanathan <subashab@codeaurora.org>
        Cc: Eric Dumazet <edumazet@google.com>
        Signed-off-by: Eric Dumazet <edumazet@google.com>
        Signed-off-by: David S. Miller <davem@davemloft.net>
    
     net/ipv4/ping.c |    5 ++++-
     1 files changed, 4 insertions(+), 1 deletions(-)
    
    commit 27e359f049d7304d2e306ca97db6e89eb1255586
    Author: Brad Spengler <spender@grsecurity.net>
    Date:   Tue Jan 27 18:31:25 2015 -0500
    
        Make it easier to work with inherited subjects that change roles.
        If a subject of the same name of the current inherited subject
        exists in the role being changed to with a similar object
        in it that would trigger inheritance on execution of the current process'
        binary, then we'll use that subject instead of the normal one obtained
        through lookup.
    
        See:
        https://forums.grsecurity.net/viewtopic.php?f=3&t=4129
    
        Conflicts:
    
            grsecurity/gracl_policy.c
    
     grsecurity/gracl.c        |   42 ++++++++++++++++++++++++++++++------------
     grsecurity/gracl_policy.c |   10 +++++-----
     2 files changed, 35 insertions(+), 17 deletions(-)
    
    Signature-tree: 2df475a22b69620a8043ce0fb9831ebd855e6e08

diff --git a/arch/x86/include/asm/desc.h b/arch/x86/include/asm/desc.h
index 2be7ac3f204e..e0fb1f604b18 100644
--- a/arch/x86/include/asm/desc.h
+++ b/arch/x86/include/asm/desc.h
@@ -261,7 +261,8 @@ static inline void native_load_tls(struct thread_struct *t, unsigned int cpu)
 	pax_close_kernel();
 }
 
-#define _LDT_empty(info)				\
+/* This intentionally ignores lm, since 32-bit apps don't have that field. */
+#define LDT_empty(info)					\
 	((info)->base_addr		== 0	&&	\
 	 (info)->limit			== 0	&&	\
 	 (info)->contents		== 0	&&	\
@@ -271,11 +272,18 @@ static inline void native_load_tls(struct thread_struct *t, unsigned int cpu)
 	 (info)->seg_not_present	== 1	&&	\
 	 (info)->useable		== 0)
 
-#ifdef CONFIG_X86_64
-#define LDT_empty(info) (_LDT_empty(info) && ((info)->lm == 0))
-#else
-#define LDT_empty(info) (_LDT_empty(info))
-#endif
+/* Lots of programs expect an all-zero user_desc to mean "no segment at all". */
+static inline bool LDT_zero(const struct user_desc *info)
+{
+	return (info->base_addr		== 0 &&
+		info->limit		== 0 &&
+		info->contents		== 0 &&
+		info->read_exec_only	== 0 &&
+		info->seg_32bit		== 0 &&
+		info->limit_in_pages	== 0 &&
+		info->seg_not_present	== 0 &&
+		info->useable		== 0);
+}
 
 static inline void clear_LDT(void)
 {
diff --git a/arch/x86/kernel/tls.c b/arch/x86/kernel/tls.c
index 36ed9559dc68..79ea0e3800e1 100644
--- a/arch/x86/kernel/tls.c
+++ b/arch/x86/kernel/tls.c
@@ -30,7 +30,28 @@ static int get_free_idx(void)
 
 static bool tls_desc_okay(const struct user_desc *info)
 {
-	if (LDT_empty(info))
+	/*
+	 * For historical reasons (i.e. no one ever documented how any
+	 * of the segmentation APIs work), user programs can and do
+	 * assume that a struct user_desc that's all zeros except for
+	 * entry_number means "no segment at all".  This never actually
+	 * worked.  In fact, up to Linux 3.19, a struct user_desc like
+	 * this would create a 16-bit read-write segment with base and
+	 * limit both equal to zero.
+	 *
+	 * That was close enough to "no segment at all" until we
+	 * hardened this function to disallow 16-bit TLS segments.  Fix
+	 * it up by interpreting these zeroed segments the way that they
+	 * were almost certainly intended to be interpreted.
+	 *
+	 * The correct way to ask for "no segment at all" is to specify
+	 * a user_desc that satisfies LDT_empty.  To keep everything
+	 * working, we accept both.
+	 *
+	 * Note that there's a similar kludge in modify_ldt -- look at
+	 * the distinction between modes 1 and 0x11.
+	 */
+	if (LDT_empty(info) || LDT_zero(info))
 		return true;
 
 	/*
@@ -72,7 +93,7 @@ static void set_tls_desc(struct task_struct *p, int idx,
 	cpu = get_cpu();
 
 	while (n-- > 0) {
-		if (LDT_empty(info))
+		if (LDT_empty(info) || LDT_zero(info))
 			desc->a = desc->b = 0;
 		else
 			fill_ldt(desc, info);
diff --git a/arch/x86/kvm/emulate.c b/arch/x86/kvm/emulate.c
index ea3fe9c6d978..39c366eab04b 100644
--- a/arch/x86/kvm/emulate.c
+++ b/arch/x86/kvm/emulate.c
@@ -2074,23 +2074,13 @@ static int em_sysenter(struct x86_emulate_ctxt *ctxt)
 	setup_syscalls_segments(ctxt, &cs, &ss);
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_CS, &msr_data);
-	switch (ctxt->mode) {
-	case X86EMUL_MODE_PROT32:
-		if ((msr_data & 0xfffc) == 0x0)
-			return emulate_gp(ctxt, 0);
-		break;
-	case X86EMUL_MODE_PROT64:
-		if (msr_data == 0x0)
-			return emulate_gp(ctxt, 0);
-		break;
-	}
+	if ((msr_data & 0xfffc) == 0x0)
+		return emulate_gp(ctxt, 0);
 
 	ctxt->eflags &= ~(EFLG_VM | EFLG_IF | EFLG_RF);
-	cs_sel = (u16)msr_data;
-	cs_sel &= ~SELECTOR_RPL_MASK;
+	cs_sel = (u16)msr_data & ~SELECTOR_RPL_MASK;
 	ss_sel = cs_sel + 8;
-	ss_sel &= ~SELECTOR_RPL_MASK;
-	if (ctxt->mode == X86EMUL_MODE_PROT64 || (efer & EFER_LMA)) {
+	if (efer & EFER_LMA) {
 		cs.d = 0;
 		cs.l = 1;
 	}
@@ -2099,10 +2089,11 @@ static int em_sysenter(struct x86_emulate_ctxt *ctxt)
 	ops->set_segment(ctxt, ss_sel, &ss, 0, VCPU_SREG_SS);
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_EIP, &msr_data);
-	ctxt->_eip = msr_data;
+	ctxt->_eip = (efer & EFER_LMA) ? msr_data : (u32)msr_data;
 
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_ESP, &msr_data);
-	ctxt->regs[VCPU_REGS_RSP] = msr_data;
+	ctxt->regs[VCPU_REGS_RSP] = (efer & EFER_LMA) ? msr_data :
+							(u32)msr_data;
 
 	return X86EMUL_CONTINUE;
 }
diff --git a/grsecurity/gracl.c b/grsecurity/gracl.c
index 0069a5966f99..99cbce0dbd36 100644
--- a/grsecurity/gracl.c
+++ b/grsecurity/gracl.c
@@ -1156,9 +1156,10 @@ gr_set_proc_res(struct task_struct *task)
 	rcu_read_lock();
 	read_lock(&tasklist_lock);
 	read_lock(&grsec_exec_file_lock);
+   except in the case of gr_set_role_label() (for __gr_get_subject_for_task)
 */
 
-struct acl_subject_label *__gr_get_subject_for_task(const struct gr_policy_state *state, struct task_struct *task, const char *filename)
+struct acl_subject_label *__gr_get_subject_for_task(const struct gr_policy_state *state, struct task_struct *task, const char *filename, int fallback)
 {
 	char *tmpname;
 	struct acl_subject_label *tmpsubj;
@@ -1200,15 +1201,15 @@ struct acl_subject_label *__gr_get_subject_for_task(const struct gr_policy_state
 	/* this also works for the reload case -- if we don't match a potentially inherited subject
 	   then we fall back to a normal lookup based on the binary's ino/dev
 	*/
-	if (tmpsubj == NULL)
+	if (tmpsubj == NULL && fallback)
 		tmpsubj = chk_subj_label(filp->f_path.dentry, filp->f_path.mnt, task->role);
 
 	return tmpsubj;
 }
 
-static struct acl_subject_label *gr_get_subject_for_task(struct task_struct *task, const char *filename)
+static struct acl_subject_label *gr_get_subject_for_task(struct task_struct *task, const char *filename, int fallback)
 {
-	return __gr_get_subject_for_task(&running_polstate, task, filename);
+	return __gr_get_subject_for_task(&running_polstate, task, filename, fallback);
 }
 
 void __gr_apply_subject_to_task(const struct gr_policy_state *state, struct task_struct *task, struct acl_subject_label *subj)
@@ -1272,7 +1273,7 @@ gr_search_file(const struct dentry * dentry, const __u32 mode,
 			task->role = current->role;
 			rcu_read_lock();
 			read_lock(&grsec_exec_file_lock);
-			subj = gr_get_subject_for_task(task, NULL);
+			subj = gr_get_subject_for_task(task, NULL, 1);
 			gr_apply_subject_to_task(task, subj);
 			read_unlock(&grsec_exec_file_lock);
 			rcu_read_unlock();
@@ -1652,6 +1653,7 @@ void
 gr_set_role_label(struct task_struct *task, const uid_t uid, const uid_t gid)
 {
 	struct acl_role_label *role = task->role;
+	struct acl_role_label *origrole = role;
 	struct acl_subject_label *subj = NULL;
 	struct acl_object_label *obj;
 	struct file *filp;
@@ -1679,10 +1681,28 @@ gr_set_role_label(struct task_struct *task, const uid_t uid, const uid_t gid)
 	     ((role->roletype & GR_ROLE_GROUP) && !gr_acl_is_capable(CAP_SETGID))))
 		return;
 
-	/* perform subject lookup in possibly new role
-	   we can use this result below in the case where role == task->role
-	*/
-	subj = chk_subj_label(filp->f_path.dentry, filp->f_path.mnt, role);
+	task->role = role;
+
+	if (task->inherited) {
+		/* if we reached our subject through inheritance, then first see
+		   if there's a subject of the same name in the new role that has
+		   an object that would result in the same inherited subject
+		*/
+		subj = gr_get_subject_for_task(task, task->acl->filename, 0);
+		if (subj) {
+			obj = chk_obj_label(filp->f_path.dentry, filp->f_path.mnt, subj);
+			if (!(obj->mode & GR_INHERIT))
+				subj = NULL;
+		}
+		
+	}
+	if (subj == NULL) {
+		/* otherwise:
+		   perform subject lookup in possibly new role
+		   we can use this result below in the case where role == task->role
+		*/
+		subj = chk_subj_label(filp->f_path.dentry, filp->f_path.mnt, role);
+	}
 
 	/* if we changed uid/gid, but result in the same role
 	   and are using inheritance, don't lose the inherited subject
@@ -1690,14 +1710,12 @@ gr_set_role_label(struct task_struct *task, const uid_t uid, const uid_t gid)
 	   would result in, we arrived via inheritance, don't
 	   lose subject
 	*/
-	if (role != task->role || (!(task->acl->mode & GR_INHERITLEARN) &&
+	if (role != origrole || (!(task->acl->mode & GR_INHERITLEARN) &&
 				   (subj == task->acl)))
 		task->acl = subj;
 
 	/* leave task->inherited unaffected */
 
-	task->role = role;
-
 	task->is_writable = 0;
 
 	/* ignore additional mmap checks for processes that are writable 
diff --git a/grsecurity/gracl_policy.c b/grsecurity/gracl_policy.c
index 3768798aca5e..94ef7e60199e 100644
--- a/grsecurity/gracl_policy.c
+++ b/grsecurity/gracl_policy.c
@@ -67,7 +67,7 @@ extern void gr_free_uidset(void);
 extern void gr_remove_uid(uid_t uid);
 extern int gr_find_uid(uid_t uid);
 
-extern struct acl_subject_label *__gr_get_subject_for_task(const struct gr_policy_state *state, struct task_struct *task, const char *filename);
+extern struct acl_subject_label *__gr_get_subject_for_task(const struct gr_policy_state *state, struct task_struct *task, const char *filename, int fallback);
 extern void __gr_apply_subject_to_task(struct gr_policy_state *state, struct task_struct *task, struct acl_subject_label *subj);
 extern int gr_streq(const char *a, const char *b, const unsigned int lena, const unsigned int lenb);
 extern void __insert_inodev_entry(const struct gr_policy_state *state, struct inodev_entry *entry);
@@ -1172,8 +1172,8 @@ static int gracl_reload_apply_policies(void *reload)
 		}
 		/* this handles non-nested inherited subjects, nested subjects will still
 		   be dropped currently */
-		subj = __gr_get_subject_for_task(polstate, task, task->acl->filename);
-		task->tmpacl = __gr_get_subject_for_task(polstate, task, NULL);
+		subj = __gr_get_subject_for_task(polstate, task, task->acl->filename, 1);
+		task->tmpacl = __gr_get_subject_for_task(polstate, task, NULL, 1);
 		/* change the role back so that we've made no modifications to the policy */
 		task->role = rtmp;
 
@@ -1205,7 +1205,7 @@ static int gracl_reload_apply_policies(void *reload)
 			/* this handles non-nested inherited subjects, nested subjects will still
 			   be dropped currently */
 			if (!reload_state->oldmode && task->inherited)
-				subj = __gr_get_subject_for_task(polstate, task, task->acl->filename);
+				subj = __gr_get_subject_for_task(polstate, task, task->acl->filename, 1);
 			else {
 				/* looked up and tagged to the task previously */
 				subj = task->tmpacl;
@@ -1754,7 +1754,7 @@ gr_set_acls(const int type)
 		if (task->exec_file) {
 			cred = __task_cred(task);
 			task->role = __lookup_acl_role_label(polstate, task, cred->uid, cred->gid);
-			subj = __gr_get_subject_for_task(polstate, task, NULL);
+			subj = __gr_get_subject_for_task(polstate, task, NULL, 1);
 			if (subj == NULL) {
 				ret = -EINVAL;
 				read_unlock(&grsec_exec_file_lock);
diff --git a/net/ipv4/ping.c b/net/ipv4/ping.c
index c95851fbd03f..db46e69e3b43 100644
--- a/net/ipv4/ping.c
+++ b/net/ipv4/ping.c
@@ -716,8 +716,11 @@ void ping_rcv(struct sk_buff *skb)
 	sk = ping_v4_lookup(net, saddr, daddr, ntohs(icmph->un.echo.id),
 			    skb->dev->ifindex);
 	if (sk != NULL) {
+		struct sk_buff *skb2 = skb_clone(skb, GFP_ATOMIC);
+
 		pr_debug("rcv on socket %p\n", sk);
-		ping_queue_rcv_skb(sk, skb_get(skb));
+		if (skb2)
+			ping_queue_rcv_skb(sk, skb2);
 		sock_put(sk);
 		return;
 	}
diff --git a/net/sctp/associola.c b/net/sctp/associola.c
index 5b2d8e6f5568..d014b053ad26 100644
--- a/net/sctp/associola.c
+++ b/net/sctp/associola.c
@@ -1272,7 +1272,6 @@ void sctp_assoc_update(struct sctp_association *asoc,
 	asoc->peer.peer_hmacs = new->peer.peer_hmacs;
 	new->peer.peer_hmacs = NULL;
 
-	sctp_auth_key_put(asoc->asoc_shared_key);
 	sctp_auth_asoc_init_active_key(asoc, GFP_ATOMIC);
 }
 
