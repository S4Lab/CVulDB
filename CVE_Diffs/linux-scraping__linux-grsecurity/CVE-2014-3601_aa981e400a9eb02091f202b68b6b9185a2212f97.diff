linux-scraping__linux-grsecurity
commit aa981e400a9eb02091f202b68b6b9185a2212f97
Author:     Brad Spengler <spender@grsecurity.net>
AuthorDate: Sat Oct 25 00:28:44 2014 -0400
Commit:     Mickaël Salaün <mic@digikod.net>
CommitDate: Sat Oct 25 00:28:44 2014 -0400

    grsec: Apply grsecurity-3.0-3.17.1-201410250027.patch
    
    commit c1d74661db03224813ae4fe0ceb50854544ff75c
    Author: Nadav Amit <namit@cs.technion.ac.il>
    Date:   Mon Oct 13 13:04:14 2014 +0300
    
        KVM: x86: PREFETCH and HINT_NOP should have SrcMem flag
    
        The decode phase of the x86 emulator assumes that every instruction with the
        ModRM flag, and which can be used with RIP-relative addressing, has either
        SrcMem or DstMem.  This is not the case for several instructions - prefetch,
        hint-nop and clflush.
    
        Adding SrcMem|NoAccess for prefetch and hint-nop and SrcMem for clflush.
    
        This fixes CVE-2014-8480.
    
        Fixes: 41061cdb98a0bec464278b4db8e894a3121671f5
        Cc: stable@vger.kernel.org
        Signed-off-by: Nadav Amit <namit@cs.technion.ac.il>
        Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    
     arch/x86/kvm/emulate.c |    7 ++++---
     1 files changed, 4 insertions(+), 3 deletions(-)
    
    commit 225788f25d44957a8ccdcd4f26b0181eba694e2c
    Author: Nadav Amit <namit@cs.technion.ac.il>
    Date:   Mon Oct 13 13:04:13 2014 +0300
    
        KVM: x86: Emulator does not decode clflush well
    
        Currently, all group15 instructions are decoded as clflush (e.g., mfence,
        xsave).  In addition, the clflush instruction requires no prefix (66/f2/f3)
        would exist. If prefix exists it may encode a different instruction (e.g.,
        clflushopt).
    
        Creating a group for clflush, and different group for each prefix.
    
        This has been the case forever, but the next patch needs the cflush group
        in order to fix a bug introduced in 3.17.
    
        Fixes: 41061cdb98a0bec464278b4db8e894a3121671f5
        Cc: stable@vger.kernel.org
        Signed-off-by: Nadav Amit <namit@cs.technion.ac.il>
        Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    
     arch/x86/kvm/emulate.c |   20 +++++++++++++++++---
     1 files changed, 17 insertions(+), 3 deletions(-)
    
    commit cce8ff8164083a23c507ee339eb5ed956723837f
    Author: Nadav Amit <namit@cs.technion.ac.il>
    Date:   Thu Sep 18 22:39:38 2014 +0300
    
        KVM: x86: Emulator fixes for eip canonical checks on near branches
    
        Before changing rip (during jmp, call, ret, etc.) the target should be asserted
        to be canonical one, as real CPUs do.  During sysret, both target rsp and rip
        should be canonical. If any of these values is noncanonical, a #GP exception
        should occur.  The exception to this rule are syscall and sysenter instructions
        in which the assigned rip is checked during the assignment to the relevant
        MSRs.
    
        This patch fixes the emulator to behave as real CPUs do for near branches.
        Far branches are handled by the next patch.
    
        This fixes CVE-2014-3647.
    
        Cc: stable@vger.kernel.org
        Signed-off-by: Nadav Amit <namit@cs.technion.ac.il>
        Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    
     arch/x86/kvm/emulate.c |   78 +++++++++++++++++++++++++++++++++---------------
     1 files changed, 54 insertions(+), 24 deletions(-)
    
    commit 2cd6c77a29a3c28c67f1bddd4e72d741cea6da50
    Author: Andy Honig <ahonig@google.com>
    Date:   Wed Aug 27 11:16:44 2014 -0700
    
        KVM: x86: Prevent host from panicking on shared MSR writes.
    
        The previous patch blocked invalid writes directly when the MSR
        is written.  As a precaution, prevent future similar mistakes by
        gracefulling handle GPs caused by writes to shared MSRs.
    
        Cc: stable@vger.kernel.org
        Signed-off-by: Andrew Honig <ahonig@google.com>
        [Remove parts obsoleted by Nadav's patch. - Paolo]
        Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    
     arch/x86/include/asm/kvm_host.h |    2 +-
     arch/x86/kvm/vmx.c              |    7 +++++--
     arch/x86/kvm/x86.c              |   11 ++++++++---
     3 files changed, 14 insertions(+), 6 deletions(-)
    
    commit a86abc5d1f3999cc30e4b6ca9a4d06897f972d4f
    Author: Nadav Amit <namit@cs.technion.ac.il>
    Date:   Tue Sep 16 03:24:05 2014 +0300
    
        KVM: x86: Check non-canonical addresses upon WRMSR
    
        Upon WRMSR, the CPU should inject #GP if a non-canonical value (address) is
        written to certain MSRs. The behavior is "almost" identical for AMD and Intel
        (ignoring MSRs that are not implemented in either architecture since they would
        anyhow #GP). However, IA32_SYSENTER_ESP and IA32_SYSENTER_EIP cause #GP if
        non-canonical address is written on Intel but not on AMD (which ignores the top
        32-bits).
    
        Accordingly, this patch injects a #GP on the MSRs which behave identically on
        Intel and AMD.  To eliminate the differences between the architecutres, the
        value which is written to IA32_SYSENTER_ESP and IA32_SYSENTER_EIP is turned to
        canonical value before writing instead of injecting a #GP.
    
        Some references from Intel and AMD manuals:
    
        According to Intel SDM description of WRMSR instruction #GP is expected on
        WRMSR "If the source register contains a non-canonical address and ECX
        specifies one of the following MSRs: IA32_DS_AREA, IA32_FS_BASE, IA32_GS_BASE,
        IA32_KERNEL_GS_BASE, IA32_LSTAR, IA32_SYSENTER_EIP, IA32_SYSENTER_ESP."
    
        According to AMD manual instruction manual:
        LSTAR/CSTAR (SYSCALL): "The WRMSR instruction loads the target RIP into the
        LSTAR and CSTAR registers.  If an RIP written by WRMSR is not in canonical
        form, a general-protection exception (#GP) occurs."
        IA32_GS_BASE and IA32_FS_BASE (WRFSBASE/WRGSBASE): "The address written to the
        base field must be in canonical form or a #GP fault will occur."
        IA32_KERNEL_GS_BASE (SWAPGS): "The address stored in the KernelGSbase MSR must
        be in canonical form."
    
        This patch fixes CVE-2014-3610.
    
        Cc: stable@vger.kernel.org
        Signed-off-by: Nadav Amit <namit@cs.technion.ac.il>
        Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    
     arch/x86/include/asm/kvm_host.h |   14 ++++++++++++++
     arch/x86/kvm/svm.c              |    2 +-
     arch/x86/kvm/vmx.c              |    2 +-
     arch/x86/kvm/x86.c              |   27 ++++++++++++++++++++++++++-
     4 files changed, 42 insertions(+), 3 deletions(-)
    
    commit e4b285e5033b2db2b7879167cfaaf77ecb207213
    Author: Andy Honig <ahonig@google.com>
    Date:   Wed Aug 27 14:42:54 2014 -0700
    
        KVM: x86: Improve thread safety in pit
    
        There's a race condition in the PIT emulation code in KVM.  In
        __kvm_migrate_pit_timer the pit_timer object is accessed without
        synchronization.  If the race condition occurs at the wrong time this
        can crash the host kernel.
    
        This fixes CVE-2014-3611.
    
        Cc: stable@vger.kernel.org
        Signed-off-by: Andrew Honig <ahonig@google.com>
        Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    
     arch/x86/kvm/i8254.c |    2 ++
     1 files changed, 2 insertions(+), 0 deletions(-)
    
    commit 7fef9c4865e7e1f10eedac13a29e86b3968effd8
    Author: Nadav Amit <namit@cs.technion.ac.il>
    Date:   Thu Sep 18 22:39:37 2014 +0300
    
        KVM: x86: Fix wrong masking on relative jump/call
    
        Relative jumps and calls do the masking according to the operand size, and not
        according to the address size as the KVM emulator does today.
    
        This patch fixes KVM behavior.
    
        Cc: stable@vger.kernel.org
        Signed-off-by: Nadav Amit <namit@cs.technion.ac.il>
        Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    
     arch/x86/kvm/emulate.c |   27 ++++++++++++++++++++++-----
     1 files changed, 22 insertions(+), 5 deletions(-)
    
    commit 3522a323e3f9379e23628b4023f8eb237fa94f96
    Author: Nadav Amit <namit@cs.technion.ac.il>
    Date:   Thu Sep 18 22:39:39 2014 +0300
    
        KVM: x86: Handle errors when RIP is set during far jumps
    
        Far jmp/call/ret may fault while loading a new RIP.  Currently KVM does not
        handle this case, and may result in failed vm-entry once the assignment is
        done.  The tricky part of doing so is that loading the new CS affects the
        VMCS/VMCB state, so if we fail during loading the new RIP, we are left in
        unconsistent state.  Therefore, this patch saves on 64-bit the old CS
        descriptor and restores it if loading RIP failed.
    
        This fixes CVE-2014-3647.
    
        Cc: stable@vger.kernel.org
        Signed-off-by: Nadav Amit <namit@cs.technion.ac.il>
        Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    
     arch/x86/kvm/emulate.c |  118 +++++++++++++++++++++++++++++++++++------------
     1 files changed, 88 insertions(+), 30 deletions(-)
    
    commit 992d1e8ef00f79b71ebd1757d3564601d3e0cbe1
    Author: Petr Matousek <pmatouse@redhat.com>
    Date:   Tue Sep 23 20:22:30 2014 +0200
    
        kvm: vmx: handle invvpid vm exit gracefully
    
        On systems with invvpid instruction support (corresponding bit in
        IA32_VMX_EPT_VPID_CAP MSR is set) guest invocation of invvpid
        causes vm exit, which is currently not handled and results in
        propagation of unknown exit to userspace.
    
        Fix this by installing an invvpid vm exit handler.
    
        This is CVE-2014-3646.
    
        Cc: stable@vger.kernel.org
        Signed-off-by: Petr Matousek <pmatouse@redhat.com>
        Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    
     arch/x86/include/uapi/asm/vmx.h |    2 ++
     arch/x86/kvm/vmx.c              |    9 ++++++++-
     2 files changed, 10 insertions(+), 1 deletions(-)
    
    commit a5d8d5cdec7f5389cd74d1671ce3ba0613a0f19c
    Author: Michael S. Tsirkin <mst@redhat.com>
    Date:   Thu Sep 18 16:21:16 2014 +0300
    
        kvm: x86: don't kill guest on unknown exit reason
    
        KVM_EXIT_UNKNOWN is a kvm bug, we don't really know whether it was
        triggered by a priveledged application.  Let's not kill the guest: WARN
        and inject #UD instead.
    
        Cc: stable@vger.kernel.org
        Signed-off-by: Michael S. Tsirkin <mst@redhat.com>
        Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    
     arch/x86/kvm/svm.c |    6 +++---
     arch/x86/kvm/vmx.c |    6 +++---
     2 files changed, 6 insertions(+), 6 deletions(-)
    
    commit c18125dc3007b407b1c1ee6f63b70fa334cdaa4f
    Author: Nadav Amit <namit@cs.technion.ac.il>
    Date:   Fri Oct 3 01:10:04 2014 +0300
    
        KVM: x86: Decoding guest instructions which cross page boundary may fail
    
        Once an instruction crosses a page boundary, the size read from the second page
        disregards the common case that part of the operand resides on the first page.
        As a result, fetch of long insturctions may fail, and thereby cause the
        decoding to fail as well.
    
        Cc: stable@vger.kernel.org
        Fixes: 5cfc7e0f5e5e1adf998df94f8e36edaf5d30d38e
        Signed-off-by: Nadav Amit <namit@cs.technion.ac.il>
        Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    
     arch/x86/kvm/emulate.c |    6 ++++--
     1 files changed, 4 insertions(+), 2 deletions(-)
    
    commit 52f3e9b00ed4a9a4242ce8da933a21e89b24d6e6
    Author: Paolo Bonzini <pbonzini@redhat.com>
    Date:   Thu Oct 23 14:54:14 2014 +0200
    
        KVM: emulate: avoid accessing NULL ctxt->memopp
    
        A failure to decode the instruction can cause a NULL pointer access.
        This is fixed simply by moving the "done" label as close as possible
        to the return.
    
        This fixes CVE-2014-8481.
    
        Reported-by: Andy Lutomirski <luto@amacapital.net>
        Cc: stable@vger.kernel.org
        Fixes: 41061cdb98a0bec464278b4db8e894a3121671f5
        Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    
     arch/x86/kvm/emulate.c |    2 +-
     1 files changed, 1 insertions(+), 1 deletions(-)
    
    commit e2b6116304cd789559f0838da870dd9b5bebd92b
    Author: Quentin Casasnovas <quentin.casasnovas@oracle.com>
    Date:   Fri Oct 17 22:55:59 2014 +0200
    
        kvm: fix excessive pages un-pinning in kvm_iommu_map error path.
    
        The third parameter of kvm_unpin_pages() when called from
        kvm_iommu_map_pages() is wrong, it should be the number of pages to un-pin
        and not the page size.
    
        This error was facilitated with an inconsistent API: kvm_pin_pages() takes
        a size, but kvn_unpin_pages() takes a number of pages, so fix the problem
        by matching the two.
    
        This was introduced by commit 350b8bd ("kvm: iommu: fix the third parameter
        of kvm_iommu_put_pages (CVE-2014-3601)"), which fixes the lack of
        un-pinning for pages intended to be un-pinned (i.e. memory leak) but
        unfortunately potentially aggravated the number of pages we un-pin that
        should have stayed pinned. As far as I understand though, the same
        practical mitigations apply.
    
        This issue was found during review of Red Hat 6.6 patches to prepare
        Ksplice rebootless updates.
    
        Thanks to Vegard for his time on a late Friday evening to help me in
        understanding this code.
    
        Fixes: 350b8bd ("kvm: iommu: fix the third parameter of... (CVE-2014-3601)")
        Cc: stable@vger.kernel.org
        Signed-off-by: Quentin Casasnovas <quentin.casasnovas@oracle.com>
        Signed-off-by: Vegard Nossum <vegard.nossum@oracle.com>
        Signed-off-by: Jamie Iles <jamie.iles@oracle.com>
        Reviewed-by: Sasha Levin <sasha.levin@oracle.com>
        Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    
     virt/kvm/iommu.c |    8 ++++----
     1 files changed, 4 insertions(+), 4 deletions(-)
    
    commit a11f76d9ebd8f4c51cad55859c4542b9de506b78
    Author: David S. Miller <davem@davemloft.net>
    Date:   Thu Oct 23 12:58:13 2014 -0700
    
        sparc64: Fix register corruption in top-most kernel stack frame during boot.
    
        Meelis Roos reported that kernels built with gcc-4.9 do not boot, we
        eventually narrowed this down to only impacting machines using
        UltraSPARC-III and derivitive cpus.
    
        The crash happens right when the first user process is spawned:
    
        [   54.451346] Kernel panic - not syncing: Attempted to kill init! exitcode=0x00000004
        [   54.451346]
        [   54.571516] CPU: 1 PID: 1 Comm: init Not tainted 3.16.0-rc2-00211-gd7933ab #96
        [   54.666431] Call Trace:
        [   54.698453]  [0000000000762f8c] panic+0xb0/0x224
        [   54.759071]  [000000000045cf68] do_exit+0x948/0x960
        [   54.823123]  [000000000042cbc0] fault_in_user_windows+0xe0/0x100
        [   54.902036]  [0000000000404ad0] __handle_user_windows+0x0/0x10
        [   54.978662] Press Stop-A (L1-A) to return to the boot prom
        [   55.050713] ---[ end Kernel panic - not syncing: Attempted to kill init! exitcode=0x00000004
    
        Further investigation showed that compiling only per_cpu_patch() with
        an older compiler fixes the boot.
    
        Detailed analysis showed that the function is not being miscompiled by
        gcc-4.9, but it is using a different register allocation ordering.
    
        With the gcc-4.9 compiled function, something during the code patching
        causes some of the %i* input registers to get corrupted.  Perhaps
        we have a TLB miss path into the firmware that is deep enough to
        cause a register window spill and subsequent restore when we get
        back from the TLB miss trap.
    
        Let's plug this up by doing two things:
    
        1) Stop using the firmware stack for client interface calls into
           the firmware.  Just use the kernel's stack.
    
        2) As soon as we can, call into a new function "start_early_boot()"
           to put a one-register-window buffer between the firmware's
           deepest stack frame and the top-most initial kernel one.
    
        Reported-by: Meelis Roos <mroos@linux.ee>
        Tested-by: Meelis Roos <mroos@linux.ee>
        Signed-off-by: David S. Miller <davem@davemloft.net>
    
     arch/sparc/include/asm/oplib_64.h |    3 +-
     arch/sparc/include/asm/setup.h    |    2 +
     arch/sparc/kernel/entry.h         |    3 --
     arch/sparc/kernel/head_64.S       |   40 +++---------------------------------
     arch/sparc/kernel/hvtramp.S       |    1 -
     arch/sparc/kernel/setup_64.c      |   28 ++++++++++++++++++-------
     arch/sparc/kernel/trampoline_64.S |   12 ++++++----
     arch/sparc/prom/cif.S             |    5 +--
     arch/sparc/prom/init_64.c         |    6 ++--
     arch/sparc/prom/p1275.c           |    2 -
     10 files changed, 40 insertions(+), 62 deletions(-)
    
    commit 6b2ca0ac2281209d949162cc39d642ccb4949240
    Author: David S. Miller <davem@davemloft.net>
    Date:   Fri Oct 24 09:59:02 2014 -0700
    
        sparc64: Implement __get_user_pages_fast().
    
        It is not sufficient to only implement get_user_pages_fast(), you
        must also implement the atomic version __get_user_pages_fast()
        otherwise you end up using the weak symbol fallback implementation
        which simply returns zero.
    
        This is dangerous, because it causes the futex code to loop forever
        if transparent hugepages are supported (see get_futex_key()).
    
        Signed-off-by: David S. Miller <davem@davemloft.net>
    
     arch/sparc/mm/gup.c |   30 ++++++++++++++++++++++++++++++
     1 files changed, 30 insertions(+), 0 deletions(-)
    
    commit 0577b0536cd8f7906989aae6928053f8b490cae8
    Author: Bjorn Helgaas <bhelgaas@google.com>
    Date:   Mon Oct 13 18:58:48 2014 -0600
    
        audit: Remove "weak" from audit_classify_compat_syscall() declaration
    
        There's only one audit_classify_compat_syscall() definition, so it doesn't
        need to be weak.
    
        Remove the "weak" attribute from the audit_classify_compat_syscall()
        declaration.
    
        Signed-off-by: Bjorn Helgaas <bhelgaas@google.com>
        Acked-by: Richard Guy Briggs <rgb@redhat.com>
        CC: AKASHI Takahiro <takahiro.akashi@linaro.org>
    
     include/linux/audit.h |    2 +-
     1 files changed, 1 insertions(+), 1 deletions(-)
    
    commit 265e2a5de9955c87481f70b04936d0df5bff96c8
    Author: Bjorn Helgaas <bhelgaas@google.com>
    Date:   Mon Oct 13 18:01:34 2014 -0600
    
        x86, intel-mid: Remove "weak" from function declarations
    
        For the following interfaces:
    
          get_penwell_ops()
          get_cloverview_ops()
          get_tangier_ops()
    
        there is only one implementation, so they do not need to be marked "weak".
    
        Remove the "weak" attribute from their declarations.
    
        Signed-off-by: Bjorn Helgaas <bhelgaas@google.com>
        Acked-by: Ingo Molnar <mingo@kernel.org>
        CC: David Cohen <david.a.cohen@linux.intel.com>
        CC: Kuppuswamy Sathyanarayanan <sathyanarayanan.kuppuswamy@linux.intel.com>
        CC: x86@kernel.org
        Conflicts:
    
            arch/x86/platform/intel-mid/intel_mid_weak_decls.h
    
     arch/x86/platform/intel-mid/intel_mid_weak_decls.h |    7 +++----
     1 files changed, 3 insertions(+), 4 deletions(-)
    
    commit ed16dce6a39eadbdb4bf602838e1f4b7babdd2c0
    Author: Bjorn Helgaas <bhelgaas@google.com>
    Date:   Mon Oct 13 18:59:09 2014 -0600
    
        clocksource: Remove "weak" from clocksource_default_clock() declaration
    
        kernel/time/jiffies.c provides a default clocksource_default_clock()
        definition explicitly marked "weak".  arch/s390 provides its own definition
        intended to override the default, but the "weak" attribute on the
        declaration applied to the s390 definition as well, so the linker chose one
        based on link order (see 10629d711ed7 ("PCI: Remove __weak annotation from
        pcibios_get_phb_of_node decl")).
    
        Remove the "weak" attribute from the clocksource_default_clock()
        declaration so we always prefer a non-weak definition over the weak one,
        independent of link order.
    
        Fixes: f1b82746c1e9 ("clocksource: Cleanup clocksource selection")
        Signed-off-by: Bjorn Helgaas <bhelgaas@google.com>
        Acked-by: John Stultz <john.stultz@linaro.org>
        Acked-by: Ingo Molnar <mingo@kernel.org>
        CC: Daniel Lezcano <daniel.lezcano@linaro.org>
        CC: Martin Schwidefsky <schwidefsky@de.ibm.com>
    
     include/linux/clocksource.h |    2 +-
     1 files changed, 1 insertions(+), 1 deletions(-)
    
    commit 7ca601442b56db657603588fc993ef9c1b5dd764
    Author: Bjorn Helgaas <bhelgaas@google.com>
    Date:   Mon Oct 13 18:59:41 2014 -0600
    
        vmcore: Remove "weak" from function declarations
    
        For the following functions:
    
          elfcorehdr_alloc()
          elfcorehdr_free()
          elfcorehdr_read()
          elfcorehdr_read_notes()
          remap_oldmem_pfn_range()
    
        fs/proc/vmcore.c provides default definitions explicitly marked "weak".
        arch/s390 provides its own definitions intended to override the default
        ones, but the "weak" attribute on the declarations applied to the s390
        definitions as well, so the linker chose one based on link order (see
        10629d711ed7 ("PCI: Remove __weak annotation from pcibios_get_phb_of_node
        decl")).
    
        Remove the "weak" attribute from the declarations so we always prefer a
        non-weak definition over the weak one, independent of link order.
    
        Fixes: be8a8d069e50 ("vmcore: introduce ELF header in new memory feature")
        Fixes: 9cb218131de1 ("vmcore: introduce remap_oldmem_pfn_range()")
        Signed-off-by: Bjorn Helgaas <bhelgaas@google.com>
        Acked-by: Andrew Morton <akpm@linux-foundation.org>
        Acked-by: Vivek Goyal <vgoyal@redhat.com>
        CC: Michael Holzheu <holzheu@linux.vnet.ibm.com>
    
     include/linux/crash_dump.h |   15 +++++++--------
     1 files changed, 7 insertions(+), 8 deletions(-)
    
    commit bd8d09cc9bbe817d7c7983e3490ed1ad2cda7b98
    Author: Vineet Gupta <vgupta@synopsys.com>
    Date:   Mon Oct 20 10:17:04 2014 -0600
    
        ARC: kgdb: generic kgdb_arch_pc() suffices
    
        The ARC version of kgdb_arch_pc() is identical to the generic version in
        kernel/debug/debug_core.c.  Drop the ARC version so we use the generic one.
    
        Signed-off-by: Vineet Gupta <vgupta@synopsys.com>
        Signed-off-by: Bjorn Helgaas <bhelgaas@google.com>
    
     arch/arc/kernel/kgdb.c |    5 -----
     1 files changed, 0 insertions(+), 5 deletions(-)
    
    commit 4e65950187eadf8a03e2746ef1c2696c20804d1d
    Author: Bjorn Helgaas <bhelgaas@google.com>
    Date:   Mon Oct 13 19:00:25 2014 -0600
    
        kgdb: Remove "weak" from kgdb_arch_pc() declaration
    
        kernel/debug/debug_core.c provides a default kgdb_arch_pc() definition
        explicitly marked "weak".  Several architectures provide their own
        definitions intended to override the default, but the "weak" attribute on
        the declaration applied to the arch definitions as well, so the linker
        chose one based on link order (see 10629d711ed7 ("PCI: Remove __weak
        annotation from pcibios_get_phb_of_node decl")).
    
        Remove the "weak" attribute from the declaration so we always prefer a
        non-weak definition over the weak one, independent of link order.
    
        Fixes: 688b744d8bc8 ("kgdb: fix signedness mixmatches, add statics, add declaration to header")
        Tested-by: Vineet Gupta <vgupta@synopsys.com>       # for ARC build
        Signed-off-by: Bjorn Helgaas <bhelgaas@google.com>
        Reviewed-by: Harvey Harrison <harvey.harrison@gmail.com>
    
     include/linux/kgdb.h |    2 +-
     1 files changed, 1 insertions(+), 1 deletions(-)
    
    commit 48da6eae4e606af4f3280270e1c0fbf55e073d82
    Author: Bjorn Helgaas <bhelgaas@google.com>
    Date:   Mon Oct 13 19:00:47 2014 -0600
    
        memory-hotplug: Remove "weak" from memory_block_size_bytes() declaration
    
        drivers/base/memory.c provides a default memory_block_size_bytes()
        definition explicitly marked "weak".  Several architectures provide their
        own definitions intended to override the default, but the "weak" attribute
        on the declaration applied to the arch definitions as well, so the linker
        chose one based on link order (see 10629d711ed7 ("PCI: Remove __weak
        annotation from pcibios_get_phb_of_node decl")).
    
        Remove the "weak" attribute from the declaration so we always prefer a
        non-weak definition over the weak one, independent of link order.
    
        Fixes: 41f107266b19 ("drivers: base: Add prototype declaration to the header file")
        Signed-off-by: Bjorn Helgaas <bhelgaas@google.com>
        Acked-by: Andrew Morton <akpm@linux-foundation.org>
        CC: Rashika Kheria <rashika.kheria@gmail.com>
        CC: Nathan Fontenot <nfont@austin.ibm.com>
        CC: Anton Blanchard <anton@au1.ibm.com>
        CC: Heiko Carstens <heiko.carstens@de.ibm.com>
        CC: Yinghai Lu <yinghai@kernel.org>
    
     include/linux/memory.h |    2 +-
     1 files changed, 1 insertions(+), 1 deletions(-)
    
    commit 913d5138eb80aa9f6166dd954cc425c7b45be60e
    Author: Bjorn Helgaas <bhelgaas@google.com>
    Date:   Mon Oct 13 19:01:03 2014 -0600
    
        uprobes: Remove "weak" from function declarations
    
        For the following interfaces:
    
          set_swbp()
          set_orig_insn()
          is_swbp_insn()
          is_trap_insn()
          uprobe_get_swbp_addr()
          arch_uprobe_ignore()
          arch_uprobe_copy_ixol()
    
        kernel/events/uprobes.c provides default definitions explicitly marked
        "weak".  Some architectures provide their own definitions intended to
        override the defaults, but the "weak" attribute on the declarations applied
        to the arch definitions as well, so the linker chose one based on link
        order (see 10629d711ed7 ("PCI: Remove __weak annotation from
        pcibios_get_phb_of_node decl")).
    
        Remove the "weak" attribute from the declarations so we always prefer a
        non-weak definition over the weak one, independent of link order.
    
        Signed-off-by: Bjorn Helgaas <bhelgaas@google.com>
        Acked-by: Ingo Molnar <mingo@kernel.org>
        Acked-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
        CC: Victor Kamensky <victor.kamensky@linaro.org>
        CC: Oleg Nesterov <oleg@redhat.com>
        CC: David A. Long <dave.long@linaro.org>
        CC: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    
     include/linux/uprobes.h |   14 +++++++-------
     1 files changed, 7 insertions(+), 7 deletions(-)
    
    commit 97c0521a8c4539f6b1fcda60fec71f7be0a999cd
    Author: Haiyang Zhang <haiyangz@microsoft.com>
    Date:   Wed Oct 22 13:47:18 2014 -0700
    
        hyperv: Fix the total_data_buflen in send path
    
        total_data_buflen is used by netvsc_send() to decide if a packet can be put
        into send buffer. It should also include the size of RNDIS message before the
        Ethernet frame. Otherwise, a messge with total size bigger than send_section_size
        may be copied into the send buffer, and cause data corruption.
    
        [Request to include this patch to the Stable branches]
    
        Signed-off-by: Haiyang Zhang <haiyangz@microsoft.com>
        Reviewed-by: K. Y. Srinivasan <kys@microsoft.com>
        Signed-off-by: David S. Miller <davem@davemloft.net>
    
     drivers/net/hyperv/netvsc_drv.c |    1 +
     1 files changed, 1 insertions(+), 0 deletions(-)
    
    commit ddf93cb678c4734e44a6fad75bf46724f259170c
    Author: Daniel Borkmann <dborkman@redhat.com>
    Date:   Sun Sep 7 23:23:38 2014 +0200
    
        crypto: memzero_explicit - make sure to clear out sensitive data
    
        Recently, in commit 13aa93c70e71 ("random: add and use memzero_explicit()
        for clearing data"), we have found that GCC may optimize some memset()
        cases away when it detects a stack variable is not being used anymore
        and going out of scope. This can happen, for example, in cases when we
        are clearing out sensitive information such as keying material or any
        e.g. intermediate results from crypto computations, etc.
    
        With the help of Coccinelle, we can figure out and fix such occurences
        in the crypto subsytem as well. Julia Lawall provided the following
        Coccinelle program:
    
          @@
          type T;
          identifier x;
          @@
    
          T x;
          ... when exists
              when any
          -memset
          +memzero_explicit
             (&x,
          -0,
             ...)
          ... when != x
              when strict
    
          @@
          type T;
          identifier x;
          @@
    
          T x[...];
          ... when exists
              when any
          -memset
          +memzero_explicit
             (x,
          -0,
             ...)
          ... when != x
              when strict
    
        Therefore, make use of the drop-in replacement memzero_explicit() for
        exactly such cases instead of using memset().
    
        Signed-off-by: Daniel Borkmann <dborkman@redhat.com>
        Cc: Julia Lawall <julia.lawall@lip6.fr>
        Cc: Herbert Xu <herbert@gondor.apana.org.au>
        Cc: Theodore Ts'o <tytso@mit.edu>
        Cc: Hannes Frederic Sowa <hannes@stressinduktion.org>
        Acked-by: Hannes Frederic Sowa <hannes@stressinduktion.org>
        Acked-by: Herbert Xu <herbert@gondor.apana.org.au>
        Signed-off-by: Theodore Ts'o <tytso@mit.edu>
    
     crypto/cts.c            |    3 ++-
     crypto/sha1_generic.c   |    2 +-
     crypto/sha256_generic.c |    5 ++---
     crypto/sha512_generic.c |    2 +-
     crypto/tgr192.c         |    4 ++--
     crypto/vmac.c           |    2 +-
     crypto/wp512.c          |    8 ++++----
     7 files changed, 13 insertions(+), 13 deletions(-)
    
    commit e92f812a8c4803671f26b873c20eb42a04b8b1c0
    Author: Daniel Borkmann <dborkman@redhat.com>
    Date:   Tue Aug 26 23:16:35 2014 -0400
    
        random: add and use memzero_explicit() for clearing data
    
        zatimend has reported that in his environment (3.16/gcc4.8.3/corei7)
        memset() calls which clear out sensitive data in extract_{buf,entropy,
        entropy_user}() in random driver are being optimized away by gcc.
    
        Add a helper memzero_explicit() (similarly as explicit_bzero() variants)
        that can be used in such cases where a variable with sensitive data is
        being cleared out in the end. Other use cases might also be in crypto
        code. [ I have put this into lib/string.c though, as it's always built-in
        and doesn't need any dependencies then. ]
    
        Fixes kernel bugzilla: 82041
    
        Reported-by: zatimend@hotmail.co.uk
        Signed-off-by: Daniel Borkmann <dborkman@redhat.com>
        Acked-by: Hannes Frederic Sowa <hannes@stressinduktion.org>
        Cc: Alexey Dobriyan <adobriyan@gmail.com>
        Signed-off-by: Theodore Ts'o <tytso@mit.edu>
        Cc: stable@vger.kernel.org
    
     drivers/char/random.c  |    8 ++++----
     include/linux/string.h |    5 +++--
     lib/string.c           |   16 ++++++++++++++++
     3 files changed, 23 insertions(+), 6 deletions(-)
    
    commit 03be4ee8796d95c56d66404785653e3f9dd35eec
    Author: Florian Westphal <fw@strlen.de>
    Date:   Mon Oct 20 13:49:17 2014 +0200
    
        net: make skb_gso_segment error handling more robust
    
        skb_gso_segment has three possible return values:
        1. a pointer to the first segmented skb
        2. an errno value (IS_ERR())
        3. NULL.  This can happen when GSO is used for header verification.
    
        However, several callers currently test IS_ERR instead of IS_ERR_OR_NULL
        and would oops when NULL is returned.
    
        Note that these call sites should never actually see such a NULL return
        value; all callers mask out the GSO bits in the feature argument.
    
        However, there have been issues with some protocol handlers erronously not
        respecting the specified feature mask in some cases.
    
        It is preferable to get 'have to turn off hw offloading, else slow' reports
        rather than 'kernel crashes'.
    
        Signed-off-by: Florian Westphal <fw@strlen.de>
        Signed-off-by: David S. Miller <davem@davemloft.net>
    
     net/ipv4/ip_output.c                 |    2 +-
     net/netfilter/nfnetlink_queue_core.c |    2 +-
     net/openvswitch/datapath.c           |    2 ++
     net/xfrm/xfrm_output.c               |    2 ++
     4 files changed, 6 insertions(+), 2 deletions(-)
    
    commit 3ab31ee4d8c5ac9afcc43f180f60951cfc229014
    Author: Li RongQing <roy.qing.li@gmail.com>
    Date:   Wed Oct 22 17:09:53 2014 +0800
    
        xfrm6: fix a potential use after free in xfrm6_policy.c
    
        pskb_may_pull() maybe change skb->data and make nh and exthdr pointer
        oboslete, so recompute the nd and exthdr
    
        Signed-off-by: Li RongQing <roy.qing.li@gmail.com>
        Signed-off-by: David S. Miller <davem@davemloft.net>
    
     net/ipv6/xfrm6_policy.c |   11 ++++++++---
     1 files changed, 8 insertions(+), 3 deletions(-)
    
    commit 7a5098fd68c6ec4e07b3114218f00839e440f8de
    Author: Brad Spengler <spender@grsecurity.net>
    Date:   Thu Oct 23 19:20:18 2014 -0400
    
        allow print_bad_pte to display symbols
    
     mm/memory.c |    4 ++--
     1 files changed, 2 insertions(+), 2 deletions(-)
    
    commit e93e26911de7070ad329336eddfd53aff42ba154
    Author: Brad Spengler <spender@grsecurity.net>
    Date:   Wed Oct 22 18:36:16 2014 -0400
    
        Remove argument from gr_update_task_in_ip_table, as it's always called with 'current'
    
        Conflicts:
    
            net/ipv4/inet_hashtables.c
    
     grsecurity/grsec_sock.c    |    4 ++--
     net/ipv4/inet_hashtables.c |    4 ++--
     2 files changed, 4 insertions(+), 4 deletions(-)
    
    commit 614044a9056cd6378df89f3ed40193ed3e336efc
    Author: Brad Spengler <spender@grsecurity.net>
    Date:   Wed Oct 22 18:19:01 2014 -0400
    
        hold sighand lock while accessing ->signal
    
        Conflicts:
    
            fs/proc/array.c
    
     fs/proc/array.c |    9 ++++++++-
     1 files changed, 8 insertions(+), 1 deletions(-)
    
    Signature-tree: a4654efbdeeff749e74f6660472fc6aa9b9792d2

diff --git a/arch/arc/kernel/kgdb.c b/arch/arc/kernel/kgdb.c
index a2ff5c5d1450..ecf6a7869375 100644
--- a/arch/arc/kernel/kgdb.c
+++ b/arch/arc/kernel/kgdb.c
@@ -158,11 +158,6 @@ int kgdb_arch_handle_exception(int e_vector, int signo, int err_code,
 	return -1;
 }
 
-unsigned long kgdb_arch_pc(int exception, struct pt_regs *regs)
-{
-	return instruction_pointer(regs);
-}
-
 int kgdb_arch_init(void)
 {
 	single_step_data.armed = 0;
diff --git a/arch/sparc/include/asm/oplib_64.h b/arch/sparc/include/asm/oplib_64.h
index f34682430fcf..2e3a4add8591 100644
--- a/arch/sparc/include/asm/oplib_64.h
+++ b/arch/sparc/include/asm/oplib_64.h
@@ -62,7 +62,8 @@ struct linux_mem_p1275 {
 /* You must call prom_init() before using any of the library services,
  * preferably as early as possible.  Pass it the romvec pointer.
  */
-void prom_init(void *cif_handler, void *cif_stack);
+void prom_init(void *cif_handler);
+void prom_init_report(void);
 
 /* Boot argument acquisition, returns the boot command line string. */
 char *prom_getbootargs(void);
diff --git a/arch/sparc/include/asm/setup.h b/arch/sparc/include/asm/setup.h
index a0669f0a6d83..4272fe8d2d8b 100644
--- a/arch/sparc/include/asm/setup.h
+++ b/arch/sparc/include/asm/setup.h
@@ -48,6 +48,8 @@ unsigned long safe_compute_effective_address(struct pt_regs *, unsigned int);
 #endif
 
 #ifdef CONFIG_SPARC64
+void __init start_early_boot(void);
+
 /* unaligned_64.c */
 int handle_ldf_stq(u32 insn, struct pt_regs *regs);
 void handle_ld_nf(u32 insn, struct pt_regs *regs);
diff --git a/arch/sparc/kernel/entry.h b/arch/sparc/kernel/entry.h
index ebaba6167dd4..88d322b67fac 100644
--- a/arch/sparc/kernel/entry.h
+++ b/arch/sparc/kernel/entry.h
@@ -65,13 +65,10 @@ struct pause_patch_entry {
 extern struct pause_patch_entry __pause_3insn_patch,
 	__pause_3insn_patch_end;
 
-void __init per_cpu_patch(void);
 void sun4v_patch_1insn_range(struct sun4v_1insn_patch_entry *,
 			     struct sun4v_1insn_patch_entry *);
 void sun4v_patch_2insn_range(struct sun4v_2insn_patch_entry *,
 			     struct sun4v_2insn_patch_entry *);
-void __init sun4v_patch(void);
-void __init boot_cpu_id_too_large(int cpu);
 extern unsigned int dcache_parity_tl1_occurred;
 extern unsigned int icache_parity_tl1_occurred;
 
diff --git a/arch/sparc/kernel/head_64.S b/arch/sparc/kernel/head_64.S
index 452f04fe8da6..fbea0ac57f22 100644
--- a/arch/sparc/kernel/head_64.S
+++ b/arch/sparc/kernel/head_64.S
@@ -660,14 +660,12 @@ tlb_fixup_done:
 	sethi	%hi(init_thread_union), %g6
 	or	%g6, %lo(init_thread_union), %g6
 	ldx	[%g6 + TI_TASK], %g4
-	mov	%sp, %l6
 
 	wr	%g0, ASI_P, %asi
 	mov	1, %g1
 	sllx	%g1, THREAD_SHIFT, %g1
 	sub	%g1, (STACKFRAME_SZ + STACK_BIAS), %g1
 	add	%g6, %g1, %sp
-	mov	0, %fp
 
 	/* Set per-cpu pointer initially to zero, this makes
 	 * the boot-cpu use the in-kernel-image per-cpu areas
@@ -694,44 +692,14 @@ tlb_fixup_done:
 	 nop
 #endif
 
-	mov	%l6, %o1			! OpenPROM stack
 	call	prom_init
 	 mov	%l7, %o0			! OpenPROM cif handler
 
-	/* Initialize current_thread_info()->cpu as early as possible.
-	 * In order to do that accurately we have to patch up the get_cpuid()
-	 * assembler sequences.  And that, in turn, requires that we know
-	 * if we are on a Starfire box or not.  While we're here, patch up
-	 * the sun4v sequences as well.
+	/* To create a one-register-window buffer between the kernel's
+	 * initial stack and the last stack frame we use from the firmware,
+	 * do the rest of the boot from a C helper function.
 	 */
-	call	check_if_starfire
-	 nop
-	call	per_cpu_patch
-	 nop
-	call	sun4v_patch
-	 nop
-
-#ifdef CONFIG_SMP
-	call	hard_smp_processor_id
-	 nop
-	cmp	%o0, NR_CPUS
-	blu,pt	%xcc, 1f
-	 nop
-	call	boot_cpu_id_too_large
-	 nop
-	/* Not reached... */
-
-1:
-#else
-	mov	0, %o0
-#endif
-	sth	%o0, [%g6 + TI_CPU]
-
-	call	prom_init_report
-	 nop
-
-	/* Off we go.... */
-	call	start_kernel
+	call	start_early_boot
 	 nop
 	/* Not reached... */
 
diff --git a/arch/sparc/kernel/hvtramp.S b/arch/sparc/kernel/hvtramp.S
index b7ddcdd1dea9..cdbfec299f2f 100644
--- a/arch/sparc/kernel/hvtramp.S
+++ b/arch/sparc/kernel/hvtramp.S
@@ -109,7 +109,6 @@ hv_cpu_startup:
 	sllx		%g5, THREAD_SHIFT, %g5
 	sub		%g5, (STACKFRAME_SZ + STACK_BIAS), %g5
 	add		%g6, %g5, %sp
-	mov		0, %fp
 
 	call		init_irqwork_curcpu
 	 nop
diff --git a/arch/sparc/kernel/setup_64.c b/arch/sparc/kernel/setup_64.c
index 3fdb455e3318..949f7737e9fc 100644
--- a/arch/sparc/kernel/setup_64.c
+++ b/arch/sparc/kernel/setup_64.c
@@ -30,6 +30,7 @@
 #include <linux/cpu.h>
 #include <linux/initrd.h>
 #include <linux/module.h>
+#include <linux/start_kernel.h>
 
 #include <asm/io.h>
 #include <asm/processor.h>
@@ -174,7 +175,7 @@ char reboot_command[COMMAND_LINE_SIZE];
 
 static struct pt_regs fake_swapper_regs = { { 0, }, 0, 0, 0, 0 };
 
-void __init per_cpu_patch(void)
+static void __init per_cpu_patch(void)
 {
 	struct cpuid_patch_entry *p;
 	unsigned long ver;
@@ -266,7 +267,7 @@ void sun4v_patch_2insn_range(struct sun4v_2insn_patch_entry *start,
 	}
 }
 
-void __init sun4v_patch(void)
+static void __init sun4v_patch(void)
 {
 	extern void sun4v_hvapi_init(void);
 
@@ -335,14 +336,25 @@ static void __init pause_patch(void)
 	}
 }
 
-#ifdef CONFIG_SMP
-void __init boot_cpu_id_too_large(int cpu)
+void __init start_early_boot(void)
 {
-	prom_printf("Serious problem, boot cpu id (%d) >= NR_CPUS (%d)\n",
-		    cpu, NR_CPUS);
-	prom_halt();
+	int cpu;
+
+	check_if_starfire();
+	per_cpu_patch();
+	sun4v_patch();
+
+	cpu = hard_smp_processor_id();
+	if (cpu >= NR_CPUS) {
+		prom_printf("Serious problem, boot cpu id (%d) >= NR_CPUS (%d)\n",
+			    cpu, NR_CPUS);
+		prom_halt();
+	}
+	current_thread_info()->cpu = cpu;
+
+	prom_init_report();
+	start_kernel();
 }
-#endif
 
 /* On Ultra, we support all of the v8 capabilities. */
 unsigned long sparc64_elf_hwcap = (HWCAP_SPARC_FLUSH | HWCAP_SPARC_STBAR |
diff --git a/arch/sparc/kernel/trampoline_64.S b/arch/sparc/kernel/trampoline_64.S
index 737f8cbc7d56..88ede1d53b4c 100644
--- a/arch/sparc/kernel/trampoline_64.S
+++ b/arch/sparc/kernel/trampoline_64.S
@@ -109,10 +109,13 @@ startup_continue:
 	brnz,pn		%g1, 1b
 	 nop
 
-	sethi		%hi(p1275buf), %g2
-	or		%g2, %lo(p1275buf), %g2
-	ldx		[%g2 + 0x10], %l2
-	add		%l2, -(192 + 128), %sp
+	/* Get onto temporary stack which will be in the locked
+	 * kernel image.
+	 */
+	sethi		%hi(tramp_stack), %g1
+	or		%g1, %lo(tramp_stack), %g1
+	add		%g1, TRAMP_STACK_SIZE, %g1
+	sub		%g1, STACKFRAME_SZ + STACK_BIAS + 256, %sp
 	flushw
 
 	/* Setup the loop variables:
@@ -394,7 +397,6 @@ after_lock_tlb:
 	sllx		%g5, THREAD_SHIFT, %g5
 	sub		%g5, (STACKFRAME_SZ + STACK_BIAS), %g5
 	add		%g6, %g5, %sp
-	mov		0, %fp
 
 	rdpr		%pstate, %o1
 	or		%o1, PSTATE_IE, %o1
diff --git a/arch/sparc/mm/gup.c b/arch/sparc/mm/gup.c
index 1aed0432c64b..ae6ce383d4df 100644
--- a/arch/sparc/mm/gup.c
+++ b/arch/sparc/mm/gup.c
@@ -160,6 +160,36 @@ static int gup_pud_range(pgd_t pgd, unsigned long addr, unsigned long end,
 	return 1;
 }
 
+int __get_user_pages_fast(unsigned long start, int nr_pages, int write,
+			  struct page **pages)
+{
+	struct mm_struct *mm = current->mm;
+	unsigned long addr, len, end;
+	unsigned long next, flags;
+	pgd_t *pgdp;
+	int nr = 0;
+
+	start &= PAGE_MASK;
+	addr = start;
+	len = (unsigned long) nr_pages << PAGE_SHIFT;
+	end = start + len;
+
+	local_irq_save(flags);
+	pgdp = pgd_offset(mm, addr);
+	do {
+		pgd_t pgd = *pgdp;
+
+		next = pgd_addr_end(addr, end);
+		if (pgd_none(pgd))
+			break;
+		if (!gup_pud_range(pgd, addr, next, write, pages, &nr))
+			break;
+	} while (pgdp++, addr = next, addr != end);
+	local_irq_restore(flags);
+
+	return nr;
+}
+
 int get_user_pages_fast(unsigned long start, int nr_pages, int write,
 			struct page **pages)
 {
diff --git a/arch/sparc/prom/cif.S b/arch/sparc/prom/cif.S
index 9c86b4b7d429..8050f381f518 100644
--- a/arch/sparc/prom/cif.S
+++ b/arch/sparc/prom/cif.S
@@ -11,11 +11,10 @@
 	.text
 	.globl	prom_cif_direct
 prom_cif_direct:
+	save	%sp, -192, %sp
 	sethi	%hi(p1275buf), %o1
 	or	%o1, %lo(p1275buf), %o1
-	ldx	[%o1 + 0x0010], %o2	! prom_cif_stack
-	save	%o2, -192, %sp
-	ldx	[%i1 + 0x0008], %l2	! prom_cif_handler
+	ldx	[%o1 + 0x0008], %l2	! prom_cif_handler
 	mov	%g4, %l0
 	mov	%g5, %l1
 	mov	%g6, %l3
diff --git a/arch/sparc/prom/init_64.c b/arch/sparc/prom/init_64.c
index d95db755828f..110b0d78b864 100644
--- a/arch/sparc/prom/init_64.c
+++ b/arch/sparc/prom/init_64.c
@@ -26,13 +26,13 @@ phandle prom_chosen_node;
  * It gets passed the pointer to the PROM vector.
  */
 
-extern void prom_cif_init(void *, void *);
+extern void prom_cif_init(void *);
 
-void __init prom_init(void *cif_handler, void *cif_stack)
+void __init prom_init(void *cif_handler)
 {
 	phandle node;
 
-	prom_cif_init(cif_handler, cif_stack);
+	prom_cif_init(cif_handler);
 
 	prom_chosen_node = prom_finddevice(prom_chosen_path);
 	if (!prom_chosen_node || (s32)prom_chosen_node == -1)
diff --git a/arch/sparc/prom/p1275.c b/arch/sparc/prom/p1275.c
index e58b81726319..c27c30e4cb5e 100644
--- a/arch/sparc/prom/p1275.c
+++ b/arch/sparc/prom/p1275.c
@@ -19,7 +19,6 @@
 struct {
 	long prom_callback;			/* 0x00 */
 	void (*prom_cif_handler)(long *);	/* 0x08 */
-	unsigned long prom_cif_stack;		/* 0x10 */
 } p1275buf;
 
 extern void prom_world(int);
@@ -51,5 +50,4 @@ void p1275_cmd_direct(unsigned long *args)
 void prom_cif_init(void *cif_handler, void *cif_stack)
 {
 	p1275buf.prom_cif_handler = (void (*)(long *))cif_handler;
-	p1275buf.prom_cif_stack = (unsigned long)cif_stack;
 }
diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h
index 7c492ed9087b..d16311f4099e 100644
--- a/arch/x86/include/asm/kvm_host.h
+++ b/arch/x86/include/asm/kvm_host.h
@@ -990,6 +990,20 @@ static inline void kvm_inject_gp(struct kvm_vcpu *vcpu, u32 error_code)
 	kvm_queue_exception_e(vcpu, GP_VECTOR, error_code);
 }
 
+static inline u64 get_canonical(u64 la)
+{
+	return ((int64_t)la << 16) >> 16;
+}
+
+static inline bool is_noncanonical_address(u64 la)
+{
+#ifdef CONFIG_X86_64
+	return get_canonical(la) != la;
+#else
+	return false;
+#endif
+}
+
 #define TSS_IOPB_BASE_OFFSET 0x66
 #define TSS_BASE_SIZE 0x68
 #define TSS_IOPB_SIZE (65536 / 8)
@@ -1048,7 +1062,7 @@ int kvm_cpu_get_interrupt(struct kvm_vcpu *v);
 void kvm_vcpu_reset(struct kvm_vcpu *vcpu);
 
 void kvm_define_shared_msr(unsigned index, u32 msr);
-void kvm_set_shared_msr(unsigned index, u64 val, u64 mask);
+int kvm_set_shared_msr(unsigned index, u64 val, u64 mask);
 
 bool kvm_is_linear_rip(struct kvm_vcpu *vcpu, unsigned long linear_rip);
 
diff --git a/arch/x86/include/uapi/asm/vmx.h b/arch/x86/include/uapi/asm/vmx.h
index 0e79420376eb..990a2fe1588d 100644
--- a/arch/x86/include/uapi/asm/vmx.h
+++ b/arch/x86/include/uapi/asm/vmx.h
@@ -67,6 +67,7 @@
 #define EXIT_REASON_EPT_MISCONFIG       49
 #define EXIT_REASON_INVEPT              50
 #define EXIT_REASON_PREEMPTION_TIMER    52
+#define EXIT_REASON_INVVPID             53
 #define EXIT_REASON_WBINVD              54
 #define EXIT_REASON_XSETBV              55
 #define EXIT_REASON_APIC_WRITE          56
@@ -114,6 +115,7 @@
 	{ EXIT_REASON_EOI_INDUCED,           "EOI_INDUCED" }, \
 	{ EXIT_REASON_INVALID_STATE,         "INVALID_STATE" }, \
 	{ EXIT_REASON_INVD,                  "INVD" }, \
+	{ EXIT_REASON_INVVPID,               "INVVPID" }, \
 	{ EXIT_REASON_INVPCID,               "INVPCID" }
 
 #endif /* _UAPIVMX_H */
diff --git a/arch/x86/kvm/emulate.c b/arch/x86/kvm/emulate.c
index 03954f7900f5..48daa1a097af 100644
--- a/arch/x86/kvm/emulate.c
+++ b/arch/x86/kvm/emulate.c
@@ -504,11 +504,6 @@ static void rsp_increment(struct x86_emulate_ctxt *ctxt, int inc)
 	masked_increment(reg_rmw(ctxt, VCPU_REGS_RSP), stack_mask(ctxt), inc);
 }
 
-static inline void jmp_rel(struct x86_emulate_ctxt *ctxt, int rel)
-{
-	register_address_increment(ctxt, &ctxt->_eip, rel);
-}
-
 static u32 desc_limit_scaled(struct desc_struct *desc)
 {
 	u32 limit = get_desc_limit(desc);
@@ -568,6 +563,38 @@ static int emulate_nm(struct x86_emulate_ctxt *ctxt)
 	return emulate_exception(ctxt, NM_VECTOR, 0, false);
 }
 
+static inline int assign_eip_far(struct x86_emulate_ctxt *ctxt, ulong dst,
+			       int cs_l)
+{
+	switch (ctxt->op_bytes) {
+	case 2:
+		ctxt->_eip = (u16)dst;
+		break;
+	case 4:
+		ctxt->_eip = (u32)dst;
+		break;
+	case 8:
+		if ((cs_l && is_noncanonical_address(dst)) ||
+		    (!cs_l && (dst & ~(u32)-1)))
+			return emulate_gp(ctxt, 0);
+		ctxt->_eip = dst;
+		break;
+	default:
+		WARN(1, "unsupported eip assignment size\n");
+	}
+	return X86EMUL_CONTINUE;
+}
+
+static inline int assign_eip_near(struct x86_emulate_ctxt *ctxt, ulong dst)
+{
+	return assign_eip_far(ctxt, dst, ctxt->mode == X86EMUL_MODE_PROT64);
+}
+
+static inline int jmp_rel(struct x86_emulate_ctxt *ctxt, int rel)
+{
+	return assign_eip_near(ctxt, ctxt->_eip + rel);
+}
+
 static u16 get_segment_selector(struct x86_emulate_ctxt *ctxt, unsigned seg)
 {
 	u16 selector;
@@ -750,8 +777,10 @@ static int __do_insn_fetch_bytes(struct x86_emulate_ctxt *ctxt, int op_size)
 static __always_inline int do_insn_fetch_bytes(struct x86_emulate_ctxt *ctxt,
 					       unsigned size)
 {
-	if (unlikely(ctxt->fetch.end - ctxt->fetch.ptr < size))
-		return __do_insn_fetch_bytes(ctxt, size);
+	unsigned done_size = ctxt->fetch.end - ctxt->fetch.ptr;
+
+	if (unlikely(done_size < size))
+		return __do_insn_fetch_bytes(ctxt, size - done_size);
 	else
 		return X86EMUL_CONTINUE;
 }
@@ -1415,7 +1444,9 @@ static int write_segment_descriptor(struct x86_emulate_ctxt *ctxt,
 
 /* Does not support long mode */
 static int __load_segment_descriptor(struct x86_emulate_ctxt *ctxt,
-				     u16 selector, int seg, u8 cpl, bool in_task_switch)
+				     u16 selector, int seg, u8 cpl,
+				     bool in_task_switch,
+				     struct desc_struct *desc)
 {
 	struct desc_struct seg_desc, old_desc;
 	u8 dpl, rpl;
@@ -1547,6 +1578,8 @@ static int __load_segment_descriptor(struct x86_emulate_ctxt *ctxt,
 	}
 load:
 	ctxt->ops->set_segment(ctxt, selector, &seg_desc, base3, seg);
+	if (desc)
+		*desc = seg_desc;
 	return X86EMUL_CONTINUE;
 exception:
 	emulate_exception(ctxt, err_vec, err_code, true);
@@ -1557,7 +1590,7 @@ static int load_segment_descriptor(struct x86_emulate_ctxt *ctxt,
 				   u16 selector, int seg)
 {
 	u8 cpl = ctxt->ops->cpl(ctxt);
-	return __load_segment_descriptor(ctxt, selector, seg, cpl, false);
+	return __load_segment_descriptor(ctxt, selector, seg, cpl, false, NULL);
 }
 
 static void write_register_operand(struct operand *op)
@@ -1951,17 +1984,31 @@ static int em_iret(struct x86_emulate_ctxt *ctxt)
 static int em_jmp_far(struct x86_emulate_ctxt *ctxt)
 {
 	int rc;
-	unsigned short sel;
+	unsigned short sel, old_sel;
+	struct desc_struct old_desc, new_desc;
+	const struct x86_emulate_ops *ops = ctxt->ops;
+	u8 cpl = ctxt->ops->cpl(ctxt);
+
+	/* Assignment of RIP may only fail in 64-bit mode */
+	if (ctxt->mode == X86EMUL_MODE_PROT64)
+		ops->get_segment(ctxt, &old_sel, &old_desc, NULL,
+				 VCPU_SREG_CS);
 
 	memcpy(&sel, ctxt->src.valptr + ctxt->op_bytes, 2);
 
-	rc = load_segment_descriptor(ctxt, sel, VCPU_SREG_CS);
+	rc = __load_segment_descriptor(ctxt, sel, VCPU_SREG_CS, cpl, false,
+				       &new_desc);
 	if (rc != X86EMUL_CONTINUE)
 		return rc;
 
-	ctxt->_eip = 0;
-	memcpy(&ctxt->_eip, ctxt->src.valptr, ctxt->op_bytes);
-	return X86EMUL_CONTINUE;
+	rc = assign_eip_far(ctxt, ctxt->src.val, new_desc.l);
+	if (rc != X86EMUL_CONTINUE) {
+		WARN_ON(!ctxt->mode != X86EMUL_MODE_PROT64);
+		/* assigning eip failed; restore the old cs */
+		ops->set_segment(ctxt, old_sel, &old_desc, 0, VCPU_SREG_CS);
+		return rc;
+	}
+	return rc;
 }
 
 static int em_grp45(struct x86_emulate_ctxt *ctxt)
@@ -1972,13 +2019,15 @@ static int em_grp45(struct x86_emulate_ctxt *ctxt)
 	case 2: /* call near abs */ {
 		long int old_eip;
 		old_eip = ctxt->_eip;
-		ctxt->_eip = ctxt->src.val;
+		rc = assign_eip_near(ctxt, ctxt->src.val);
+		if (rc != X86EMUL_CONTINUE)
+			break;
 		ctxt->src.val = old_eip;
 		rc = em_push(ctxt);
 		break;
 	}
 	case 4: /* jmp abs */
-		ctxt->_eip = ctxt->src.val;
+		rc = assign_eip_near(ctxt, ctxt->src.val);
 		break;
 	case 5: /* jmp far */
 		rc = em_jmp_far(ctxt);
@@ -2013,30 +2062,47 @@ static int em_cmpxchg8b(struct x86_emulate_ctxt *ctxt)
 
 static int em_ret(struct x86_emulate_ctxt *ctxt)
 {
-	ctxt->dst.type = OP_REG;
-	ctxt->dst.addr.reg = &ctxt->_eip;
-	ctxt->dst.bytes = ctxt->op_bytes;
-	return em_pop(ctxt);
+	int rc;
+	unsigned long eip;
+
+	rc = emulate_pop(ctxt, &eip, ctxt->op_bytes);
+	if (rc != X86EMUL_CONTINUE)
+		return rc;
+
+	return assign_eip_near(ctxt, eip);
 }
 
 static int em_ret_far(struct x86_emulate_ctxt *ctxt)
 {
 	int rc;
-	unsigned long cs;
+	unsigned long eip, cs;
+	u16 old_cs;
 	int cpl = ctxt->ops->cpl(ctxt);
+	struct desc_struct old_desc, new_desc;
+	const struct x86_emulate_ops *ops = ctxt->ops;
 
-	rc = emulate_pop(ctxt, &ctxt->_eip, ctxt->op_bytes);
+	if (ctxt->mode == X86EMUL_MODE_PROT64)
+		ops->get_segment(ctxt, &old_cs, &old_desc, NULL,
+				 VCPU_SREG_CS);
+
+	rc = emulate_pop(ctxt, &eip, ctxt->op_bytes);
 	if (rc != X86EMUL_CONTINUE)
 		return rc;
-	if (ctxt->op_bytes == 4)
-		ctxt->_eip = (u32)ctxt->_eip;
 	rc = emulate_pop(ctxt, &cs, ctxt->op_bytes);
 	if (rc != X86EMUL_CONTINUE)
 		return rc;
 	/* Outer-privilege level return is not implemented */
 	if (ctxt->mode >= X86EMUL_MODE_PROT16 && (cs & 3) > cpl)
 		return X86EMUL_UNHANDLEABLE;
-	rc = load_segment_descriptor(ctxt, (u16)cs, VCPU_SREG_CS);
+	rc = __load_segment_descriptor(ctxt, (u16)cs, VCPU_SREG_CS, 0, false,
+				       &new_desc);
+	if (rc != X86EMUL_CONTINUE)
+		return rc;
+	rc = assign_eip_far(ctxt, eip, new_desc.l);
+	if (rc != X86EMUL_CONTINUE) {
+		WARN_ON(!ctxt->mode != X86EMUL_MODE_PROT64);
+		ops->set_segment(ctxt, old_cs, &old_desc, 0, VCPU_SREG_CS);
+	}
 	return rc;
 }
 
@@ -2297,7 +2363,7 @@ static int em_sysexit(struct x86_emulate_ctxt *ctxt)
 {
 	const struct x86_emulate_ops *ops = ctxt->ops;
 	struct desc_struct cs, ss;
-	u64 msr_data;
+	u64 msr_data, rcx, rdx;
 	int usermode;
 	u16 cs_sel = 0, ss_sel = 0;
 
@@ -2313,6 +2379,9 @@ static int em_sysexit(struct x86_emulate_ctxt *ctxt)
 	else
 		usermode = X86EMUL_MODE_PROT32;
 
+	rcx = reg_read(ctxt, VCPU_REGS_RCX);
+	rdx = reg_read(ctxt, VCPU_REGS_RDX);
+
 	cs.dpl = 3;
 	ss.dpl = 3;
 	ops->get_msr(ctxt, MSR_IA32_SYSENTER_CS, &msr_data);
@@ -2330,6 +2399,9 @@ static int em_sysexit(struct x86_emulate_ctxt *ctxt)
 		ss_sel = cs_sel + 8;
 		cs.d = 0;
 		cs.l = 1;
+		if (is_noncanonical_address(rcx) ||
+		    is_noncanonical_address(rdx))
+			return emulate_gp(ctxt, 0);
 		break;
 	}
 	cs_sel |= SELECTOR_RPL_MASK;
@@ -2338,8 +2410,8 @@ static int em_sysexit(struct x86_emulate_ctxt *ctxt)
 	ops->set_segment(ctxt, cs_sel, &cs, 0, VCPU_SREG_CS);
 	ops->set_segment(ctxt, ss_sel, &ss, 0, VCPU_SREG_SS);
 
-	ctxt->_eip = reg_read(ctxt, VCPU_REGS_RDX);
-	*reg_write(ctxt, VCPU_REGS_RSP) = reg_read(ctxt, VCPU_REGS_RCX);
+	ctxt->_eip = rdx;
+	*reg_write(ctxt, VCPU_REGS_RSP) = rcx;
 
 	return X86EMUL_CONTINUE;
 }
@@ -2457,19 +2529,24 @@ static int load_state_from_tss16(struct x86_emulate_ctxt *ctxt,
 	 * Now load segment descriptors. If fault happens at this stage
 	 * it is handled in a context of new task
 	 */
-	ret = __load_segment_descriptor(ctxt, tss->ldt, VCPU_SREG_LDTR, cpl, true);
+	ret = __load_segment_descriptor(ctxt, tss->ldt, VCPU_SREG_LDTR, cpl,
+					true, NULL);
 	if (ret != X86EMUL_CONTINUE)
 		return ret;
-	ret = __load_segment_descriptor(ctxt, tss->es, VCPU_SREG_ES, cpl, true);
+	ret = __load_segment_descriptor(ctxt, tss->es, VCPU_SREG_ES, cpl,
+					true, NULL);
 	if (ret != X86EMUL_CONTINUE)
 		return ret;
-	ret = __load_segment_descriptor(ctxt, tss->cs, VCPU_SREG_CS, cpl, true);
+	ret = __load_segment_descriptor(ctxt, tss->cs, VCPU_SREG_CS, cpl,
+					true, NULL);
 	if (ret != X86EMUL_CONTINUE)
 		return ret;
-	ret = __load_segment_descriptor(ctxt, tss->ss, VCPU_SREG_SS, cpl, true);
+	ret = __load_segment_descriptor(ctxt, tss->ss, VCPU_SREG_SS, cpl,
+					true, NULL);
 	if (ret != X86EMUL_CONTINUE)
 		return ret;
-	ret = __load_segment_descriptor(ctxt, tss->ds, VCPU_SREG_DS, cpl, true);
+	ret = __load_segment_descriptor(ctxt, tss->ds, VCPU_SREG_DS, cpl,
+					true, NULL);
 	if (ret != X86EMUL_CONTINUE)
 		return ret;
 
@@ -2594,25 +2671,32 @@ static int load_state_from_tss32(struct x86_emulate_ctxt *ctxt,
 	 * Now load segment descriptors. If fault happenes at this stage
 	 * it is handled in a context of new task
 	 */
-	ret = __load_segment_descriptor(ctxt, tss->ldt_selector, VCPU_SREG_LDTR, cpl, true);
+	ret = __load_segment_descriptor(ctxt, tss->ldt_selector, VCPU_SREG_LDTR,
+					cpl, true, NULL);
 	if (ret != X86EMUL_CONTINUE)
 		return ret;
-	ret = __load_segment_descriptor(ctxt, tss->es, VCPU_SREG_ES, cpl, true);
+	ret = __load_segment_descriptor(ctxt, tss->es, VCPU_SREG_ES, cpl,
+					true, NULL);
 	if (ret != X86EMUL_CONTINUE)
 		return ret;
-	ret = __load_segment_descriptor(ctxt, tss->cs, VCPU_SREG_CS, cpl, true);
+	ret = __load_segment_descriptor(ctxt, tss->cs, VCPU_SREG_CS, cpl,
+					true, NULL);
 	if (ret != X86EMUL_CONTINUE)
 		return ret;
-	ret = __load_segment_descriptor(ctxt, tss->ss, VCPU_SREG_SS, cpl, true);
+	ret = __load_segment_descriptor(ctxt, tss->ss, VCPU_SREG_SS, cpl,
+					true, NULL);
 	if (ret != X86EMUL_CONTINUE)
 		return ret;
-	ret = __load_segment_descriptor(ctxt, tss->ds, VCPU_SREG_DS, cpl, true);
+	ret = __load_segment_descriptor(ctxt, tss->ds, VCPU_SREG_DS, cpl,
+					true, NULL);
 	if (ret != X86EMUL_CONTINUE)
 		return ret;
-	ret = __load_segment_descriptor(ctxt, tss->fs, VCPU_SREG_FS, cpl, true);
+	ret = __load_segment_descriptor(ctxt, tss->fs, VCPU_SREG_FS, cpl,
+					true, NULL);
 	if (ret != X86EMUL_CONTINUE)
 		return ret;
-	ret = __load_segment_descriptor(ctxt, tss->gs, VCPU_SREG_GS, cpl, true);
+	ret = __load_segment_descriptor(ctxt, tss->gs, VCPU_SREG_GS, cpl,
+					true, NULL);
 	if (ret != X86EMUL_CONTINUE)
 		return ret;
 
@@ -2880,10 +2964,13 @@ static int em_aad(struct x86_emulate_ctxt *ctxt)
 
 static int em_call(struct x86_emulate_ctxt *ctxt)
 {
+	int rc;
 	long rel = ctxt->src.val;
 
 	ctxt->src.val = (unsigned long)ctxt->_eip;
-	jmp_rel(ctxt, rel);
+	rc = jmp_rel(ctxt, rel);
+	if (rc != X86EMUL_CONTINUE)
+		return rc;
 	return em_push(ctxt);
 }
 
@@ -2892,34 +2979,50 @@ static int em_call_far(struct x86_emulate_ctxt *ctxt)
 	u16 sel, old_cs;
 	ulong old_eip;
 	int rc;
+	struct desc_struct old_desc, new_desc;
+	const struct x86_emulate_ops *ops = ctxt->ops;
+	int cpl = ctxt->ops->cpl(ctxt);
 
-	old_cs = get_segment_selector(ctxt, VCPU_SREG_CS);
 	old_eip = ctxt->_eip;
+	ops->get_segment(ctxt, &old_cs, &old_desc, NULL, VCPU_SREG_CS);
 
 	memcpy(&sel, ctxt->src.valptr + ctxt->op_bytes, 2);
-	if (load_segment_descriptor(ctxt, sel, VCPU_SREG_CS))
+	rc = __load_segment_descriptor(ctxt, sel, VCPU_SREG_CS, cpl, false,
+				       &new_desc);
+	if (rc != X86EMUL_CONTINUE)
 		return X86EMUL_CONTINUE;
 
-	ctxt->_eip = 0;
-	memcpy(&ctxt->_eip, ctxt->src.valptr, ctxt->op_bytes);
+	rc = assign_eip_far(ctxt, ctxt->src.val, new_desc.l);
+	if (rc != X86EMUL_CONTINUE)
+		goto fail;
 
 	ctxt->src.val = old_cs;
 	rc = em_push(ctxt);
 	if (rc != X86EMUL_CONTINUE)
-		return rc;
+		goto fail;
 
 	ctxt->src.val = old_eip;
-	return em_push(ctxt);
+	rc = em_push(ctxt);
+	/* If we failed, we tainted the memory, but the very least we should
+	   restore cs */
+	if (rc != X86EMUL_CONTINUE)
+		goto fail;
+	return rc;
+fail:
+	ops->set_segment(ctxt, old_cs, &old_desc, 0, VCPU_SREG_CS);
+	return rc;
+
 }
 
 static int em_ret_near_imm(struct x86_emulate_ctxt *ctxt)
 {
 	int rc;
+	unsigned long eip;
 
-	ctxt->dst.type = OP_REG;
-	ctxt->dst.addr.reg = &ctxt->_eip;
-	ctxt->dst.bytes = ctxt->op_bytes;
-	rc = emulate_pop(ctxt, &ctxt->dst.val, ctxt->op_bytes);
+	rc = emulate_pop(ctxt, &eip, ctxt->op_bytes);
+	if (rc != X86EMUL_CONTINUE)
+		return rc;
+	rc = assign_eip_near(ctxt, eip);
 	if (rc != X86EMUL_CONTINUE)
 		return rc;
 	rsp_increment(ctxt, ctxt->src.val);
@@ -3250,20 +3353,24 @@ static int em_lmsw(struct x86_emulate_ctxt *ctxt)
 
 static int em_loop(struct x86_emulate_ctxt *ctxt)
 {
+	int rc = X86EMUL_CONTINUE;
+
 	register_address_increment(ctxt, reg_rmw(ctxt, VCPU_REGS_RCX), -1);
 	if ((address_mask(ctxt, reg_read(ctxt, VCPU_REGS_RCX)) != 0) &&
 	    (ctxt->b == 0xe2 || test_cc(ctxt->b ^ 0x5, ctxt->eflags)))
-		jmp_rel(ctxt, ctxt->src.val);
+		rc = jmp_rel(ctxt, ctxt->src.val);
 
-	return X86EMUL_CONTINUE;
+	return rc;
 }
 
 static int em_jcxz(struct x86_emulate_ctxt *ctxt)
 {
+	int rc = X86EMUL_CONTINUE;
+
 	if (address_mask(ctxt, reg_read(ctxt, VCPU_REGS_RCX)) == 0)
-		jmp_rel(ctxt, ctxt->src.val);
+		rc = jmp_rel(ctxt, ctxt->src.val);
 
-	return X86EMUL_CONTINUE;
+	return rc;
 }
 
 static int em_in(struct x86_emulate_ctxt *ctxt)
@@ -3351,6 +3458,12 @@ static int em_bswap(struct x86_emulate_ctxt *ctxt)
 	return X86EMUL_CONTINUE;
 }
 
+static int em_clflush(struct x86_emulate_ctxt *ctxt)
+{
+	/* emulating clflush regardless of cpuid */
+	return X86EMUL_CONTINUE;
+}
+
 static bool valid_cr(int nr)
 {
 	switch (nr) {
@@ -3683,6 +3796,16 @@ static const struct opcode group11[] = {
 	X7(D(Undefined)),
 };
 
+static const struct gprefix pfx_0f_ae_7 = {
+	I(SrcMem | ByteOp, em_clflush), N, N, N,
+};
+
+static const struct group_dual group15 = { {
+	N, N, N, N, N, N, N, GP(0, &pfx_0f_ae_7),
+}, {
+	N, N, N, N, N, N, N, N,
+} };
+
 static const struct gprefix pfx_0f_6f_0f_7f = {
 	I(Mmx, em_mov), I(Sse | Aligned, em_mov), N, I(Sse | Unaligned, em_mov),
 };
@@ -3887,10 +4010,11 @@ static const struct opcode twobyte_table[256] = {
 	N, I(ImplicitOps | EmulateOnUD, em_syscall),
 	II(ImplicitOps | Priv, em_clts, clts), N,
 	DI(ImplicitOps | Priv, invd), DI(ImplicitOps | Priv, wbinvd), N, N,
-	N, D(ImplicitOps | ModRM), N, N,
+	N, D(ImplicitOps | ModRM | SrcMem | NoAccess), N, N,
 	/* 0x10 - 0x1F */
 	N, N, N, N, N, N, N, N,
-	D(ImplicitOps | ModRM), N, N, N, N, N, N, D(ImplicitOps | ModRM),
+	D(ImplicitOps | ModRM | SrcMem | NoAccess),
+	N, N, N, N, N, N, D(ImplicitOps | ModRM | SrcMem | NoAccess),
 	/* 0x20 - 0x2F */
 	DIP(ModRM | DstMem | Priv | Op3264 | NoMod, cr_read, check_cr_read),
 	DIP(ModRM | DstMem | Priv | Op3264 | NoMod, dr_read, check_dr_read),
@@ -3942,7 +4066,7 @@ static const struct opcode twobyte_table[256] = {
 	F(DstMem | SrcReg | ModRM | BitOp | Lock | PageTable, em_bts),
 	F(DstMem | SrcReg | Src2ImmByte | ModRM, em_shrd),
 	F(DstMem | SrcReg | Src2CL | ModRM, em_shrd),
-	D(ModRM), F(DstReg | SrcMem | ModRM, em_imul),
+	GD(0, &group15), F(DstReg | SrcMem | ModRM, em_imul),
 	/* 0xB0 - 0xB7 */
 	I2bv(DstMem | SrcReg | ModRM | Lock | PageTable, em_cmpxchg),
 	I(DstReg | SrcMemFAddr | ModRM | Src2SS, em_lseg),
@@ -4458,10 +4582,10 @@ int x86_decode_insn(struct x86_emulate_ctxt *ctxt, void *insn, int insn_len)
 	/* Decode and fetch the destination operand: register or memory. */
 	rc = decode_operand(ctxt, &ctxt->dst, (ctxt->d >> DstShift) & OpMask);
 
-done:
 	if (ctxt->rip_relative)
 		ctxt->memopp->addr.mem.ea += ctxt->_eip;
 
+done:
 	return (rc != X86EMUL_CONTINUE) ? EMULATION_FAILED : EMULATION_OK;
 }
 
@@ -4711,7 +4835,7 @@ int x86_emulate_insn(struct x86_emulate_ctxt *ctxt)
 		break;
 	case 0x70 ... 0x7f: /* jcc (short) */
 		if (test_cc(ctxt->b, ctxt->eflags))
-			jmp_rel(ctxt, ctxt->src.val);
+			rc = jmp_rel(ctxt, ctxt->src.val);
 		break;
 	case 0x8d: /* lea r16/r32, m */
 		ctxt->dst.val = ctxt->src.addr.mem.ea;
@@ -4741,7 +4865,7 @@ int x86_emulate_insn(struct x86_emulate_ctxt *ctxt)
 		break;
 	case 0xe9: /* jmp rel */
 	case 0xeb: /* jmp rel short */
-		jmp_rel(ctxt, ctxt->src.val);
+		rc = jmp_rel(ctxt, ctxt->src.val);
 		ctxt->dst.type = OP_NONE; /* Disable writeback. */
 		break;
 	case 0xf4:              /* hlt */
@@ -4864,13 +4988,11 @@ int x86_emulate_insn(struct x86_emulate_ctxt *ctxt)
 		break;
 	case 0x80 ... 0x8f: /* jnz rel, etc*/
 		if (test_cc(ctxt->b, ctxt->eflags))
-			jmp_rel(ctxt, ctxt->src.val);
+			rc = jmp_rel(ctxt, ctxt->src.val);
 		break;
 	case 0x90 ... 0x9f:     /* setcc r/m8 */
 		ctxt->dst.val = test_cc(ctxt->b, ctxt->eflags);
 		break;
-	case 0xae:              /* clflush */
-		break;
 	case 0xb6 ... 0xb7:	/* movzx */
 		ctxt->dst.bytes = ctxt->op_bytes;
 		ctxt->dst.val = (ctxt->src.bytes == 1) ? (u8) ctxt->src.val
diff --git a/arch/x86/kvm/i8254.c b/arch/x86/kvm/i8254.c
index 518d86471b76..298781d4cfb4 100644
--- a/arch/x86/kvm/i8254.c
+++ b/arch/x86/kvm/i8254.c
@@ -262,8 +262,10 @@ void __kvm_migrate_pit_timer(struct kvm_vcpu *vcpu)
 		return;
 
 	timer = &pit->pit_state.timer;
+	mutex_lock(&pit->pit_state.lock);
 	if (hrtimer_cancel(timer))
 		hrtimer_start_expires(timer, HRTIMER_MODE_ABS);
+	mutex_unlock(&pit->pit_state.lock);
 }
 
 static void destroy_pit_timer(struct kvm_pit *pit)
diff --git a/arch/x86/kvm/svm.c b/arch/x86/kvm/svm.c
index e3b93f9dcb4b..fd8459960d8c 100644
--- a/arch/x86/kvm/svm.c
+++ b/arch/x86/kvm/svm.c
@@ -3234,7 +3234,7 @@ static int wrmsr_interception(struct vcpu_svm *svm)
 	msr.host_initiated = false;
 
 	svm->next_rip = kvm_rip_read(&svm->vcpu) + 2;
-	if (svm_set_msr(&svm->vcpu, &msr)) {
+	if (kvm_set_msr(&svm->vcpu, &msr)) {
 		trace_kvm_msr_write_ex(ecx, data);
 		kvm_inject_gp(&svm->vcpu, 0);
 	} else {
@@ -3534,9 +3534,9 @@ static int handle_exit(struct kvm_vcpu *vcpu)
 
 	if (exit_code >= ARRAY_SIZE(svm_exit_handlers)
 	    || !svm_exit_handlers[exit_code]) {
-		kvm_run->exit_reason = KVM_EXIT_UNKNOWN;
-		kvm_run->hw.hardware_exit_reason = exit_code;
-		return 0;
+		WARN_ONCE(1, "vmx: unexpected exit reason 0x%x\n", exit_code);
+		kvm_queue_exception(vcpu, UD_VECTOR);
+		return 1;
 	}
 
 	return svm_exit_handlers[exit_code](svm);
diff --git a/arch/x86/kvm/vmx.c b/arch/x86/kvm/vmx.c
index d567dc03b4b1..deb39590879d 100644
--- a/arch/x86/kvm/vmx.c
+++ b/arch/x86/kvm/vmx.c
@@ -2640,12 +2640,15 @@ static int vmx_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr_info)
 	default:
 		msr = find_msr_entry(vmx, msr_index);
 		if (msr) {
+			u64 old_msr_data = msr->data;
 			msr->data = data;
 			if (msr - vmx->guest_msrs < vmx->save_nmsrs) {
 				preempt_disable();
-				kvm_set_shared_msr(msr->index, msr->data,
-						   msr->mask);
+				ret = kvm_set_shared_msr(msr->index, msr->data,
+							 msr->mask);
 				preempt_enable();
+				if (ret)
+					msr->data = old_msr_data;
 			}
 			break;
 		}
@@ -5278,7 +5281,7 @@ static int handle_wrmsr(struct kvm_vcpu *vcpu)
 	msr.data = data;
 	msr.index = ecx;
 	msr.host_initiated = false;
-	if (vmx_set_msr(vcpu, &msr) != 0) {
+	if (kvm_set_msr(vcpu, &msr) != 0) {
 		trace_kvm_msr_write_ex(ecx, data);
 		kvm_inject_gp(vcpu, 0);
 		return 1;
@@ -6651,6 +6654,12 @@ static int handle_invept(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+static int handle_invvpid(struct kvm_vcpu *vcpu)
+{
+	kvm_queue_exception(vcpu, UD_VECTOR);
+	return 1;
+}
+
 /*
  * The exit handlers return 1 if the exit was handled fully and guest execution
  * may resume.  Otherwise they set the kvm_run parameter to indicate what needs
@@ -6696,6 +6705,7 @@ static int (*const kvm_vmx_exit_handlers[])(struct kvm_vcpu *vcpu) = {
 	[EXIT_REASON_MWAIT_INSTRUCTION]	      = handle_mwait,
 	[EXIT_REASON_MONITOR_INSTRUCTION]     = handle_monitor,
 	[EXIT_REASON_INVEPT]                  = handle_invept,
+	[EXIT_REASON_INVVPID]                 = handle_invvpid,
 };
 
 static const int kvm_vmx_max_exit_handlers =
@@ -6929,7 +6939,7 @@ static bool nested_vmx_exit_handled(struct kvm_vcpu *vcpu)
 	case EXIT_REASON_VMPTRST: case EXIT_REASON_VMREAD:
 	case EXIT_REASON_VMRESUME: case EXIT_REASON_VMWRITE:
 	case EXIT_REASON_VMOFF: case EXIT_REASON_VMON:
-	case EXIT_REASON_INVEPT:
+	case EXIT_REASON_INVEPT: case EXIT_REASON_INVVPID:
 		/*
 		 * VMX instructions trap unconditionally. This allows L1 to
 		 * emulate them for its L2 guest, i.e., allows 3-level nesting!
@@ -7070,10 +7080,10 @@ static int vmx_handle_exit(struct kvm_vcpu *vcpu)
 	    && kvm_vmx_exit_handlers[exit_reason])
 		return kvm_vmx_exit_handlers[exit_reason](vcpu);
 	else {
-		vcpu->run->exit_reason = KVM_EXIT_UNKNOWN;
-		vcpu->run->hw.hardware_exit_reason = exit_reason;
+		WARN_ONCE(1, "vmx: unexpected exit reason 0x%x\n", exit_reason);
+		kvm_queue_exception(vcpu, UD_VECTOR);
+		return 1;
 	}
-	return 0;
 }
 
 static void update_cr8_intercept(struct kvm_vcpu *vcpu, int tpr, int irr)
diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c
index f6eee205a096..c23d3c52aca0 100644
--- a/arch/x86/kvm/x86.c
+++ b/arch/x86/kvm/x86.c
@@ -229,20 +229,25 @@ static void kvm_shared_msr_cpu_online(void)
 		shared_msr_update(i, shared_msrs_global.msrs[i]);
 }
 
-void kvm_set_shared_msr(unsigned slot, u64 value, u64 mask)
+int kvm_set_shared_msr(unsigned slot, u64 value, u64 mask)
 {
 	unsigned int cpu = smp_processor_id();
 	struct kvm_shared_msrs *smsr = per_cpu_ptr(shared_msrs, cpu);
+	int err;
 
 	if (((value ^ smsr->values[slot].curr) & mask) == 0)
-		return;
+		return 0;
 	smsr->values[slot].curr = value;
-	wrmsrl(shared_msrs_global.msrs[slot], value);
+	err = wrmsrl_safe(shared_msrs_global.msrs[slot], value);
+	if (err)
+		return 1;
+
 	if (!smsr->registered) {
 		smsr->urn.on_user_return = kvm_on_user_return;
 		user_return_notifier_register(&smsr->urn);
 		smsr->registered = true;
 	}
+	return 0;
 }
 EXPORT_SYMBOL_GPL(kvm_set_shared_msr);
 
@@ -984,7 +989,6 @@ void kvm_enable_efer_bits(u64 mask)
 }
 EXPORT_SYMBOL_GPL(kvm_enable_efer_bits);
 
-
 /*
  * Writes msr value into into the appropriate "register".
  * Returns 0 on success, non-0 otherwise.
@@ -992,8 +996,34 @@ EXPORT_SYMBOL_GPL(kvm_enable_efer_bits);
  */
 int kvm_set_msr(struct kvm_vcpu *vcpu, struct msr_data *msr)
 {
+	switch (msr->index) {
+	case MSR_FS_BASE:
+	case MSR_GS_BASE:
+	case MSR_KERNEL_GS_BASE:
+	case MSR_CSTAR:
+	case MSR_LSTAR:
+		if (is_noncanonical_address(msr->data))
+			return 1;
+		break;
+	case MSR_IA32_SYSENTER_EIP:
+	case MSR_IA32_SYSENTER_ESP:
+		/*
+		 * IA32_SYSENTER_ESP and IA32_SYSENTER_EIP cause #GP if
+		 * non-canonical address is written on Intel but not on
+		 * AMD (which ignores the top 32-bits, because it does
+		 * not implement 64-bit SYSENTER).
+		 *
+		 * 64-bit code should hence be able to write a non-canonical
+		 * value on AMD.  Making the address canonical ensures that
+		 * vmentry does not fail on Intel after writing a non-canonical
+		 * value, and that something deterministic happens if the guest
+		 * invokes 64-bit SYSENTER.
+		 */
+		msr->data = get_canonical(msr->data);
+	}
 	return kvm_x86_ops->set_msr(vcpu, msr);
 }
+EXPORT_SYMBOL_GPL(kvm_set_msr);
 
 /*
  * Adapt set_msr() to msr_io()'s calling convention
diff --git a/arch/x86/platform/intel-mid/intel_mid_weak_decls.h b/arch/x86/platform/intel-mid/intel_mid_weak_decls.h
index 7208aeb98a46..59a68ed4cabc 100644
--- a/arch/x86/platform/intel-mid/intel_mid_weak_decls.h
+++ b/arch/x86/platform/intel-mid/intel_mid_weak_decls.h
@@ -10,10 +10,9 @@
  */
 
 
-/* __attribute__((weak)) makes these declarations overridable */
 /* For every CPU addition a new get_<cpuname>_ops interface needs
  * to be added.
  */
-extern const void *get_penwell_ops(void) __attribute__((weak));
-extern const void *get_cloverview_ops(void) __attribute__((weak));
-extern const void *get_tangier_ops(void) __attribute__((weak));
+extern const void *get_penwell_ops(void);
+extern const void *get_cloverview_ops(void);
+extern const void *get_tangier_ops(void);
diff --git a/crypto/cts.c b/crypto/cts.c
index 042223f8e733..133f0874c95e 100644
--- a/crypto/cts.c
+++ b/crypto/cts.c
@@ -202,7 +202,8 @@ static int cts_cbc_decrypt(struct crypto_cts_ctx *ctx,
 	/* 5. Append the tail (BB - Ln) bytes of Xn (tmp) to Cn to create En */
 	memcpy(s + bsize + lastn, tmp + lastn, bsize - lastn);
 	/* 6. Decrypt En to create Pn-1 */
-	memset(iv, 0, sizeof(iv));
+	memzero_explicit(iv, sizeof(iv));
+
 	sg_set_buf(&sgsrc[0], s + bsize, bsize);
 	sg_set_buf(&sgdst[0], d, bsize);
 	err = crypto_blkcipher_decrypt_iv(&lcldesc, sgdst, sgsrc, bsize);
diff --git a/crypto/sha1_generic.c b/crypto/sha1_generic.c
index 42794803c480..7bb047432782 100644
--- a/crypto/sha1_generic.c
+++ b/crypto/sha1_generic.c
@@ -64,7 +64,7 @@ int crypto_sha1_update(struct shash_desc *desc, const u8 *data,
 			src = data + done;
 		} while (done + SHA1_BLOCK_SIZE <= len);
 
-		memset(temp, 0, sizeof(temp));
+		memzero_explicit(temp, sizeof(temp));
 		partial = 0;
 	}
 	memcpy(sctx->buffer + partial, src, len - done);
diff --git a/crypto/sha256_generic.c b/crypto/sha256_generic.c
index 543366779524..32c5e5ea205a 100644
--- a/crypto/sha256_generic.c
+++ b/crypto/sha256_generic.c
@@ -210,10 +210,9 @@ static void sha256_transform(u32 *state, const u8 *input)
 
 	/* clear any sensitive info... */
 	a = b = c = d = e = f = g = h = t1 = t2 = 0;
-	memset(W, 0, 64 * sizeof(u32));
+	memzero_explicit(W, 64 * sizeof(u32));
 }
 
-
 static int sha224_init(struct shash_desc *desc)
 {
 	struct sha256_state *sctx = shash_desc_ctx(desc);
@@ -316,7 +315,7 @@ static int sha224_final(struct shash_desc *desc, u8 *hash)
 	sha256_final(desc, D);
 
 	memcpy(hash, D, SHA224_DIGEST_SIZE);
-	memset(D, 0, SHA256_DIGEST_SIZE);
+	memzero_explicit(D, SHA256_DIGEST_SIZE);
 
 	return 0;
 }
diff --git a/crypto/sha512_generic.c b/crypto/sha512_generic.c
index 6ed124f3ea0f..04d295a8bc08 100644
--- a/crypto/sha512_generic.c
+++ b/crypto/sha512_generic.c
@@ -238,7 +238,7 @@ static int sha384_final(struct shash_desc *desc, u8 *hash)
 	sha512_final(desc, D);
 
 	memcpy(hash, D, 48);
-	memset(D, 0, 64);
+	memzero_explicit(D, 64);
 
 	return 0;
 }
diff --git a/crypto/tgr192.c b/crypto/tgr192.c
index 87403556fd0b..3c7af0d1ff7a 100644
--- a/crypto/tgr192.c
+++ b/crypto/tgr192.c
@@ -612,7 +612,7 @@ static int tgr160_final(struct shash_desc *desc, u8 * out)
 
 	tgr192_final(desc, D);
 	memcpy(out, D, TGR160_DIGEST_SIZE);
-	memset(D, 0, TGR192_DIGEST_SIZE);
+	memzero_explicit(D, TGR192_DIGEST_SIZE);
 
 	return 0;
 }
@@ -623,7 +623,7 @@ static int tgr128_final(struct shash_desc *desc, u8 * out)
 
 	tgr192_final(desc, D);
 	memcpy(out, D, TGR128_DIGEST_SIZE);
-	memset(D, 0, TGR192_DIGEST_SIZE);
+	memzero_explicit(D, TGR192_DIGEST_SIZE);
 
 	return 0;
 }
diff --git a/crypto/vmac.c b/crypto/vmac.c
index 2eb11a30c29c..d84c24bd7ff7 100644
--- a/crypto/vmac.c
+++ b/crypto/vmac.c
@@ -613,7 +613,7 @@ static int vmac_final(struct shash_desc *pdesc, u8 *out)
 	}
 	mac = vmac(ctx->partial, ctx->partial_size, nonce, NULL, ctx);
 	memcpy(out, &mac, sizeof(vmac_t));
-	memset(&mac, 0, sizeof(vmac_t));
+	memzero_explicit(&mac, sizeof(vmac_t));
 	memset(&ctx->__vmac_ctx, 0, sizeof(struct vmac_ctx));
 	ctx->partial_size = 0;
 	return 0;
diff --git a/crypto/wp512.c b/crypto/wp512.c
index 180f1d6e03f4..ec64e7762fbb 100644
--- a/crypto/wp512.c
+++ b/crypto/wp512.c
@@ -1102,8 +1102,8 @@ static int wp384_final(struct shash_desc *desc, u8 *out)
 	u8 D[64];
 
 	wp512_final(desc, D);
-	memcpy (out, D, WP384_DIGEST_SIZE);
-	memset (D, 0, WP512_DIGEST_SIZE);
+	memcpy(out, D, WP384_DIGEST_SIZE);
+	memzero_explicit(D, WP512_DIGEST_SIZE);
 
 	return 0;
 }
@@ -1113,8 +1113,8 @@ static int wp256_final(struct shash_desc *desc, u8 *out)
 	u8 D[64];
 
 	wp512_final(desc, D);
-	memcpy (out, D, WP256_DIGEST_SIZE);
-	memset (D, 0, WP512_DIGEST_SIZE);
+	memcpy(out, D, WP256_DIGEST_SIZE);
+	memzero_explicit(D, WP512_DIGEST_SIZE);
 
 	return 0;
 }
diff --git a/drivers/char/random.c b/drivers/char/random.c
index a39afb7241ec..7c499f374a0b 100644
--- a/drivers/char/random.c
+++ b/drivers/char/random.c
@@ -1103,7 +1103,7 @@ static void extract_buf(struct entropy_store *r, __u8 *out)
 	__mix_pool_bytes(r, hash.w, sizeof(hash.w));
 	spin_unlock_irqrestore(&r->lock, flags);
 
-	memset(workspace, 0, sizeof(workspace));
+	memzero_explicit(workspace, sizeof(workspace));
 
 	/*
 	 * In case the hash function has some recognizable output
@@ -1115,7 +1115,7 @@ static void extract_buf(struct entropy_store *r, __u8 *out)
 	hash.w[2] ^= rol32(hash.w[2], 16);
 
 	memcpy(out, &hash, EXTRACT_SIZE);
-	memset(&hash, 0, sizeof(hash));
+	memzero_explicit(&hash, sizeof(hash));
 }
 
 /*
@@ -1172,7 +1172,7 @@ static ssize_t extract_entropy(struct entropy_store *r, void *buf,
 	}
 
 	/* Wipe data just returned from memory */
-	memset(tmp, 0, sizeof(tmp));
+	memzero_explicit(tmp, sizeof(tmp));
 
 	return ret;
 }
@@ -1215,7 +1215,7 @@ static ssize_t extract_entropy_user(struct entropy_store *r, void __user *buf,
 	}
 
 	/* Wipe data just returned from memory */
-	memset(tmp, 0, sizeof(tmp));
+	memzero_explicit(tmp, sizeof(tmp));
 
 	return ret;
 }
diff --git a/drivers/net/hyperv/netvsc_drv.c b/drivers/net/hyperv/netvsc_drv.c
index 0fcb5e7eb073..148fda3be898 100644
--- a/drivers/net/hyperv/netvsc_drv.c
+++ b/drivers/net/hyperv/netvsc_drv.c
@@ -556,6 +556,7 @@ static int netvsc_start_xmit(struct sk_buff *skb, struct net_device *net)
 do_send:
 	/* Start filling in the page buffers with the rndis hdr */
 	rndis_msg->msg_len += rndis_msg_size;
+	packet->total_data_buflen = rndis_msg->msg_len;
 	packet->page_buf_cnt = init_page_array(rndis_msg, rndis_msg_size,
 					skb, &packet->page_buf[0]);
 
diff --git a/fs/proc/array.c b/fs/proc/array.c
index 25c14e86059b..9b9b79aa1375 100644
--- a/fs/proc/array.c
+++ b/fs/proc/array.c
@@ -647,7 +647,14 @@ int proc_pid_statm(struct seq_file *m, struct pid_namespace *ns,
 #ifdef CONFIG_GRKERNSEC_PROC_IPADDR
 int proc_pid_ipaddr(struct seq_file *m, struct pid_namespace *ns, struct pid *pid, struct task_struct *task)
 {
-	return seq_printf(m, "%pI4\n", &task->signal->curr_ip);
+	unsigned long flags;
+	u32 curr_ip = 0;
+
+	if (lock_task_sighand(task, &flags)) {
+		curr_ip = task->signal->curr_ip;
+		unlock_task_sighand(task, &flags);
+	}
+	return seq_printf(m, "%pI4\n", &curr_ip);
 }
 #endif
 
diff --git a/grsecurity/grsec_sock.c b/grsecurity/grsec_sock.c
index c0aef3a01d8a..e3650b67d18a 100644
--- a/grsecurity/grsec_sock.c
+++ b/grsecurity/grsec_sock.c
@@ -121,10 +121,10 @@ static struct signal_struct * gr_lookup_task_ip_table(__u32 saddr, __u32 daddr,
 
 #endif
 
-void gr_update_task_in_ip_table(struct task_struct *task, const struct inet_sock *inet)
+void gr_update_task_in_ip_table(const struct inet_sock *inet)
 {
 #ifdef CONFIG_GRKERNSEC
-	struct signal_struct *sig = task->signal;
+	struct signal_struct *sig = current->signal;
 	struct conn_table_entry *newent;
 
 	newent = kmalloc(sizeof(struct conn_table_entry), GFP_ATOMIC);
diff --git a/include/linux/audit.h b/include/linux/audit.h
index ab759e829caa..1514eef61614 100644
--- a/include/linux/audit.h
+++ b/include/linux/audit.h
@@ -86,7 +86,7 @@ extern unsigned compat_dir_class[];
 extern unsigned compat_chattr_class[];
 extern unsigned compat_signal_class[];
 
-extern int __weak audit_classify_compat_syscall(int abi, unsigned syscall);
+extern int audit_classify_compat_syscall(int abi, unsigned syscall);
 
 /* audit_names->type values */
 #define	AUDIT_TYPE_UNKNOWN	0	/* we don't know yet */
diff --git a/include/linux/clocksource.h b/include/linux/clocksource.h
index 653f0e2b6ca9..abcafaa20b86 100644
--- a/include/linux/clocksource.h
+++ b/include/linux/clocksource.h
@@ -287,7 +287,7 @@ extern struct clocksource* clocksource_get_next(void);
 extern void clocksource_change_rating(struct clocksource *cs, int rating);
 extern void clocksource_suspend(void);
 extern void clocksource_resume(void);
-extern struct clocksource * __init __weak clocksource_default_clock(void);
+extern struct clocksource * __init clocksource_default_clock(void);
 extern void clocksource_mark_unstable(struct clocksource *cs);
 
 extern u64
diff --git a/include/linux/crash_dump.h b/include/linux/crash_dump.h
index 72ab536ad3de..3849fce7ecfe 100644
--- a/include/linux/crash_dump.h
+++ b/include/linux/crash_dump.h
@@ -14,14 +14,13 @@
 extern unsigned long long elfcorehdr_addr;
 extern unsigned long long elfcorehdr_size;
 
-extern int __weak elfcorehdr_alloc(unsigned long long *addr,
-				   unsigned long long *size);
-extern void __weak elfcorehdr_free(unsigned long long addr);
-extern ssize_t __weak elfcorehdr_read(char *buf, size_t count, u64 *ppos);
-extern ssize_t __weak elfcorehdr_read_notes(char *buf, size_t count, u64 *ppos);
-extern int __weak remap_oldmem_pfn_range(struct vm_area_struct *vma,
-					 unsigned long from, unsigned long pfn,
-					 unsigned long size, pgprot_t prot);
+extern int elfcorehdr_alloc(unsigned long long *addr, unsigned long long *size);
+extern void elfcorehdr_free(unsigned long long addr);
+extern ssize_t elfcorehdr_read(char *buf, size_t count, u64 *ppos);
+extern ssize_t elfcorehdr_read_notes(char *buf, size_t count, u64 *ppos);
+extern int remap_oldmem_pfn_range(struct vm_area_struct *vma,
+				  unsigned long from, unsigned long pfn,
+				  unsigned long size, pgprot_t prot);
 
 extern ssize_t copy_oldmem_page(unsigned long, char *, size_t,
 						unsigned long, int);
diff --git a/include/linux/kgdb.h b/include/linux/kgdb.h
index c134867620df..19f605fd212d 100644
--- a/include/linux/kgdb.h
+++ b/include/linux/kgdb.h
@@ -283,7 +283,7 @@ struct kgdb_io {
 
 extern struct kgdb_arch		arch_kgdb_ops;
 
-extern unsigned long __weak kgdb_arch_pc(int exception, struct pt_regs *regs);
+extern unsigned long kgdb_arch_pc(int exception, struct pt_regs *regs);
 
 #ifdef CONFIG_SERIAL_KGDB_NMI
 extern int kgdb_register_nmi_console(void);
diff --git a/include/linux/memory.h b/include/linux/memory.h
index bb7384e3c3d8..8b8d8d12348e 100644
--- a/include/linux/memory.h
+++ b/include/linux/memory.h
@@ -35,7 +35,7 @@ struct memory_block {
 };
 
 int arch_get_memory_phys_device(unsigned long start_pfn);
-unsigned long __weak memory_block_size_bytes(void);
+unsigned long memory_block_size_bytes(void);
 
 /* These states are exposed to userspace as text strings in sysfs */
 #define	MEM_ONLINE		(1<<0) /* exposed to userspace */
diff --git a/include/linux/string.h b/include/linux/string.h
index d36977e029af..3b42b3732da6 100644
--- a/include/linux/string.h
+++ b/include/linux/string.h
@@ -132,7 +132,7 @@ int bprintf(u32 *bin_buf, size_t size, const char *fmt, ...) __printf(3, 4);
 #endif
 
 extern ssize_t memory_read_from_buffer(void *to, size_t count, loff_t *ppos,
-			const void *from, size_t available);
+				       const void *from, size_t available);
 
 /**
  * strstarts - does @str start with @prefix?
@@ -144,7 +144,8 @@ static inline bool strstarts(const char *str, const char *prefix)
 	return strncmp(str, prefix, strlen(prefix)) == 0;
 }
 
-extern size_t memweight(const void *ptr, size_t bytes);
+size_t memweight(const void *ptr, size_t bytes);
+void memzero_explicit(void *s, size_t count);
 
 /**
  * kbasename - return the last part of a pathname.
diff --git a/include/linux/uprobes.h b/include/linux/uprobes.h
index 4f844c6b03ee..60beb5dc7977 100644
--- a/include/linux/uprobes.h
+++ b/include/linux/uprobes.h
@@ -98,11 +98,11 @@ struct uprobes_state {
 	struct xol_area		*xol_area;
 };
 
-extern int __weak set_swbp(struct arch_uprobe *aup, struct mm_struct *mm, unsigned long vaddr);
-extern int __weak set_orig_insn(struct arch_uprobe *aup, struct mm_struct *mm, unsigned long vaddr);
-extern bool __weak is_swbp_insn(uprobe_opcode_t *insn);
-extern bool __weak is_trap_insn(uprobe_opcode_t *insn);
-extern unsigned long __weak uprobe_get_swbp_addr(struct pt_regs *regs);
+extern int set_swbp(struct arch_uprobe *aup, struct mm_struct *mm, unsigned long vaddr);
+extern int set_orig_insn(struct arch_uprobe *aup, struct mm_struct *mm, unsigned long vaddr);
+extern bool is_swbp_insn(uprobe_opcode_t *insn);
+extern bool is_trap_insn(uprobe_opcode_t *insn);
+extern unsigned long uprobe_get_swbp_addr(struct pt_regs *regs);
 extern unsigned long uprobe_get_trap_addr(struct pt_regs *regs);
 extern int uprobe_write_opcode(struct mm_struct *mm, unsigned long vaddr, uprobe_opcode_t);
 extern int uprobe_register(struct inode *inode, loff_t offset, struct uprobe_consumer *uc);
@@ -128,8 +128,8 @@ extern bool arch_uprobe_xol_was_trapped(struct task_struct *tsk);
 extern int  arch_uprobe_exception_notify(struct notifier_block *self, unsigned long val, void *data);
 extern void arch_uprobe_abort_xol(struct arch_uprobe *aup, struct pt_regs *regs);
 extern unsigned long arch_uretprobe_hijack_return_addr(unsigned long trampoline_vaddr, struct pt_regs *regs);
-extern bool __weak arch_uprobe_ignore(struct arch_uprobe *aup, struct pt_regs *regs);
-extern void __weak arch_uprobe_copy_ixol(struct page *page, unsigned long vaddr,
+extern bool arch_uprobe_ignore(struct arch_uprobe *aup, struct pt_regs *regs);
+extern void arch_uprobe_copy_ixol(struct page *page, unsigned long vaddr,
 					 void *src, unsigned long len);
 #else /* !CONFIG_UPROBES */
 struct uprobes_state {
diff --git a/lib/string.c b/lib/string.c
index f3c6ff596414..70db57a81f7c 100644
--- a/lib/string.c
+++ b/lib/string.c
@@ -604,6 +604,22 @@ void *memset(void *s, int c, size_t count)
 EXPORT_SYMBOL(memset);
 #endif
 
+/**
+ * memzero_explicit - Fill a region of memory (e.g. sensitive
+ *		      keying data) with 0s.
+ * @s: Pointer to the start of the area.
+ * @count: The size of the area.
+ *
+ * memzero_explicit() doesn't need an arch-specific version as
+ * it just invokes the one of memset() implicitly.
+ */
+void memzero_explicit(void *s, size_t count)
+{
+	memset(s, 0, count);
+	OPTIMIZER_HIDE_VAR(s);
+}
+EXPORT_SYMBOL(memzero_explicit);
+
 #ifndef __HAVE_ARCH_MEMCPY
 /**
  * memcpy - Copy one area of memory to another
diff --git a/mm/memory.c b/mm/memory.c
index 2917c980d9eb..a3eb2ce573ea 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -697,10 +697,10 @@ static void print_bad_pte(struct vm_area_struct *vma, unsigned long addr,
 	 * Choose text because data symbols depend on CONFIG_KALLSYMS_ALL=y
 	 */
 	if (vma->vm_ops)
-		printk(KERN_ALERT "vma->vm_ops->fault: %pSR\n",
+		printk(KERN_ALERT "vma->vm_ops->fault: %pAR\n",
 		       vma->vm_ops->fault);
 	if (vma->vm_file)
-		printk(KERN_ALERT "vma->vm_file->f_op->mmap: %pSR\n",
+		printk(KERN_ALERT "vma->vm_file->f_op->mmap: %pAR\n",
 		       vma->vm_file->f_op->mmap);
 	dump_stack();
 	add_taint(TAINT_BAD_PAGE, LOCKDEP_NOW_UNRELIABLE);
diff --git a/net/ipv4/inet_hashtables.c b/net/ipv4/inet_hashtables.c
index e3e615924b9c..ba0916a84147 100644
--- a/net/ipv4/inet_hashtables.c
+++ b/net/ipv4/inet_hashtables.c
@@ -50,7 +50,7 @@ static unsigned int inet_sk_ehashfn(const struct sock *sk)
 	return inet_ehashfn(net, laddr, lport, faddr, fport);
 }
 
-extern void gr_update_task_in_ip_table(struct task_struct *task, const struct inet_sock *inet);
+extern void gr_update_task_in_ip_table(const struct inet_sock *inet);
 
 /*
  * Allocate and initialize a new local port bind bucket.
@@ -557,7 +557,7 @@ int __inet_hash_connect(struct inet_timewait_death_row *death_row,
 			twrefcnt += inet_twsk_bind_unhash(tw, hinfo);
 		spin_unlock(&head->lock);
 
-		gr_update_task_in_ip_table(current, inet_sk(sk));
+		gr_update_task_in_ip_table(inet_sk(sk));
 
 		if (tw) {
 			inet_twsk_deschedule(tw, death_row);
diff --git a/net/ipv4/ip_output.c b/net/ipv4/ip_output.c
index c43a1e235182..73cbbe1d0f76 100644
--- a/net/ipv4/ip_output.c
+++ b/net/ipv4/ip_output.c
@@ -231,7 +231,7 @@ static int ip_finish_output_gso(struct sk_buff *skb)
 	 */
 	features = netif_skb_features(skb);
 	segs = skb_gso_segment(skb, features & ~NETIF_F_GSO_MASK);
-	if (IS_ERR(segs)) {
+	if (IS_ERR_OR_NULL(segs)) {
 		kfree_skb(skb);
 		return -ENOMEM;
 	}
diff --git a/net/ipv6/xfrm6_policy.c b/net/ipv6/xfrm6_policy.c
index d75ca57ee32e..442240d78fc6 100644
--- a/net/ipv6/xfrm6_policy.c
+++ b/net/ipv6/xfrm6_policy.c
@@ -170,8 +170,10 @@ _decode_session6(struct sk_buff *skb, struct flowi *fl, int reverse)
 		case IPPROTO_DCCP:
 			if (!onlyproto && (nh + offset + 4 < skb->data ||
 			     pskb_may_pull(skb, nh + offset + 4 - skb->data))) {
-				__be16 *ports = (__be16 *)exthdr;
+				__be16 *ports;
 
+				nh = skb_network_header(skb);
+				ports = (__be16 *)(nh + offset);
 				fl6->fl6_sport = ports[!!reverse];
 				fl6->fl6_dport = ports[!reverse];
 			}
@@ -180,8 +182,10 @@ _decode_session6(struct sk_buff *skb, struct flowi *fl, int reverse)
 
 		case IPPROTO_ICMPV6:
 			if (!onlyproto && pskb_may_pull(skb, nh + offset + 2 - skb->data)) {
-				u8 *icmp = (u8 *)exthdr;
+				u8 *icmp;
 
+				nh = skb_network_header(skb);
+				icmp = (u8 *)(nh + offset);
 				fl6->fl6_icmp_type = icmp[0];
 				fl6->fl6_icmp_code = icmp[1];
 			}
@@ -192,8 +196,9 @@ _decode_session6(struct sk_buff *skb, struct flowi *fl, int reverse)
 		case IPPROTO_MH:
 			if (!onlyproto && pskb_may_pull(skb, nh + offset + 3 - skb->data)) {
 				struct ip6_mh *mh;
-				mh = (struct ip6_mh *)exthdr;
 
+				nh = skb_network_header(skb);
+				mh = (struct ip6_mh *)(nh + offset);
 				fl6->fl6_mh_type = mh->ip6mh_type;
 			}
 			fl6->flowi6_proto = nexthdr;
diff --git a/net/netfilter/nfnetlink_queue_core.c b/net/netfilter/nfnetlink_queue_core.c
index 108120f216b1..5b169db2049a 100644
--- a/net/netfilter/nfnetlink_queue_core.c
+++ b/net/netfilter/nfnetlink_queue_core.c
@@ -665,7 +665,7 @@ nfqnl_enqueue_packet(struct nf_queue_entry *entry, unsigned int queuenum)
 	 * returned by nf_queue.  For instance, callers rely on -ECANCELED to
 	 * mean 'ignore this hook'.
 	 */
-	if (IS_ERR(segs))
+	if (IS_ERR_OR_NULL(segs))
 		goto out_err;
 	queued = 0;
 	err = 0;
diff --git a/net/openvswitch/datapath.c b/net/openvswitch/datapath.c
index 64dc864a417f..7a9e2a464f3a 100644
--- a/net/openvswitch/datapath.c
+++ b/net/openvswitch/datapath.c
@@ -332,6 +332,8 @@ static int queue_gso_packets(struct datapath *dp, struct sk_buff *skb,
 	segs = __skb_gso_segment(skb, NETIF_F_SG, false);
 	if (IS_ERR(segs))
 		return PTR_ERR(segs);
+	if (segs == NULL)
+		return -EINVAL;
 
 	/* Queue all of the segments. */
 	skb = segs;
diff --git a/net/xfrm/xfrm_output.c b/net/xfrm/xfrm_output.c
index c51e8f7b8653..e44f36057a2a 100644
--- a/net/xfrm/xfrm_output.c
+++ b/net/xfrm/xfrm_output.c
@@ -157,6 +157,8 @@ static int xfrm_output_gso(struct sk_buff *skb)
 	kfree_skb(skb);
 	if (IS_ERR(segs))
 		return PTR_ERR(segs);
+	if (segs == NULL)
+		return -EINVAL;
 
 	do {
 		struct sk_buff *nskb = segs->next;
diff --git a/virt/kvm/iommu.c b/virt/kvm/iommu.c
index 714b94932312..1f0dc1e5f1f0 100644
--- a/virt/kvm/iommu.c
+++ b/virt/kvm/iommu.c
@@ -43,13 +43,13 @@ static void kvm_iommu_put_pages(struct kvm *kvm,
 				gfn_t base_gfn, unsigned long npages);
 
 static pfn_t kvm_pin_pages(struct kvm_memory_slot *slot, gfn_t gfn,
-			   unsigned long size)
+			   unsigned long npages)
 {
 	gfn_t end_gfn;
 	pfn_t pfn;
 
 	pfn     = gfn_to_pfn_memslot(slot, gfn);
-	end_gfn = gfn + (size >> PAGE_SHIFT);
+	end_gfn = gfn + npages;
 	gfn    += 1;
 
 	if (is_error_noslot_pfn(pfn))
@@ -119,7 +119,7 @@ int kvm_iommu_map_pages(struct kvm *kvm, struct kvm_memory_slot *slot)
 		 * Pin all pages we are about to map in memory. This is
 		 * important because we unmap and unpin in 4kb steps later.
 		 */
-		pfn = kvm_pin_pages(slot, gfn, page_size);
+		pfn = kvm_pin_pages(slot, gfn, page_size >> PAGE_SHIFT);
 		if (is_error_noslot_pfn(pfn)) {
 			gfn += 1;
 			continue;
@@ -131,7 +131,7 @@ int kvm_iommu_map_pages(struct kvm *kvm, struct kvm_memory_slot *slot)
 		if (r) {
 			printk(KERN_ERR "kvm_iommu_map_address:"
 			       "iommu failed to map pfn=%llx\n", pfn);
-			kvm_unpin_pages(kvm, pfn, page_size);
+			kvm_unpin_pages(kvm, pfn, page_size >> PAGE_SHIFT);
 			goto unmap_pages;
 		}
 
