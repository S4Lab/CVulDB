seL4__seL4
commit 29695d26367259ad646a278fd8e574e90ea70f29
Author:     Adrian Danis <Adrian.Danis@data61.csiro.au>
AuthorDate: Mon Jan 8 11:17:30 2018 +1100
Commit:     Adrian Danis <Adrian.Danis@data61.csiro.au>
CommitDate: Wed Jan 17 16:38:52 2018 +1100

    x64: SKIM window to mitigate Meltdown (CVE-2017-5754) on x86-64
    
    Introduces a kernel option that, when enabled, reduces the kernel window in a user address
    space to just be Static Kernel Image and Microstate (SKIM), instead of the full kernel
    address space. This isolates the important kernel data from the user preventing a
    Meltdown style attack being able to violate secrecy. The kernel text and read only data,
    i.e. anything that is static from boot, is not secret and can be allowed in the SKIM window
    and potentially read by the user. Additionally to switch to and from the actual kernel
    address space a small amount of state needs to also be in the SKIM window.
    
    This is only an implementation for x86-64, although the same design is applicable to ia32

diff --git a/Kconfig b/Kconfig
index 982b2297..175237f8 100644
--- a/Kconfig
+++ b/Kconfig
@@ -556,7 +556,7 @@ menu "Build Options"
 
     config DANGEROUS_CODE_INJECTION
 	    bool "Build kernel with support for executing arbitrary code in protected mode"
-           depends on !ARM_HYPERVISOR_SUPPORT && !VERIFICATION_BUILD && !PLAT_HIKEY
+           depends on !ARM_HYPERVISOR_SUPPORT && !VERIFICATION_BUILD && !PLAT_HIKEY && !KERNEL_SKIM_WINDOW
         default n
         help
             Adds a system call that allows users to specify code to be run in kernel
diff --git a/config.cmake b/config.cmake
index b1ff240c..996170fe 100644
--- a/config.cmake
+++ b/config.cmake
@@ -242,7 +242,7 @@ config_option(KernelDangerousCodeInjection DANGEROUS_CODE_INJECTION
     "Adds a system call that allows users to specify code to be run in kernel mode. \
     Useful for profiling."
     DEFAULT OFF
-    DEPENDS "NOT KernelARMHypervisorSupport;NOT KernelVerificationBuild;NOT KernelPlatformHikey"
+    DEPENDS "NOT KernelARMHypervisorSupport;NOT KernelVerificationBuild;NOT KernelPlatformHikey;NOT KernelSkimWindow"
 )
 
 config_option(KernelDebugDisablePrefetchers DEBUG_DISABLE_PREFETCHERS
diff --git a/include/arch/x86/arch/64/mode/fastpath/fastpath.h b/include/arch/x86/arch/64/mode/fastpath/fastpath.h
index 3853ef15..7b942f7f 100644
--- a/include/arch/x86/arch/64/mode/fastpath/fastpath.h
+++ b/include/arch/x86/arch/64/mode/fastpath/fastpath.h
@@ -139,12 +139,26 @@ fastpath_restore(word_t badge, word_t msgInfo, tcb_t *cur_thread)
     c_exit_hook();
     lazyFPURestore(cur_thread);
 
+    if (config_set(CONFIG_KERNEL_SKIM_WINDOW)) {
+        /* see restore_user_context for a full explanation of why we do this */
+        word_t *irqstack = x64KSIRQStack[CURRENT_CPU_INDEX()];
+        irqstack[0] = 0;
+        irqstack[1] = 0;
+        irqstack[2] = 0;
+        irqstack[3] = 0;
+        irqstack[4] = 0;
+        irqstack[5] = 0;
+    }
+
 #ifdef CONFIG_HARDWARE_DEBUG_API
     restore_user_debug_context(cur_thread);
 #endif
 
 #ifdef ENABLE_SMP_SUPPORT
     cpu_id_t cpu = getCurrentCPUIndex();
+#ifdef CONFIG_KERNEL_SKIM_WINDOW
+    word_t next_cr3 = MODE_NODE_STATE(x64KSCurrentUserCR3);
+#endif
     swapgs();
 #endif /* ENABLE_SMP_SUPPORT */
     /* Now that we have swapped back to the user gs we can safely
@@ -160,6 +174,9 @@ fastpath_restore(word_t badge, word_t msgInfo, tcb_t *cur_thread)
     if (config_set(CONFIG_SYSENTER)) {
         cur_thread->tcbArch.tcbContext.registers[FLAGS] &= ~FLAGS_IF;
 
+#if defined(ENABLE_SMP_SUPPORT) && defined(CONFIG_KERNEL_SKIM_WINDOW)
+        register word_t next_cr3_r11 asm("r11") = next_cr3;
+#endif
         asm volatile (
             "movq %%rcx, %%rsp\n"
             "popq %%rax\n"
@@ -186,13 +203,26 @@ fastpath_restore(word_t badge, word_t msgInfo, tcb_t *cur_thread)
             "popq %%rcx\n"
             // Skip TLS_BASE FaultIP
             "addq $16, %%rsp\n"
+#if defined(ENABLE_SMP_SUPPORT) && defined(CONFIG_KERNEL_SKIM_WINDOW)
+            "popq %%rsp\n"
+            "movq %%r11, %%cr3\n"
+            "movq %%rsp, %%r11\n"
+#else
             "popq %%r11\n"
+#ifdef CONFIG_KERNEL_SKIM_WINDOW
+            "movq (x64KSCurrentUserCR3), %%rsp\n"
+            "movq %%rsp, %%cr3\n"
+#endif /* CONFIG_KERNEL_SKIM_WINDOW */
+#endif /* defined(ENABLE_SMP_SUPPORT) && defined(CONFIG_KERNEL_SKIM_WINDOW) */
             "sti\n"
             "rex.w sysexit\n"
             :
             : "c" (&cur_thread->tcbArch.tcbContext.registers[RAX]),
             "D" (badge),
             "S" (msgInfo),
+#if defined(ENABLE_SMP_SUPPORT) && defined(CONFIG_KERNEL_SKIM_WINDOW)
+            "r"(next_cr3_r11),
+#endif
             [IF] "i" (FLAGS_IF)
             : "memory"
         );
@@ -214,7 +244,17 @@ fastpath_restore(word_t badge, word_t msgInfo, tcb_t *cur_thread)
             //restore RFLAGS
             "popq %%r11\n"
             // Restore NextIP
+#if defined(ENABLE_SMP_SUPPORT) && defined(CONFIG_KERNEL_SKIM_WINDOW)
+            "popq %%rsp\n"
+            "movq %%rcx, %%cr3\n"
+            "movq %%rsp, %%rcx\n"
+#else
             "popq %%rcx\n"
+#ifdef CONFIG_KERNEL_SKIM_WINDOW
+            "movq (x64KSCurrentUserCR3), %%rsp\n"
+            "movq %%rsp, %%cr3\n"
+#endif /* CONFIG_KERNEL_SKIM_WINDOW */
+#endif /* defined(ENABLE_SMP_SUPPORT) && defined(CONFIG_KERNEL_SKIM_WINDOW) */
             // clear RSP to not leak information to the user
             "xor %%rsp, %%rsp\n"
             // More register but we can ignore and are done restoring
@@ -224,6 +264,9 @@ fastpath_restore(word_t badge, word_t msgInfo, tcb_t *cur_thread)
             : "r"(&cur_thread->tcbArch.tcbContext.registers[RAX]),
             "D" (badge),
             "S" (msgInfo)
+#if defined(ENABLE_SMP_SUPPORT) && defined(CONFIG_KERNEL_SKIM_WINDOW)
+                 ,"c" (next_cr3)
+#endif
             : "memory"
         );
     }
diff --git a/include/arch/x86/arch/64/mode/machine.h b/include/arch/x86/arch/64/mode/machine.h
index 09e43792..2d249257 100644
--- a/include/arch/x86/arch/64/mode/machine.h
+++ b/include/arch/x86/arch/64/mode/machine.h
@@ -17,6 +17,7 @@
 #include <arch/model/statedata.h>
 #include <arch/machine/cpu_registers.h>
 #include <arch/model/smp.h>
+#include <plat_mode/machine/hardware.h>
 
 static inline cr3_t makeCR3(paddr_t addr, word_t pcid)
 {
@@ -26,12 +27,30 @@ static inline cr3_t makeCR3(paddr_t addr, word_t pcid)
 /* Address space control */
 static inline cr3_t getCurrentCR3(void)
 {
+#ifdef CONFIG_KERNEL_SKIM_WINDOW
+    /* If we're running in the kernel to call this function, then by definition
+     * this must be the current cr3 */
+    return cr3_new(kpptr_to_paddr(x64KSKernelPML4), 0);
+#else
     return MODE_NODE_STATE(x64KSCurrentCR3);
+#endif
 }
 
 static inline cr3_t getCurrentUserCR3(void)
 {
+#ifdef CONFIG_KERNEL_SKIM_WINDOW
+    // Construct a cr3_t from the state word, dropping any command information
+    // if needed
+    word_t cr3_word = MODE_NODE_STATE(x64KSCurrentUserCR3);
+    cr3_t cr3_ret;
+    if (config_set(CONFIG_SUPPORT_PCID)) {
+        cr3_word &= ~BIT(63);
+    }
+    cr3_ret.words[0] = cr3_word;
+    return cr3_ret;
+#else
     return getCurrentCR3();
+#endif
 }
 
 static inline paddr_t getCurrentUserVSpaceRoot(void)
@@ -41,7 +60,14 @@ static inline paddr_t getCurrentUserVSpaceRoot(void)
 
 static inline void setCurrentCR3(cr3_t cr3, word_t preserve_translation)
 {
+#ifdef CONFIG_KERNEL_SKIM_WINDOW
+    /* we should only ever be enabling the kernel window, as the bulk of the
+     * cr3 loading when using the SKIM window will happen on kernel entry/exit
+     * in assembly stubs */
+    assert(cr3_get_pml4_base_address(cr3) == kpptr_to_paddr(x64KSKernelPML4));
+#else
     MODE_NODE_STATE(x64KSCurrentCR3) = cr3;
+#endif
     word_t cr3_word = cr3.words[0];
     if (config_set(CONFIG_SUPPORT_PCID)) {
         if (preserve_translation) {
@@ -58,7 +84,18 @@ static inline void setCurrentCR3(cr3_t cr3, word_t preserve_translation)
    If translation needs to be flushed then setCurrentCR3 should be used instead */
 static inline void setCurrentUserCR3(cr3_t cr3)
 {
+#ifdef CONFIG_KERNEL_SKIM_WINDOW
+    // To make the restore stubs more efficient we will set the preserve_translation
+    // command in the state. If we look at the cr3 later on we need to remember to
+    // remove that bit
+    word_t cr3_word = cr3.words[0];
+    if (config_set(CONFIG_SUPPORT_PCID)) {
+        cr3_word |= BIT(63);
+    }
+    MODE_NODE_STATE(x64KSCurrentUserCR3) = cr3_word;
+#else
     setCurrentCR3(cr3, 1);
+#endif
 }
 
 static inline void setCurrentVSpaceRoot(paddr_t addr, word_t pcid)
@@ -68,7 +105,11 @@ static inline void setCurrentVSpaceRoot(paddr_t addr, word_t pcid)
 
 static inline void setCurrentUserVSpaceRoot(paddr_t addr, word_t pcid)
 {
+#ifdef CONFIG_KERNEL_SKIM_WINDOW
+    setCurrentUserCR3(makeCR3(addr, pcid));
+#else
     setCurrentVSpaceRoot(addr, pcid);
+#endif
 }
 
 /* GDT installation */
diff --git a/include/arch/x86/arch/64/mode/model/smp.h b/include/arch/x86/arch/64/mode/model/smp.h
index c9baf99d..3a5b235b 100644
--- a/include/arch/x86/arch/64/mode/model/smp.h
+++ b/include/arch/x86/arch/64/mode/model/smp.h
@@ -38,6 +38,18 @@ compile_assert(nodeInfoIsCacheSized, (sizeof(nodeInfo_t) % L1_CACHE_LINE_SIZE) =
 
 extern nodeInfo_t node_info[CONFIG_MAX_NUM_NODES] ALIGN(L1_CACHE_LINE_SIZE);
 
+#ifdef CONFIG_KERNEL_SKIM_WINDOW
+/* we only need 1 word of scratch space, which we know we will at least get by the size
+ 8 of the nodeInfo_t struct so we define this array as char to ensure it is sized correctly.
+ We need each element of the array to be precisely the same size as each element of node_info
+ so that the offset between the element of each array is a constant */
+extern char nodeSkimScratch[CONFIG_MAX_NUM_NODES][sizeof(nodeInfo_t)] ALIGN(L1_CACHE_LINE_SIZE) VISIBLE SKIM_BSS;
+compile_assert(nodeInfoAndScratchSameSize, sizeof(node_info) == sizeof(nodeSkimScratch))
+/* this will be declared in the linker script as the offset between node_info and
+ * nodeSkimScratch */
+extern char nodeSkimScratchOffset[];
+#endif
+
 static inline CONST cpu_id_t getCurrentCPUIndex(void)
 {
     cpu_id_t index;
diff --git a/include/arch/x86/arch/64/mode/model/statedata.h b/include/arch/x86/arch/64/mode/model/statedata.h
index 2c25f785..4bc50436 100644
--- a/include/arch/x86/arch/64/mode/model/statedata.h
+++ b/include/arch/x86/arch/64/mode/model/statedata.h
@@ -13,11 +13,12 @@
 #ifndef __ARCH_MODE_MODEL_STATEDATA_H_
 #define __ARCH_MODE_MODEL_STATEDATA_H_
 
+#include <config.h>
 #include <object/structures.h>
 #include <arch/types.h>
 #include <model/statedata.h>
 
-extern pml4e_t x64KSKernelPML4[BIT(PML4_INDEX_BITS)];
+extern pml4e_t x64KSKernelPML4[BIT(PML4_INDEX_BITS)] VISIBLE;
 extern pdpte_t x64KSKernelPDPT[BIT(PDPT_INDEX_BITS)];
 #ifdef CONFIG_HUGE_PAGE
 extern pde_t x64KSKernelPD[BIT(PD_INDEX_BITS)];
@@ -26,8 +27,23 @@ extern pde_t x64KSKernelPDs[BIT(PDPT_INDEX_BITS)][BIT(PD_INDEX_BITS)];
 #endif
 extern pte_t x64KSKernelPT[BIT(PT_INDEX_BITS)];
 
+#ifdef CONFIG_KERNEL_SKIM_WINDOW
+extern pml4e_t x64KSSKIMPML4[BIT(PML4_INDEX_BITS)] ALIGN(BIT(seL4_PML4Bits));
+extern pdpte_t x64KSSKIMPDPT[BIT(PDPT_INDEX_BITS)] ALIGN(BIT(seL4_PDPTBits));
+/* we only need a single PD regardless of huge pages or not as the skim window
+   just has a few 2M entries out of the 1gb region of the kernel image */
+extern pde_t x64KSSKIMPD[BIT(PD_INDEX_BITS)] ALIGN(BIT(seL4_PageDirBits));
+#endif
+
 NODE_STATE_BEGIN(modeNodeState)
+#ifdef CONFIG_KERNEL_SKIM_WINDOW
+/* we declare this as a word_t and not a cr3_t as we cache both state and potentially
+ * command information (state being pml4 base and pcid) and command being whether or not
+ * to flush translation. the formal cr3_t type only talks about the state */
+NODE_STATE_DECLARE(word_t, x64KSCurrentUserCR3);
+#else
 NODE_STATE_DECLARE(cr3_t, x64KSCurrentCR3);
+#endif
 NODE_STATE_END(modeNodeState);
 
 /* hardware interrupt handlers push up to 6 words onto the stack. The order of the
@@ -38,6 +54,6 @@ NODE_STATE_END(modeNodeState);
  macro as that *could* be configured to be less than 16, which would be incorrect
  for us here */
 #define IRQ_STACK_SIZE 6
-extern word_t x64KSIRQStack[CONFIG_MAX_NUM_NODES][IRQ_STACK_SIZE + 2] ALIGN(64) VISIBLE;
+extern word_t x64KSIRQStack[CONFIG_MAX_NUM_NODES][IRQ_STACK_SIZE + 2] ALIGN(64) VISIBLE SKIM_BSS;
 
 #endif /* __ARCH_MODE_MODEL_STATEDATA_H_ */
diff --git a/include/arch/x86/arch/64/mode/object/structures.h b/include/arch/x86/arch/64/mode/object/structures.h
index 11a8c549..008c3e23 100644
--- a/include/arch/x86/arch/64/mode/object/structures.h
+++ b/include/arch/x86/arch/64/mode/object/structures.h
@@ -41,7 +41,12 @@ compile_assert(unsinged_int_size_32,
 compile_assert(uint64_t_size_64,
                sizeof(uint64_t) == 8)
 
+#ifdef CONFIG_KERNEL_SKIM_WINDOW
+#define X86_GLOBAL_VSPACE_ROOT x64KSSKIMPML4
+#else
 #define X86_GLOBAL_VSPACE_ROOT x64KSKernelPML4
+#endif
+
 #define X86_KERNEL_VSPACE_ROOT x64KSKernelPML4
 
 #define PML4E_SIZE_BITS seL4_PML4EntryBits
diff --git a/include/arch/x86/arch/kernel/vspace.h b/include/arch/x86/arch/kernel/vspace.h
index c28697a9..a3dd02dd 100644
--- a/include/arch/x86/arch/kernel/vspace.h
+++ b/include/arch/x86/arch/kernel/vspace.h
@@ -46,6 +46,7 @@ bool_t map_kernel_window(
     uint32_t   num_drhu,
     paddr_t*   drhu_list
 );
+bool_t map_skim_window(vptr_t skim_start, vptr_t skim_end);
 bool_t map_kernel_window_devices(
     pte_t *pt,
     uint32_t num_ioapic,
diff --git a/include/arch/x86/arch/model/statedata.h b/include/arch/x86/arch/model/statedata.h
index d1aadece..2527f80e 100644
--- a/include/arch/x86/arch/model/statedata.h
+++ b/include/arch/x86/arch/model/statedata.h
@@ -68,7 +68,7 @@ typedef struct x86_arch_global_state {
 } x86_arch_global_state_t;
 compile_assert(x86_arch_global_state_padded, (sizeof(x86_arch_global_state_t) % L1_CACHE_LINE_SIZE) == 0)
 
-extern x86_arch_global_state_t x86KSGlobalState[CONFIG_MAX_NUM_NODES] ALIGN(L1_CACHE_LINE_SIZE);
+extern x86_arch_global_state_t x86KSGlobalState[CONFIG_MAX_NUM_NODES] ALIGN(L1_CACHE_LINE_SIZE) SKIM_BSS;
 
 extern asid_pool_t* x86KSASIDTable[];
 extern uint32_t x86KScacheLineSizeBits;
diff --git a/include/arch/x86/arch/object/vcpu.h b/include/arch/x86/arch/object/vcpu.h
index f3d20961..06233916 100644
--- a/include/arch/x86/arch/object/vcpu.h
+++ b/include/arch/x86/arch/object/vcpu.h
@@ -304,8 +304,10 @@ struct vcpu {
     /* Last used EPT root */
     word_t last_ept_root;
 
+#ifndef CONFIG_KERNEL_SKIM_WINDOW
     /* Last set host cr3 */
     word_t last_host_cr3;
+#endif
 
 #ifdef ENABLE_SMP_SUPPORT
     /* Core this VCPU was last loaded on, or is currently loaded on */
diff --git a/include/linker.h b/include/linker.h
index e7cd49bf..88aab73a 100644
--- a/include/linker.h
+++ b/include/linker.h
@@ -29,4 +29,11 @@
 /* data will be aligned to n bytes in a special BSS section */
 #define ALIGN_BSS(n) ALIGN(n) SECTION(".bss.aligned")
 
+/* data that will be mapped into and permitted to be used in the restricted SKIM
+ * address space */
+#define SKIM_DATA SECTION(".skim.data")
+
+/* bss data that is permitted to be used in the restricted SKIM address space */
+#define SKIM_BSS SECTION(".skim.bss")
+
 #endif /* __LINKER_H */
diff --git a/src/arch/x86/64/c_traps.c b/src/arch/x86/64/c_traps.c
index d23d581e..7a7b2bab 100644
--- a/src/arch/x86/64/c_traps.c
+++ b/src/arch/x86/64/c_traps.c
@@ -170,8 +170,11 @@ void VISIBLE NORETURN restore_user_context(void)
 
 #ifdef ENABLE_SMP_SUPPORT
     cpu_id_t cpu = getCurrentCPUIndex();
+#ifdef CONFIG_KERNEL_SKIM_WINDOW
+    word_t user_cr3 = MODE_NODE_STATE(x64KSCurrentUserCR3);
+#endif /* CONFIG_KERNEL_SKIM_WINDOW */
     swapgs();
-#endif
+#endif /* ENABLE_SMP_SUPPORT */
 
     /* Now that we have swapped back to the user gs we can safely
      * update the GS base. We must *not* use any kernel functions
@@ -188,8 +191,27 @@ void VISIBLE NORETURN restore_user_context(void)
     // but are current singlestepping, do a full return like an interrupt
     if (likely(cur_thread->tcbArch.tcbContext.registers[Error] == -1) &&
             (!config_set(CONFIG_SYSENTER) || !config_set(CONFIG_HARDWARE_DEBUG_API) || ((cur_thread->tcbArch.tcbContext.registers[FLAGS] & FLAGS_TF) == 0))) {
+        if (config_set(CONFIG_KERNEL_SKIM_WINDOW)) {
+            /* if we are using the SKIM window then we are trying to hide kernel state from
+             * the user in the case of Meltdown where the kernel region is effectively readable
+             * by the user. To prevent a storage channel across threads through the irq stack,
+             * which is idirectly controlled by the user, we need to clear the stack. We perform
+             * this here since when we return *from* an interrupt we must use this stack and
+             * cannot clear it. This means if we restore from interrupt, then enter from a syscall
+             * and switch to a different thread we must either on syscall entry, or before leaving
+             * the kernel, clear the irq stack. */
+            irqstack[0] = 0;
+            irqstack[1] = 0;
+            irqstack[2] = 0;
+            irqstack[3] = 0;
+            irqstack[4] = 0;
+            irqstack[5] = 0;
+        }
         if (config_set(CONFIG_SYSENTER)) {
             cur_thread->tcbArch.tcbContext.registers[FLAGS] &= ~FLAGS_IF;
+#if defined(ENABLE_SMP_SUPPORT) && defined(CONFIG_KERNEL_SKIM_WINDOW)
+            register word_t user_cr3_r11 asm("r11") = user_cr3;
+#endif
             asm volatile(
                 // Set our stack pointer to the top of the tcb so we can efficiently pop
                 "movq %0, %%rsp\n"
@@ -219,7 +241,17 @@ void VISIBLE NORETURN restore_user_context(void)
                 "popq %%rcx\n"
                 // Skip TLS_BASE, FaultIP
                 "addq $16, %%rsp\n"
+#if defined(ENABLE_SMP_SUPPORT) && defined(CONFIG_KERNEL_SKIM_WINDOW)
+                "popq %%rsp\n"
+                "movq %%r11, %%cr3\n"
+                "movq %%rsp, %%r11\n"
+#else
                 "popq %%r11\n"
+#ifdef CONFIG_KERNEL_SKIM_WINDOW
+                "movq (x64KSCurrentUserCR3), %%rsp\n"
+                "movq %%rsp, %%cr3\n"
+#endif /* CONFIG_KERNEL_SKIM_WINDOW */
+#endif /* defined(ENABLE_SMP_SUPPORT) && defined(CONFIG_KERNEL_SKIM_WINDOW) */
                 // More register but we can ignore and are done restoring
                 // enable interrupt disabled by sysenter
                 "sti\n"
@@ -230,6 +262,9 @@ void VISIBLE NORETURN restore_user_context(void)
                 "rex.w sysexit\n"
                 :
                 : "r"(&cur_thread->tcbArch.tcbContext.registers[RDI]),
+#if defined(ENABLE_SMP_SUPPORT) && defined(CONFIG_KERNEL_SKIM_WINDOW)
+                  "r"(user_cr3_r11),
+#endif
                 [IF] "i" (FLAGS_IF)
                 // Clobber memory so the compiler is forced to complete all stores
                 // before running this assembler
@@ -255,7 +290,17 @@ void VISIBLE NORETURN restore_user_context(void)
                 //restore RFLAGS
                 "popq %%r11\n"
                 // Restore NextIP
+#if defined(ENABLE_SMP_SUPPORT) && defined(CONFIG_KERNEL_SKIM_WINDOW)
+                "popq %%rsp\n"
+                "movq %%rcx, %%cr3\n"
+                "movq %%rsp, %%rcx\n"
+#else
                 "popq %%rcx\n"
+#ifdef CONFIG_KERNEL_SKIM_WINDOW
+                "movq (x64KSCurrentUserCR3), %%rsp\n"
+                "movq %%rsp, %%cr3\n"
+#endif /* CONFIG_KERNEL_SKIM_WINDOW */
+#endif /* defined(ENABLE_SMP_SUPPORT) && defined(CONFIG_KERNEL_SKIM_WINDOW) */
                 // clear RSP to not leak information to the user
                 "xor %%rsp, %%rsp\n"
                 // More register but we can ignore and are done restoring
@@ -263,6 +308,9 @@ void VISIBLE NORETURN restore_user_context(void)
                 "rex.w sysret\n"
                 :
                 : "r"(&cur_thread->tcbArch.tcbContext.registers[RDI])
+#if defined(ENABLE_SMP_SUPPORT) && defined(CONFIG_KERNEL_SKIM_WINDOW)
+                 ,"c" (user_cr3)
+#endif
                 // Clobber memory so the compiler is forced to complete all stores
                 // before running this assembler
                 : "memory"
@@ -270,6 +318,10 @@ void VISIBLE NORETURN restore_user_context(void)
         }
     } else {
         /* construct our return from interrupt frame */
+#ifdef CONFIG_KERNEL_SKIM_WINDOW
+        /* Have to zero this to prevent storage channel */
+        irqstack[0] = 0;
+#endif
         irqstack[1] = getRegister(cur_thread, NextIP);
         irqstack[2] = getRegister(cur_thread, CS);
         irqstack[3] = getRegister(cur_thread, FLAGS);
@@ -294,21 +346,48 @@ void VISIBLE NORETURN restore_user_context(void)
             /* skip RFLAGS, Error NextIP RSP, TLS_BASE, FaultIP */
             "addq $48, %%rsp\n"
             "popq %%r11\n"
+
+#if defined(ENABLE_SMP_SUPPORT) && defined(CONFIG_KERNEL_SKIM_WINDOW)
+            /* pop into rsp as we're done with the stack for now and we need to
+             * preserve our rcx value as it has our next cr3 value */
+            "popq %%rsp\n"
+#else
             "popq %%rcx\n"
+#endif /* defined(ENABLE_SMP_SUPPORT) && defined(CONFIG_KERNEL_SKIM_WINDOW) */
+
 #ifdef ENABLE_SMP_SUPPORT
             // Swapping gs twice here is worth it as it allows us to efficiently
             // set the user gs base previously
             "swapgs\n"
+#ifdef CONFIG_KERNEL_SKIM_WINDOW
+            /* now we stash rcx in the scratch space that we can access once we've
+             * loaded the user cr3 */
+            "movq %%rsp, %%gs:%c[scratch_offset]\n"
+#endif /* CONFIG_KERNEL_SKIM_WINDOW */
             "movq %%gs:8, %%rsp\n"
+#ifdef CONFIG_KERNEL_SKIM_WINDOW
+            /* change to the user address space and then load the value of rcx that
+             * we stashed */
+            "movq %%rcx, %%cr3\n"
+            "movq %%gs:%c[scratch_offset], %%rcx\n"
+#endif /* CONFIG_KERNEL_SKIM_WINDOW */
             "addq $8, %%rsp\n"
             // Switch to the user GS value
             "swapgs\n"
-#else
+#else /* !ENABLE_SMP_SUPPORT */
+#ifdef CONFIG_KERNEL_SKIM_WINDOW
+            "movq (x64KSCurrentUserCR3), %%rsp\n"
+            "movq %%rsp, %%cr3\n"
+#endif /* CONFIG_KERNEL_SKIM_WINDOW */
             "leaq x64KSIRQStack + 8, %%rsp\n"
-#endif
+#endif /* ENABLE_SMP_SUPPORT */
             "iretq\n"
             :
             : "r"(&cur_thread->tcbArch.tcbContext.registers[RDI])
+#if defined(ENABLE_SMP_SUPPORT) && defined(CONFIG_KERNEL_SKIM_WINDOW)
+             ,"c" (user_cr3)
+             ,[scratch_offset] "i" (nodeSkimScratchOffset)
+#endif
             // Clobber memory so the compiler is forced to complete all stores
             // before running this assembler
             : "memory"
diff --git a/src/arch/x86/64/kernel/vspace.c b/src/arch/x86/64/kernel/vspace.c
index 9a909fb0..921c2a13 100644
--- a/src/arch/x86/64/kernel/vspace.c
+++ b/src/arch/x86/64/kernel/vspace.c
@@ -21,6 +21,12 @@
 #include <mode/kernel/tlb.h>
 #include <arch/kernel/tlb_bitmap.h>
 
+/* When using the SKIM window to isolate the kernel from the user we also need to
+ * not use global mappings as having global mappings and entries in the TLB is
+ * equivalent, for the purpose of exploitation, to having the mappings in the
+ * kernel window */
+#define KERNEL_IS_GLOBAL() (config_set(CONFIG_KERNEL_SKIM_WINDOW) ? 0 : 1)
+
 /* For the boot code we create two windows into the physical address space
  * One is at the same location as the kernel window, and is placed up high
  * The other is a 1-to-1 mapping of the first 512gb of memory. The purpose
@@ -76,7 +82,7 @@ map_kernel_window(
                                                        0, /* xd */
                                                        PADDR_BASE,
                                                        0, /* PAT */
-                                                       1, /* global */
+                                                       KERNEL_IS_GLOBAL(), /* global */
                                                        0, /* dirty */
                                                        0, /* accessed */
                                                        0, /* cache_disabled */
@@ -96,7 +102,7 @@ map_kernel_window(
                                            0,          /* xd               */
                                            paddr,      /* physical address */
                                            0,          /* PAT              */
-                                           1,          /* global           */
+                                           KERNEL_IS_GLOBAL(), /* global   */
                                            0,          /* dirty            */
                                            0,          /* accessed         */
                                            0,          /* cache_disabled   */
@@ -195,7 +201,7 @@ map_kernel_window(
                                                   0, /* xd */
                                                   paddr,
                                                   0, /* pat */
-                                                  1, /* global */
+                                                  KERNEL_IS_GLOBAL(), /* global */
                                                   0, /* dirty */
                                                   0, /* accessed */
                                                   0, /* cache disabled */
@@ -255,6 +261,56 @@ map_kernel_window(
     return true;
 }
 
+#ifdef CONFIG_KERNEL_SKIM_WINDOW
+BOOT_CODE bool_t
+map_skim_window(vptr_t skim_start, vptr_t skim_end)
+{
+    /* place the PDPT into the PML4 */
+    x64KSSKIMPML4[GET_PML4_INDEX(PPTR_BASE)] = pml4e_new(
+                                                     0, /* xd */
+                                                     kpptr_to_paddr(x64KSSKIMPDPT),
+                                                     0, /* accessed */
+                                                     0, /* cache_disabled */
+                                                     0, /* write_through */
+                                                     0, /* super_user */
+                                                     1, /* read_write */
+                                                     1  /* present */
+                                                 );
+    /* place the PD into the kernel_base slot of the PDPT */
+    x64KSSKIMPDPT[GET_PDPT_INDEX(KERNEL_BASE)] = pdpte_pdpte_pd_new(
+                                                       0, /* xd */
+                                                       kpptr_to_paddr(x64KSSKIMPD),
+                                                       0, /* accessed */
+                                                       0, /* cache_disabled */
+                                                       0, /* write_through */
+                                                       0, /* super_user */
+                                                       1, /* read_write */
+                                                       1  /* present */
+                                                   );
+    /* map the skim portion into the PD. we expect it to be 2M aligned */
+    assert((skim_start % BIT(seL4_LargePageBits)) == 0);
+    assert((skim_end % BIT(seL4_LargePageBits)) == 0);
+    uint64_t paddr = kpptr_to_paddr((void*)skim_start);
+    for (int i = GET_PD_INDEX(skim_start); i < GET_PD_INDEX(skim_end); i++) {
+        x64KSSKIMPD[i] = pde_pde_large_new(
+                                        0, /* xd */
+                                        paddr,
+                                        0, /* pat */
+                                        KERNEL_IS_GLOBAL(), /* global */
+                                        0, /* dirty */
+                                        0, /* accessed */
+                                        0, /* cache_disabled */
+                                        0, /* write_through */
+                                        0, /* super_user */
+                                        1, /* read_write */
+                                        1  /* present */
+                                    );
+        paddr += BIT(seL4_LargePageBits);
+    }
+    return true;
+}
+#endif
+
 BOOT_CODE void
 init_tss(tss_t *tss)
 {
diff --git a/src/arch/x86/64/model/smp.c b/src/arch/x86/64/model/smp.c
index 16c4af73..4ec25cf7 100644
--- a/src/arch/x86/64/model/smp.c
+++ b/src/arch/x86/64/model/smp.c
@@ -16,6 +16,7 @@
 #ifdef ENABLE_SMP_SUPPORT
 
 nodeInfo_t node_info[CONFIG_MAX_NUM_NODES] ALIGN(L1_CACHE_LINE_SIZE);
+char nodeSkimScratch[CONFIG_MAX_NUM_NODES][sizeof(nodeInfo_t)] ALIGN(L1_CACHE_LINE_SIZE);
 extern char kernel_stack_alloc[CONFIG_MAX_NUM_NODES][BIT(CONFIG_KERNEL_STACK_BITS)];
 
 BOOT_CODE void
diff --git a/src/arch/x86/64/model/statedata.c b/src/arch/x86/64/model/statedata.c
index 232f86f5..4add0042 100644
--- a/src/arch/x86/64/model/statedata.c
+++ b/src/arch/x86/64/model/statedata.c
@@ -13,7 +13,7 @@
 #include <arch/model/statedata.h>
 
 /* The privileged kernel mapping PD & PT */
-pml4e_t x64KSKernelPML4[BIT(PML4_INDEX_BITS)] ALIGN(BIT(seL4_PML4Bits));
+pml4e_t x64KSKernelPML4[BIT(PML4_INDEX_BITS)] ALIGN(BIT(seL4_PML4Bits)) VISIBLE;
 pdpte_t x64KSKernelPDPT[BIT(PDPT_INDEX_BITS)] ALIGN(BIT(seL4_PDPTBits));
 #ifdef CONFIG_HUGE_PAGE
 pde_t x64KSKernelPD[BIT(PD_INDEX_BITS)] ALIGN(BIT(seL4_PageDirBits));
@@ -22,6 +22,16 @@ pde_t x64KSKernelPDs[BIT(PDPT_INDEX_BITS)][BIT(PD_INDEX_BITS)] ALIGN(BIT(seL4_Pa
 #endif
 pte_t x64KSKernelPT[BIT(PT_INDEX_BITS)] ALIGN(BIT(seL4_PageTableBits));
 
+#ifdef CONFIG_KERNEL_SKIM_WINDOW
+pml4e_t x64KSSKIMPML4[BIT(PML4_INDEX_BITS)] ALIGN(BIT(seL4_PML4Bits));
+pdpte_t x64KSSKIMPDPT[BIT(PDPT_INDEX_BITS)] ALIGN(BIT(seL4_PDPTBits));
+pde_t x64KSSKIMPD[BIT(PD_INDEX_BITS)] ALIGN(BIT(seL4_PageDirBits));
+#endif
+
+#ifdef CONFIG_KERNEL_SKIM_WINDOW
+UP_STATE_DEFINE(word_t, x64KSCurrentUserCR3);
+#else
 UP_STATE_DEFINE(cr3_t, x64KSCurrentCR3);
+#endif
 
-word_t x64KSIRQStack[CONFIG_MAX_NUM_NODES][IRQ_STACK_SIZE + 2] ALIGN(64) VISIBLE;
+word_t x64KSIRQStack[CONFIG_MAX_NUM_NODES][IRQ_STACK_SIZE + 2] ALIGN(64) VISIBLE SKIM_BSS;
diff --git a/src/arch/x86/64/traps.S b/src/arch/x86/64/traps.S
index c5b99ecf..8209cde9 100644
--- a/src/arch/x86/64/traps.S
+++ b/src/arch/x86/64/traps.S
@@ -12,6 +12,7 @@
 
 #include <machine/assembler.h>
 #include <config.h>
+#include <plat_mode/machine/hardware.h>
 
 /*
  * The exception in 64-bit mode:
@@ -80,6 +81,27 @@
 #define LOAD_IRQ_STACK(x)  leaq    x64KSIRQStack, %x
 #endif
 
+#ifdef CONFIG_KERNEL_SKIM_WINDOW
+/* If using PCIDs then our final value is not an address with a valid 32-bit relocation for
+ * the linker, but rather requires a full 64-bit representation. To work around this we
+ * use movabs to generate a full 64-bit immediate if using PCID, but lea if not. We prefer
+ * lea where possible as it has a more efficient instruction representation
+ */
+#ifdef CONFIG_SUPPORT_PCID
+#define LOAD_KERNEL_AS(reg) \
+    movabs $x64KSKernelPML4 - KERNEL_BASE_OFFSET + (1 << 63), %reg; \
+    movq %reg, %cr3;
+#else /* !CONFIG_SUPPORT_PCID */
+#define LOAD_KERNEL_AS(reg) \
+    lea x64KSKernelPML4 - KERNEL_BASE_OFFSET, %reg; \
+    movq %reg, %cr3;
+#endif /* CONFIG_SUPPORT_PCID */
+#else /* !CONFIG_KERNEL_SKIM_WINDOW */
+
+#define LOAD_KERNEL_AS(reg)
+
+#endif /* CONFIG_KERNEL_SKIM_WINDOW */
+
 /* Registers to be pushed after an interrupt
    %rcx must be pushed beforehand */
 #define INT_SAVE_STATE                              \
@@ -138,6 +160,7 @@ int_##number:                                       \
     testq   $3, 16(%rsp);                           \
     jz      1f;                                     \
 2:                                                  \
+    LOAD_KERNEL_AS(rsp)                             \
     /* we need to not skip RSP, TLS_BASE, FaultIP, R11 and RCX for now */ \
     MAYBE_SWAPGS;                                   \
     LOAD_USER_CONTEXT_OFFSET(5);                    \
@@ -570,6 +593,7 @@ END_FUNC(kernel_exception)
 # the current RFLAGS (after saving) have been masked
 # with IA32_FMASK.
 BEGIN_FUNC(handle_fastsyscall)
+    LOAD_KERNEL_AS(rsp)
     MAYBE_SWAPGS
     LOAD_USER_CONTEXT
     pushq   $-1             # set Error -1 to mean entry via syscall
@@ -605,6 +629,7 @@ END_FUNC(handle_fastsyscall)
 #   RSP : NULL
 BEGIN_FUNC(handle_syscall)
     /* We need to save r11, rdx TLS_BASE and RSP */
+    LOAD_KERNEL_AS(rsp)
     MAYBE_SWAPGS
     LOAD_USER_CONTEXT_OFFSET(4)
     push    %r11
diff --git a/src/arch/x86/config.cmake b/src/arch/x86/config.cmake
index 7871e1d8..d0a890b4 100644
--- a/src/arch/x86/config.cmake
+++ b/src/arch/x86/config.cmake
@@ -235,6 +235,15 @@ config_option(KernelMultiboot2Header MULTIBOOT2_HEADER
     DEPENDS "KernelArchX86"
 )
 
+config_option(KernelSkimWindow KERNEL_SKIM_WINDOW
+    "Prevent against the Meltdown vulnerability by using a reduced Static Kernel
+    Image and Micro-state window instead of having all kernel state in the kernel window.
+    This only needs to be enabled if deploying to a vulnerable processor"
+    DEFAULT ON
+    DEPENDS "KernelSel4ArchX86_64"
+    DEFAULT_DISABLED OFF
+)
+
 config_option(KernelExportPMCUser EXPORT_PMC_USER
     "Grant user access to the Performance Monitoring Counters.
     This allows the user to read performance counters, although
diff --git a/src/arch/x86/kernel/boot_sys.c b/src/arch/x86/kernel/boot_sys.c
index d9ed3281..479235df 100644
--- a/src/arch/x86/kernel/boot_sys.c
+++ b/src/arch/x86/kernel/boot_sys.c
@@ -41,6 +41,8 @@ extern char boot_stack_top[1];
 /* locations in kernel image */
 extern char ki_boot_end[1];
 extern char ki_end[1];
+extern char ki_skim_start[1];
+extern char ki_skim_end[1];
 
 #ifdef CONFIG_PRINTING
 /* kernel entry point */
@@ -209,6 +211,12 @@ try_boot_sys_node(cpu_id_t cpu_id)
      * be set *right now* instead of delayed */
     asm volatile("" ::: "memory");
 
+#ifdef CONFIG_KERNEL_SKIM_WINDOW
+    if (!map_skim_window((vptr_t)ki_skim_start, (vptr_t)ki_skim_end)) {
+        return false;
+    }
+#endif
+
     /* reuse boot code/data memory */
     boot_mem_reuse_p_reg.start = PADDR_LOAD;
     boot_mem_reuse_p_reg.end = (paddr_t)ki_boot_end - KERNEL_BASE_OFFSET;
@@ -405,6 +413,34 @@ try_boot_sys(void)
         printf("Warning: Your kernel was not compiled for the current microarchitecture.\n");
     }
 
+    cpuid_007h_edx_t edx;
+    edx.words[0] = x86_cpuid_edx(0x7, 0);
+    /* see if we can definitively say whether or not we need the skim window by
+     * checking whether the CPU is vulnerable to rogue data cache loads (rdcl) */
+    if (cpuid_007h_edx_get_ia32_arch_cap_msr(edx)) {
+        ia32_arch_capabilities_msr_t cap_msr;
+        cap_msr.words[0] = x86_rdmsr(IA32_ARCH_CAPABILITIES_MSR);
+        if (ia32_arch_capabilities_msr_get_rdcl_no(cap_msr) && config_set(CONFIG_KERNEL_SKIM_WINDOW)) {
+            printf("CPU reports not vulnerable to Rogue Data Cache Load (aka Meltdown https://meltdownattack.com) "
+                   "yet SKIM window is enabled. Performance is needlessly being impacted, consider disabling.\n");
+        } else if (!ia32_arch_capabilities_msr_get_rdcl_no(cap_msr) && !config_set(CONFIG_KERNEL_SKIM_WINDOW)) {
+            printf("CPU reports vulernable to Rogue Data Cache Load (aka Meltdown https://meltdownattack.com) "
+                   "yet SKIM window is *not* enabled. Please re-build with SKIM window enabled.");
+            return false;
+        }
+    } else {
+        /* hardware doesn't tell us directly so guess based on CPU vendor */
+        if (config_set(CONFIG_KERNEL_SKIM_WINDOW) && x86_cpuid_get_identity()->vendor == X86_VENDOR_AMD) {
+            printf("SKIM window for mitigating Meltdown (https://www.meltdownattack.com) "
+                    "not necessary for AMD and performance is being needlessly affected, "
+                    "consider disabling\n");
+        }
+        if (!config_set(CONFIG_KERNEL_SKIM_WINDOW) && x86_cpuid_get_identity()->vendor == X86_VENDOR_INTEL) {
+            printf("***WARNING*** SKIM window not enabled, this machine is probably vulernable "
+                   "to Meltdown (https://www.meltdownattack.com), consider enabling\n");
+        }
+    }
+
 #ifdef ENABLE_SMP_SUPPORT
     /* copy boot code for APs to lower memory to run in real mode */
     if (!copy_boot_code_aps(boot_state.mem_lower)) {
diff --git a/src/arch/x86/model/statedata.c b/src/arch/x86/model/statedata.c
index fbfd3dae..b508880c 100644
--- a/src/arch/x86/model/statedata.c
+++ b/src/arch/x86/model/statedata.c
@@ -24,7 +24,7 @@ UP_STATE_DEFINE(interrupt_t, x86KSPendingInterrupt);
 
 /* ==== proper read/write kernel state ==== */
 
-x86_arch_global_state_t x86KSGlobalState[CONFIG_MAX_NUM_NODES] ALIGN(L1_CACHE_LINE_SIZE);
+x86_arch_global_state_t x86KSGlobalState[CONFIG_MAX_NUM_NODES] ALIGN(L1_CACHE_LINE_SIZE) SKIM_BSS;
 
 /* The top level ASID table */
 asid_pool_t* x86KSASIDTable[BIT(asidHighBits)];
diff --git a/src/arch/x86/object/vcpu.c b/src/arch/x86/object/vcpu.c
index 058f5125..176de1cd 100644
--- a/src/arch/x86/object/vcpu.c
+++ b/src/arch/x86/object/vcpu.c
@@ -451,6 +451,11 @@ vcpu_init(vcpu_t *vcpu)
     /* Set host SP to point just beyond the first field to be stored on exit. */
     vmwrite(VMX_HOST_RSP, (word_t)&vcpu->gp_registers[n_vcpu_gp_register]);
     vmwrite(VMX_HOST_RIP, (word_t)&handle_vmexit);
+    if (config_set(CONFIG_KERNEL_SKIM_WINDOW)) {
+        /* if we have a skim window then our host cr3 is a constant and is always the
+         * the kernel address space, so we set it here instead of lazily in restoreVMCS */
+        vmwrite(VMX_HOST_CR3, makeCR3(kpptr_to_paddr(x64KSKernelPML4), 0).words[0]);
+    }
 
     vmwrite(VMX_HOST_ES_SELECTOR, SEL_DS_0);
     vmwrite(VMX_HOST_CS_SELECTOR, SEL_CS_0);
@@ -1534,10 +1539,12 @@ restoreVMCS(void)
         switchVCPU(expected_vmcs);
     }
 
+#ifndef CONFIG_KERNEL_SKIM_WINDOW
     if (getCurrentCR3().words[0] != expected_vmcs->last_host_cr3) {
         expected_vmcs->last_host_cr3 = getCurrentCR3().words[0];
         vmwrite(VMX_HOST_CR3, getCurrentCR3().words[0]);
     }
+#endif
     if (expected_vmcs->vpid == VPID_INVALID) {
         vpid_t vpid = findFreeVPID();
         storeVPID(expected_vmcs, vpid);
diff --git a/src/plat/pc99/Kconfig b/src/plat/pc99/Kconfig
index 517ccae7..865d2ecb 100644
--- a/src/plat/pc99/Kconfig
+++ b/src/plat/pc99/Kconfig
@@ -300,6 +300,15 @@ depends on PLAT_PC99
             Value of zero indicates no preference
 endmenu
 
+config KERNEL_SKIM_WINDOW
+    bool "Kernel Static Kernel Image and Micro-state (SKIM) window"
+    depends on ARCH_X86_64
+    default y
+    help
+        Prevent against the Meltdown vulnerability by using a reduced Static Kernel
+        Image and Micro-state window instead of having all kernel state in the kernel window.
+        This only needs to be enabled if deploying to a vulnerable processor
+
 config EXPORT_PMC_USER
     bool "User access to PMC"
     depends on ARCH_X86 && !VERIFICATION_BUILD
diff --git a/src/plat/pc99/linker.lds b/src/plat/pc99/linker.lds
index 5b668545..e7a65dfa 100644
--- a/src/plat/pc99/linker.lds
+++ b/src/plat/pc99/linker.lds
@@ -25,6 +25,9 @@ PADDR_BASE = 0x00000000;
 PADDR_LOAD = 0x00100000;
 KERNEL_BASE = 0xffffffff80000000;
 OUTPUT_FORMAT(elf64-x86-64)
+#if defined(ENABLE_SMP_SUPPORT) && defined(CONFIG_KERNEL_SKIM_WINDOW)
+nodeSkimScratchOffset = nodeSkimScratch - node_info;
+#endif
 #endif
 
 KERNEL_OFFSET = KERNEL_BASE - PADDR_BASE;
@@ -66,8 +69,18 @@ SECTIONS
         *(.boot.bss)
         . = ALIGN(4K);
     }
+#ifdef CONFIG_KERNEL_SKIM_WINDOW
+    /* Align up so that the SKIM portion of the kernel is by itself
+       on large pages */
+#ifdef CONFIG_ARCH_IA32
+    . = ALIGN(4M);
+#else
+    . = ALIGN(2M);
+#endif
+#endif
 
     ki_boot_end = .;
+    ki_skim_start = .;
 
     .text . : AT(ADDR(.text) - KERNEL_OFFSET)
     {
@@ -80,6 +93,29 @@ SECTIONS
         *(.rodata.*)
     }
 
+    .skim_data . : AT(ADDR(.skim_data) - KERNEL_OFFSET)
+    {
+        *(.skim.data)
+        *(.skim.data.*)
+    }
+
+    .skim_bss . : AT(ADDR(.skim_bss) - KERNEL_OFFSET)
+    {
+        *(.skim.bss)
+        *(.skim.bss.*)
+    }
+
+#ifdef CONFIG_KERNEL_SKIM_WINDOW
+    /* Align up so that the SKIM portion of the kernel is by itself
+       on large pages */
+#ifdef CONFIG_ARCH_IA32
+    . = ALIGN(4M);
+#else
+    . = ALIGN(2M);
+#endif
+#endif
+    ki_skim_end = .;
+
     .data . : AT(ADDR(.data) - KERNEL_OFFSET)
     {
         *(.data)
