Canonical-kernel__Ubuntu-kernel
commit 4d76e204d079ac3003d5f80eaafa2a9bcb337e58
Author:     Linus Torvalds <torvalds@linux-foundation.org>
AuthorDate: Sat Aug 14 11:44:56 2010 -0700
Commit:     Stefan Bader <stefan.bader@canonical.com>
CommitDate: Tue Aug 17 12:43:47 2010 +0200

    mm: fix page table unmap for stack guard page properly
    
    CVE-2010-2240
    
    We do in fact need to unmap the page table _before_ doing the whole
    stack guard page logic, because if it is needed (mainly 32-bit x86 with
    PAE and CONFIG_HIGHPTE, but other architectures may use it too) then it
    will do a kmap_atomic/kunmap_atomic.
    
    And those kmaps will create an atomic region that we cannot do
    allocations in.  However, the whole stack expand code will need to do
    anon_vma_prepare() and vma_lock_anon_vma() and they cannot do that in an
    atomic region.
    
    Now, a better model might actually be to do the anon_vma_prepare() when
    _creating_ a VM_GROWSDOWN segment, and not have to worry about any of
    this at page fault time.  But in the meantime, this is the
    straightforward fix for the issue.
    
    See https://bugzilla.kernel.org/show_bug.cgi?id=16588 for details.
    
    Reported-by: Wylda <wylda@volny.cz>
    Reported-by: Sedat Dilek <sedat.dilek@gmail.com>
    Reported-by: Mike Pagano <mpagano@gentoo.org>
    Reported-by: Fran√ßois Valenduc <francois.valenduc@tvcablenet.be>
    Tested-by: Ed Tomlinson <edt@aei.ca>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: Greg KH <gregkh@suse.de>
    Cc: stable@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>
    (cherry-picked from commit 11ac552477e32835cb6970bf0a70c210807f5673 upstream)
    Signed-off-by: Stefan Bader <stefan.bader@canonical.com>

diff --git a/mm/memory.c b/mm/memory.c
index c02c86190d34..7b83de90aa91 100644
--- a/mm/memory.c
+++ b/mm/memory.c
@@ -2207,14 +2207,13 @@ static int do_anonymous_page(struct mm_struct *mm, struct vm_area_struct *vma,
 	spinlock_t *ptl;
 	pte_t entry;
 
-	if (check_stack_guard_page(vma, address) < 0) {
-		pte_unmap(page_table);
+	pte_unmap(page_table);
+
+	/* Check if we need to add a guard page to the stack */
+	if (check_stack_guard_page(vma, address) < 0)
 		return VM_FAULT_SIGBUS;
-	}
 
 	/* Allocate our own private page. */
-	pte_unmap(page_table);
-
 	if (unlikely(anon_vma_prepare(vma)))
 		goto oom;
 	page = alloc_zeroed_user_highpage_movable(vma, address);
