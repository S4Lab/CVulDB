Canonical-kernel__Ubuntu-kernel
commit 1d25bb6e12d5f9f25cc4e329f5931aa380d28e3c
Author:     Neal Cardwell <ncardwell@google.com>
AuthorDate: Mon Nov 21 17:15:14 2011 +0000
Commit:     Luis Henriques <luis.henriques@canonical.com>
CommitDate: Thu Sep 6 09:29:57 2012 +0100

    tcp: do not scale TSO segment size with reordering degree
    
    CVE-2012-3412
    
    BugLink: http://bugs.launchpad.net/bugs/1037456
    
    Since 2005 (c1b4a7e69576d65efc31a8cea0714173c2841244)
    tcp_tso_should_defer has been using tcp_max_burst() as a target limit
    for deciding how large to make outgoing TSO packets when not using
    sysctl_tcp_tso_win_divisor. But since 2008
    (dd9e0dda66ba38a2ddd1405ac279894260dc5c36) tcp_max_burst() returns the
    reordering degree. We should not have tcp_tso_should_defer attempt to
    build larger segments just because there is more reordering. This
    commit splits the notion of deferral size used in TSO from the notion
    of burst size used in cwnd moderation, and returns the TSO deferral
    limit to its original value.
    
    Signed-off-by: Neal Cardwell <ncardwell@google.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    (cherry picked from commit 6b5a5c0dbb11dcff4e1b0f1ef87a723197948ed4)
    Acked-by: Herton Krzesinski <herton.krzesinski@canonical.com>
    Signed-off-by: Tim Gardner <tim.gardner@canonical.com>

diff --git a/include/net/tcp.h b/include/net/tcp.h
index 38509f047382..0405897ce822 100644
--- a/include/net/tcp.h
+++ b/include/net/tcp.h
@@ -811,6 +811,14 @@ static inline u32 rfc3390_bytes_to_packets(const u32 smss)
 extern void tcp_enter_cwr(struct sock *sk, const int set_ssthresh);
 extern __u32 tcp_init_cwnd(struct tcp_sock *tp, struct dst_entry *dst);
 
+/* The maximum number of MSS of available cwnd for which TSO defers
+ * sending if not using sysctl_tcp_tso_win_divisor.
+ */
+static inline __u32 tcp_max_tso_deferred_mss(const struct tcp_sock *tp)
+{
+	return 3;
+}
+
 /* Slow start with delack produces 3 packets of burst, so that
  * it is safe "de facto".  This will be the default - same as
  * the default reordering threshold - but if reordering increases,
diff --git a/net/ipv4/tcp_cong.c b/net/ipv4/tcp_cong.c
index 850c737e08e2..fc6d475f488f 100644
--- a/net/ipv4/tcp_cong.c
+++ b/net/ipv4/tcp_cong.c
@@ -292,7 +292,7 @@ int tcp_is_cwnd_limited(const struct sock *sk, u32 in_flight)
 	    left * sysctl_tcp_tso_win_divisor < tp->snd_cwnd &&
 	    left * tp->mss_cache < sk->sk_gso_max_size)
 		return 1;
-	return left <= tcp_max_burst(tp);
+	return left <= tcp_max_tso_deferred_mss(tp);
 }
 EXPORT_SYMBOL_GPL(tcp_is_cwnd_limited);
 
diff --git a/net/ipv4/tcp_output.c b/net/ipv4/tcp_output.c
index 8b0d0167e44a..140166c44023 100644
--- a/net/ipv4/tcp_output.c
+++ b/net/ipv4/tcp_output.c
@@ -1576,7 +1576,7 @@ static int tcp_tso_should_defer(struct sock *sk, struct sk_buff *skb)
 		 * frame, so if we have space for more than 3 frames
 		 * then send now.
 		 */
-		if (limit > tcp_max_burst(tp) * tp->mss_cache)
+		if (limit > tcp_max_tso_deferred_mss(tp) * tp->mss_cache)
 			goto send_now;
 	}
 
