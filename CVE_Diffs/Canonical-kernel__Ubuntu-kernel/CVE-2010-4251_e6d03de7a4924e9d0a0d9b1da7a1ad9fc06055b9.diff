Canonical-kernel__Ubuntu-kernel
commit e6d03de7a4924e9d0a0d9b1da7a1ad9fc06055b9
Author:     Eric Dumazet <eric.dumazet@gmail.com>
AuthorDate: Sun Nov 8 10:20:19 2009 +0000
Commit:     Herton Ronaldo Krzesinski <herton.krzesinski@canonical.com>
CommitDate: Wed Aug 10 14:26:42 2011 -0300

    udp: multicast RX should increment SNMP/sk_drops counter in allocation failures CVE-2010-4251
    
    BugLink: http://bugs.launchpad.net/bugs/807462
    
    When skb_clone() fails, we should increment sk_drops and SNMP counters.
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>
    (cherry picked from commit f6b8f32ca71406de718391369490f6b1e81fe0bb)
    
    Signed-off-by: Paolo Pisati <paolo.pisati@canonical.com>
    Signed-off-by: Tim Gardner <tim.gardner@canonical.com>

diff --git a/net/ipv4/udp.c b/net/ipv4/udp.c
index 6a39004383c3..595144dd1704 100644
--- a/net/ipv4/udp.c
+++ b/net/ipv4/udp.c
@@ -1192,12 +1192,22 @@ static void flush_stack(struct sock **stack, unsigned int count,
 {
 	unsigned int i;
 	struct sk_buff *skb1 = NULL;
+	struct sock *sk;
 
 	for (i = 0; i < count; i++) {
+		sk = stack[i];
 		if (likely(skb1 == NULL))
 			skb1 = (i == final) ? skb : skb_clone(skb, GFP_ATOMIC);
 
-		if (skb1 && udp_queue_rcv_skb(stack[i], skb1) <= 0)
+		if (!skb1) {
+			atomic_inc(&sk->sk_drops);
+			UDP_INC_STATS_BH(sock_net(sk), UDP_MIB_RCVBUFERRORS,
+					 IS_UDPLITE(sk));
+			UDP_INC_STATS_BH(sock_net(sk), UDP_MIB_INERRORS,
+					 IS_UDPLITE(sk));
+		}
+
+		if (skb1 && udp_queue_rcv_skb(sk, skb1) <= 0)
 			skb1 = NULL;
 	}
 	if (unlikely(skb1))
diff --git a/net/ipv6/udp.c b/net/ipv6/udp.c
index ede7a73b793c..6fe18460c703 100644
--- a/net/ipv6/udp.c
+++ b/net/ipv6/udp.c
@@ -450,14 +450,20 @@ static void flush_stack(struct sock **stack, unsigned int count,
 	for (i = 0; i < count; i++) {
 		skb1 = (i == final) ? skb : skb_clone(skb, GFP_ATOMIC);
 
+		sk = stack[i];
 		if (skb1) {
-			sk = stack[i];
 			bh_lock_sock(sk);
 			if (!sock_owned_by_user(sk))
 				udpv6_queue_rcv_skb(sk, skb1);
 			else
 				sk_add_backlog(sk, skb1);
 			bh_unlock_sock(sk);
+		} else {
+			atomic_inc(&sk->sk_drops);
+			UDP6_INC_STATS_BH(sock_net(sk),
+					UDP_MIB_RCVBUFERRORS, IS_UDPLITE(sk));
+			UDP6_INC_STATS_BH(sock_net(sk),
+					UDP_MIB_INERRORS, IS_UDPLITE(sk));
 		}
 	}
 }
