libvirt__libvirt
commit c430c002dd8287c5d7b834993ddfbd61435248c4
Author:     Jiri Denemark <jdenemar@redhat.com>
AuthorDate: Thu Dec 19 22:10:04 2013 +0100
Commit:     Guido GÃ¼nther <agx@sigxcpu.org>
CommitDate: Sat Jan 11 13:40:27 2014 +0100

    qemu: Do not access stale data in virDomainBlockStats
    
    CVE-2013-6458
    https://bugzilla.redhat.com/show_bug.cgi?id=1043069
    
    When virDomainDetachDeviceFlags is called concurrently to
    virDomainBlockStats: libvirtd may crash because qemuDomainBlockStats
    finds a disk in vm->def before getting a job on a domain and uses the
    disk pointer after getting the job. However, the domain in unlocked
    while waiting on a job condition and thus data behind the disk pointer
    may disappear. This happens when thread 1 runs
    virDomainDetachDeviceFlags and enters monitor to actually remove the
    disk. Then another thread starts running virDomainBlockStats, finds the
    disk in vm->def, and while it's waiting on the job condition (owned by
    the first thread), the first thread finishes the disk removal. When the
    second thread gets the job, the memory pointed to be the disk pointer is
    already gone.
    
    That said, every API that is going to begin a job should do that before
    fetching data from vm->def.
    
    Conflicts:
            src/qemu/qemu_driver.c
    
    (cherry picked from commit db86da5ca2109e4006c286a09b6c75bfe10676ad)

diff --git a/src/qemu/qemu_driver.c b/src/qemu/qemu_driver.c
index c0b4707b43..3f46c10a3f 100644
--- a/src/qemu/qemu_driver.c
+++ b/src/qemu/qemu_driver.c
@@ -7604,34 +7604,29 @@ qemuDomainBlockStats(virDomainPtr dom,
         goto cleanup;
     }
 
-    if (!virDomainObjIsActive(vm)) {
-        qemuReportError(VIR_ERR_OPERATION_INVALID,
-                        "%s", _("domain is not running"));
+    if (qemuDomainObjBeginJob(driver, vm, QEMU_JOB_QUERY) < 0)
         goto cleanup;
+
+    if (!virDomainObjIsActive(vm)) {
+        virReportError(VIR_ERR_OPERATION_INVALID,
+                       "%s", _("domain is not running"));
+        goto endjob;
     }
 
     if ((i = virDomainDiskIndexByName(vm->def, path, false)) < 0) {
-        qemuReportError(VIR_ERR_INVALID_ARG,
-                        _("invalid path: %s"), path);
-        goto cleanup;
+        virReportError(VIR_ERR_INVALID_ARG,
+                       _("invalid path: %s"), path);
+        goto endjob;
     }
     disk = vm->def->disks[i];
 
     if (!disk->info.alias) {
-        qemuReportError(VIR_ERR_INTERNAL_ERROR,
-                        _("missing disk device alias name for %s"), disk->dst);
-        goto cleanup;
+        virReportError(VIR_ERR_INTERNAL_ERROR,
+                       _("missing disk device alias name for %s"), disk->dst);
+        goto endjob;
     }
 
     priv = vm->privateData;
-    if (qemuDomainObjBeginJob(driver, vm, QEMU_JOB_QUERY) < 0)
-        goto cleanup;
-
-    if (!virDomainObjIsActive(vm)) {
-        qemuReportError(VIR_ERR_OPERATION_INVALID,
-                        "%s", _("domain is not running"));
-        goto endjob;
-    }
 
     qemuDomainObjEnterMonitor(driver, vm);
     ret = qemuMonitorGetBlockStatsInfo(priv->mon,
