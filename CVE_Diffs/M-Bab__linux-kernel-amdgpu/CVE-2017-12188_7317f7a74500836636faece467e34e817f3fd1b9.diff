M-Bab__linux-kernel-amdgpu
commit 7317f7a74500836636faece467e34e817f3fd1b9
Author:     Ladi Prosek <lprosek@redhat.com>
AuthorDate: Thu Oct 5 11:10:23 2017 +0200
Commit:     Seth Forshee <seth.forshee@canonical.com>
CommitDate: Thu Oct 19 09:49:29 2017 -0500

    KVM: MMU: always terminate page walks at level 1
    
    BugLink: http://bugs.launchpad.net/bugs/1724669
    
    commit 829ee279aed43faa5cb1e4d65c0cad52f2426c53 upstream.
    
    is_last_gpte() is not equivalent to the pseudo-code given in commit
    6bb69c9b69c31 ("KVM: MMU: simplify last_pte_bitmap") because an incorrect
    value of last_nonleaf_level may override the result even if level == 1.
    
    It is critical for is_last_gpte() to return true on level == 1 to
    terminate page walks. Otherwise memory corruption may occur as level
    is used as an index to various data structures throughout the page
    walking code.  Even though the actual bug would be wherever the MMU is
    initialized (as in the previous patch), be defensive and ensure here
    that is_last_gpte() returns the correct value.
    
    This patch is also enough to fix CVE-2017-12188.
    
    Fixes: 6bb69c9b69c315200ddc2bc79aee14c0184cf5b2
    Cc: Andy Honig <ahonig@google.com>
    Signed-off-by: Ladi Prosek <lprosek@redhat.com>
    [Panic if walk_addr_generic gets an incorrect level; this is a serious
     bug and it's not worth a WARN_ON where the recovery path might hide
     further exploitable issues; suggested by Andrew Honig. - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Signed-off-by: Seth Forshee <seth.forshee@canonical.com>

diff --git a/arch/x86/kvm/mmu.c b/arch/x86/kvm/mmu.c
index 7558531c1215..5c37786e4d35 100644
--- a/arch/x86/kvm/mmu.c
+++ b/arch/x86/kvm/mmu.c
@@ -3934,13 +3934,6 @@ static bool sync_mmio_spte(struct kvm_vcpu *vcpu, u64 *sptep, gfn_t gfn,
 static inline bool is_last_gpte(struct kvm_mmu *mmu,
 				unsigned level, unsigned gpte)
 {
-	/*
-	 * PT_PAGE_TABLE_LEVEL always terminates.  The RHS has bit 7 set
-	 * iff level <= PT_PAGE_TABLE_LEVEL, which for our purpose means
-	 * level == PT_PAGE_TABLE_LEVEL; set PT_PAGE_SIZE_MASK in gpte then.
-	 */
-	gpte |= level - PT_PAGE_TABLE_LEVEL - 1;
-
 	/*
 	 * The RHS has bit 7 set iff level < mmu->last_nonleaf_level.
 	 * If it is clear, there are no large pages at this level, so clear
@@ -3948,6 +3941,13 @@ static inline bool is_last_gpte(struct kvm_mmu *mmu,
 	 */
 	gpte &= level - mmu->last_nonleaf_level;
 
+	/*
+	 * PT_PAGE_TABLE_LEVEL always terminates.  The RHS has bit 7 set
+	 * iff level <= PT_PAGE_TABLE_LEVEL, which for our purpose means
+	 * level == PT_PAGE_TABLE_LEVEL; set PT_PAGE_SIZE_MASK in gpte then.
+	 */
+	gpte |= level - PT_PAGE_TABLE_LEVEL - 1;
+
 	return gpte & PT_PAGE_SIZE_MASK;
 }
 
diff --git a/arch/x86/kvm/paging_tmpl.h b/arch/x86/kvm/paging_tmpl.h
index b0454c7e4cff..da06dc8c4fc4 100644
--- a/arch/x86/kvm/paging_tmpl.h
+++ b/arch/x86/kvm/paging_tmpl.h
@@ -334,10 +334,11 @@ static int FNAME(walk_addr_generic)(struct guest_walker *walker,
 		--walker->level;
 
 		index = PT_INDEX(addr, walker->level);
-
 		table_gfn = gpte_to_gfn(pte);
 		offset    = index * sizeof(pt_element_t);
 		pte_gpa   = gfn_to_gpa(table_gfn) + offset;
+
+		BUG_ON(walker->level < 1);
 		walker->table_gfn[walker->level - 1] = table_gfn;
 		walker->pte_gpa[walker->level - 1] = pte_gpa;
 
