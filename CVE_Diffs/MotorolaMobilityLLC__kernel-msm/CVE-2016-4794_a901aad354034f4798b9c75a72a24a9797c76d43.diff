MotorolaMobilityLLC__kernel-msm
commit a901aad354034f4798b9c75a72a24a9797c76d43
Author:     Tejun Heo <tj@kernel.org>
AuthorDate: Wed May 25 11:48:25 2016 -0400
Commit:     Srihari Sathyanarayana <sriharis@motorola.com>
CommitDate: Mon Nov 14 22:11:11 2016 -0600

    percpu: fix synchronization between synchronous map extension and chunk destruction
    
    For non-atomic allocations, pcpu_alloc() can try to extend the area
    map synchronously after dropping pcpu_lock; however, the extension
    wasn't synchronized against chunk destruction and the chunk might get
    freed while extension is in progress.
    
    This patch fixes the bug by putting most of non-atomic allocations
    under pcpu_alloc_mutex to synchronize against pcpu_balance_work which
    is responsible for async chunk management including destruction.
    
    Mot-CRs-fixed:IKSECURITY-1696
    CVE-Fixed:CVE-2016-4794
    
    Change-Id: Ic4270eaa0bd740c8f6919f655e4b04554c425219
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-and-tested-by: Alexei Starovoitov <alexei.starovoitov@gmail.com>
    Reported-by: Vlastimil Babka <vbabka@suse.cz>
    Reported-by: Sasha Levin <sasha.levin@oracle.com>
    Cc: stable@vger.kernel.org # v3.18+
    Fixes: 1a4d76076cda ("percpu: implement asynchronous chunk population")
    Signed-off-by: sriharis <sriharis@motorola.com>
    Reviewed-on: https://gerrit.mot.com/918267
    SME-Granted: SME Approvals Granted
    SLTApproved: Slta Waiver <sltawvr@motorola.com>
    Tested-by: Jira Key <jirakey@motorola.com>
    Reviewed-by: Igor Kovalenko <igork@motorola.com>
    Submit-Approved: Jira Key <jirakey@motorola.com>

diff --git a/mm/percpu.c b/mm/percpu.c
index 88bb6c92d83a..ae2288fe8838 100644
--- a/mm/percpu.c
+++ b/mm/percpu.c
@@ -160,7 +160,7 @@ static struct pcpu_chunk *pcpu_reserved_chunk;
 static int pcpu_reserved_chunk_limit;
 
 static DEFINE_SPINLOCK(pcpu_lock);	/* all internal data structures */
-static DEFINE_MUTEX(pcpu_alloc_mutex);	/* chunk create/destroy, [de]pop */
+static DEFINE_MUTEX(pcpu_alloc_mutex);	/* chunk create/destroy, [de]pop, map ext */
 
 static struct list_head *pcpu_slot __read_mostly; /* chunk list slots */
 
@@ -437,6 +437,8 @@ static int pcpu_extend_area_map(struct pcpu_chunk *chunk, int new_alloc)
 	size_t old_size = 0, new_size = new_alloc * sizeof(new[0]);
 	unsigned long flags;
 
+	lockdep_assert_held(&pcpu_alloc_mutex);
+
 	new = pcpu_mem_zalloc(new_size);
 	if (!new)
 		return -ENOMEM;
@@ -897,6 +899,9 @@ static void __percpu *pcpu_alloc(size_t size, size_t align, bool reserved,
 		return NULL;
 	}
 
+	if (!is_atomic)
+		mutex_lock(&pcpu_alloc_mutex);
+
 	spin_lock_irqsave(&pcpu_lock, flags);
 
 	/* serve reserved allocations from the reserved chunk if available */
@@ -969,12 +974,9 @@ restart:
 	if (is_atomic)
 		goto fail;
 
-	mutex_lock(&pcpu_alloc_mutex);
-
 	if (list_empty(&pcpu_slot[pcpu_nr_slots - 1])) {
 		chunk = pcpu_create_chunk();
 		if (!chunk) {
-			mutex_unlock(&pcpu_alloc_mutex);
 			err = "failed to allocate new chunk";
 			goto fail;
 		}
@@ -985,7 +987,6 @@ restart:
 		spin_lock_irqsave(&pcpu_lock, flags);
 	}
 
-	mutex_unlock(&pcpu_alloc_mutex);
 	goto restart;
 
 area_found:
@@ -995,8 +996,6 @@ area_found:
 	if (!is_atomic) {
 		int page_start, page_end, rs, re;
 
-		mutex_lock(&pcpu_alloc_mutex);
-
 		page_start = PFN_DOWN(off);
 		page_end = PFN_UP(off + size);
 
@@ -1007,7 +1006,6 @@ area_found:
 
 			spin_lock_irqsave(&pcpu_lock, flags);
 			if (ret) {
-				mutex_unlock(&pcpu_alloc_mutex);
 				pcpu_free_area(chunk, off, &occ_pages);
 				err = "failed to populate";
 				goto fail_unlock;
@@ -1047,6 +1045,8 @@ fail:
 		/* see the flag handling in pcpu_blance_workfn() */
 		pcpu_atomic_alloc_failed = true;
 		pcpu_schedule_balance_work();
+	} else {
+		mutex_unlock(&pcpu_alloc_mutex);
 	}
 	return NULL;
 }
